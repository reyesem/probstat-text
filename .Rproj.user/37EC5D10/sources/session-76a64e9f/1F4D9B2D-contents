# Autocorrelation {#sec-autocorrelation}

{{< include _setupcode.qmd >}}

A critical assumption throughout the text has been that of independence.  For example, we might assume that 

$$Y_i = \mu + \varepsilon_i$$

where the $\varepsilon_i$ are independent random variables.  For a more concrete example, suppose we say that $\mu = 0$ and $\varepsilon \sim N(0, 1)$, again, all independent.  And, suppose we take a sample of size 100.  @fig-time-series illustrates several possible samples, where the observations are made sequentially.  That is, @fig-time-series shows several time-series plots (plot of the observations over time).

```{r}
#| label: fig-time-series
#| fig-cap: The time-series plot for several random samples of size 100 taken from a Standard Normal distribution.  One sample is highlighted for illustration.  No visible patterns are present.

set.seed(20240826)

plotdat <- tibble(
  Observation = rep(seq(100), times = 10),
  Group = rep(seq(10), each = 100),
  Highlight = (Group == 1),
  Response = rnorm(length(Observation))
)

ggplot(plotdat) +
  aes(y = Response, x = Observation) +
  geom_line(mapping = aes(group = Group), color = 'grey75', alpha = 0.5) +
  geom_line(
    data = filter(plotdat, Highlight),
    color = 'black', linewidth = 1.1
  ) +
  geom_point(
    data = filter(plotdat, Highlight)
  )
```

There are no visible trends in the location or spread of the responses as we move across the graphic.  While any one time-series plot _may_ show some small trend, overall, across repeated samples, no trend is observed.  This lack of trend is the result of the independence between observations.  Any one observation does not help us predict the location of the next.


In contrast, suppose we continue to take $\mu = 0$, but we say that $\varepsilon_1 \sim N(0, 1)$ and 

$$\varepsilon_i \mid \varepsilon_{i-1} \sim N\left(\varepsilon_{i-1}, 1\right)$$

for $i = 2, 3, \dotsc, n$.  Notice that the distribution of one error term depends on the value of the previous error.  We again consider taking a sample of size 100.  @fig-time-series-auto shows several time-series plots for this updated situation.

```{r}
#| label: fig-time-series-auto
#| fig-cap: The time-series plot for several samples of size 100 taken sequentially.  The first observation is taken from a Standard Normal distribution; each subsequent observation is taken from a Normal distribution centered on the previous observation with a variance of 1.  One sample is highlighted for illustration.  

set.seed(20240826)

plotdat <- tibble(
  Observation = rep(seq(100), times = 10),
  Group = rep(seq(10), each = 100),
  Highlight = (Group == 4),
  Response = NA
)

for (i in 1:nrow(plotdat)) {
  if (plotdat$Observation[i] == 1) {
    plotdat$Response[i] <- rnorm(1)
  } else {
    plotdat$Response[i] <- rnorm(1, mean = plotdat$Response[i-1])
  }
}

ggplot(plotdat) +
  aes(y = Response, x = Observation) +
  geom_line(mapping = aes(group = Group), color = 'grey75', alpha = 0.5) +
  geom_line(
    data = filter(plotdat, Highlight),
    color = 'black', linewidth = 1.1
  ) +
  geom_point(
    data = filter(plotdat, Highlight)
  )
```

Unlike @fig-time-series, we notice a clear trend in the location of the series highlighted in @fig-time-series-auto.  The location of the response tends to decrease over time.  The lack of independence is revealing itself in a trend in the location of the response over time.

This is known as auto-correlation.  While the specifics of this phenomena would be studied in a course on regression modeling, what we see is that the relationship between one observation and the next is time-dependent.  That is, observations close together in time (indices that are near one another) are related to one another, while observations further apart in time are less related to one another.

:::{#exm-covariance-auto}
## Covariance in Autocorrelation Model
Consider the simple autocorrelation model

$$Y_i = \varepsilon_i$$

where $\varepsilon_1 \sim N(0, 1)$ and 

$$\varepsilon_i \mid \varepsilon_{i-1} \sim N\left(\varepsilon_{i-1}, 1\right)$$

for $i > 1$.  Determine the covariance between two observations.
:::

:::{.soln}
Without loss of generality, consider $Y_1$ and $Y_2$.  Observe that

$$Cov\left(Y_1, Y_2\right) = Cov\left(\varepsilon_1, \varepsilon_2\right) = E\left(\varepsilon_{1} \varepsilon_{2}\right) - E\left(\varepsilon_{1}\right)E\left(\varepsilon_{2}\right).$$

In order to evaluate these expectations, we need an interim result: for any two random variables $X$ and $Y$, we have that $E(X) = E(E(X \mid Y))$.  That is, in the inner expectation, we take the conditional expectation of $X$ given $Y$; the result will be only a function of the random variable $Y$.  In the outer expectation, we take the expectation with respect to $Y$.

Returning to our problem at hand, we know that $E\left(\varepsilon_1\right) = 0$.  Applying our latest result, we have

$$
\begin{aligned}
  E\left(\varepsilon_2\right) 
    &= E\left[E\left(\varepsilon_2 \mid \varepsilon_1\right)\right] \\
    &= E\left[\varepsilon_1\right] \\
    &= 0.
\end{aligned}
$$

We also have that

$$
\begin{aligned}
  E\left(\varepsilon_1 \varepsilon_2\right) 
    &= E\left[E\left(\varepsilon_1 \varepsilon_2 \mid \varepsilon_1\right)\right] \\
    &= E\left[ \varepsilon_1 E\left(\varepsilon_2 \mid \varepsilon_1\right)\right] \\
    &= E\left[\varepsilon_1^2\right] \\
    &= Var\left(\varepsilon_1\right) + E^2\left(\varepsilon_1\right) \\
    &= 1.
\end{aligned}
$$

Line 2 comes from recognizing that if we are "given" $\varepsilon_1$, then it is constant in terms of the inner expectation and comes out of the expectation.  Now, we have shown that

$$Cov\left(Y_1, Y_2\right) = 1$$

meaning that there is a correlation between terms that are next to one another.  
:::
