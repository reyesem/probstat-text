{"title":"Matrix View of Regression","markdown":{"headingText":"Matrix View of Regression","headingAttr":{"id":"sec-matrix","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n{{< include _setupcode.qmd >}}\n\n@def-classical-simple-regression introduced us to a model for the mean response as a function of a single predictor.  Specifically, we considered $\\left(Y_1, x_1\\right), \\left(Y_2, x_2\\right), \\dotsc, \\left(Y_n, x_n\\right)$ to be observations made on a sample of $n$ units.  Under the classical simple linear regression model, the distributional model for the response among the population is given by\n\n$$Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{Ind}}{\\sim} N\\left(\\beta_0 + \\beta_1 x_i, \\sigma^2\\right)$$\n\nwhere $\\beta_0$, $\\beta_1$, and $\\sigma^2$ are unknown parameters.\n\nThis model is simplistic in that it only considers a single predictor.  In reality, we often want the mean response to depend on several variables.\n\n:::{.callout-note}\n## Predictors vs. Covariates\nA statistical model posits a relationship between a response and one or more variables.  Depending on the discipline or the use of the statistical model, we might refer to the variables in the model as \"predictors,\" \"covariates,\" \"factors,\" or \"treatments.\"  We tend to refer to quantitative variables as \"predictors\" and categorical variables as \"factors\" when they appear in the mean model.  However, these terms may be used differently in different disciplines.\n:::\n\nOn one hand, incorporating additional predictors into the model is straight-forward; for example, we might consider $Y_i$ to be the response measured on the $i$-th observation in a sample, and $x_{1,i}, x_{2, i}, \\dotsc, x_{p,i}$ to be the 1st, 2nd, $\\dotsc$, $p$-th predictor measured on the $i$-th observation (for $i = 1, 2, \\dotsc, n$).  Then, we might posit the following model for the response among the population:\n\n$$Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{Ind}}{\\sim} N\\left(\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{j,i}, \\sigma^2\\right)$$\n\nwhere $\\beta_0, \\beta_1, \\beta_2, \\dotsc, \\beta_p$ and $\\sigma^2$ are unknown parameters.\n\n:::{.callout-note}\nAs in @def-classical-simple-regression, note that we only consider the response to be a random variable; the predictors are considered fixed (hence the use of a lowercase $x$ instead of a capital $X$).  This is consistent with the idea of a designed experiment for which the values of the predictor can be determined in advance by the researchers.  \n\nHowever, for observational studies, the values of the predictor cannot be fixed.  That is, in practice, the predictor is also unknown in advance and is therefore a random variable as well.  In such cases, we can proceed in the same manner, considering the _conditional_ distribution of the response _given_ the predictor.\n:::\n\nWhile we can continue to extend results from @sec-regression to the case of $p$ predictors, the notation can become tedious.  It helps to express our model using matrices.\n\n\n## Expressing Linear Combinations\nYou might have noticed that we wrote our mean model as a linear combination of predictors\n\n$$\\sum_{j=1}^{p} \\beta_j x_{j,i}.$$\n\nActually, while it may not be clear at this stage, it is better to recognize the mean model as a linear combination of _parameters_, written as\n\n$$\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{j,i} = 1 \\beta_0 + x_{1,i} \\beta_1 + \\dotsb + x_{p,i} \\beta_p.$$\n\nRecall that a linear combination can be written as a dot product of two vectors.  Therefore, we can express this linear combination as\n\n$$1 \\beta_0 + x_{1,i} \\beta_1 + \\dotsb + x_{p,i} \\beta_p = \\bm{x}_i^\\top \\bs{\\beta},$$\n\nwhere\n\n$$\\bm{x}_i = \\begin{pmatrix} 1 \\\\x_{1,i} \\\\ x_{2,i} \\\\ \\vdots \\\\ x_{p,i} \\end{pmatrix}$$\n\nand\n\n$$\\bs{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix}.$$\n\n:::{.callout-note}\nAll vectors are by default column vectors, which corresponds to how they are stored in statistical programming languages.\n:::\n\nNotice we maintain the $i$ subscript, because the linear combination $\\bm{x}_i^\\top \\bs{\\beta}$ will differ for each observation in our sample (the values of the variables for one observation will differ from the values of the variables for other observations).  We might then re-express distributional model for the response as\n\n$$Y_i \\stackrel{\\text{Ind}}{\\sim} N\\left(\\bm{x}_i^\\top \\bs{\\beta}, \\sigma^2\\right).$$\n\n## Multiple Regression\nOften termed \"multiple regression,\" we write out the model for the response when we allow the mean to be a linear function of several predictors. \n\n:::{#def-multiple-model}\n## Classical Regression Model\nLet $Y_i$ represent the response for the $i$-th observation, and let $\\bm{x}_i$ represent the vector of observed predictors for the $i$-th observation in a sample of $n$ units, including the intercept term.  Under the classical linear regression model, the distributional model for the response among the population is given by\n\n$$Y_i \\stackrel{\\text{Ind}}{\\sim} N\\left(\\bm{x}_i^\\top \\bs{\\beta}, \\sigma^2\\right).$$\n\n:::\n\n:::{.callout-note}\nWe have been assuming that the first element in $\\bm{x}_i$ is a 1 to capture the intercept.  It is possible to express a model without an intercept term in which case the first element of $\\bm{x}_i$ is $x_{1,i}$.\n:::\n\nNotice that @def-multiple-model simply extends the form of the distributional model we have previously considered.  It makes it clear that\n\n$$E\\left(Y_i\\right) = \\bm{x}_i^\\top \\bs{\\beta}$$\n\nfor each unit.  While this presentation connects the process for the inference of a single mean with that of regression, it is not the common presentation.  Instead, the model is traditionally presented as saying that\n\n$$Y_i = \\bm{x}_i^\\top \\bs{\\beta} + \\varepsilon_i$$\n\nwhere $\\varepsilon_i \\stackrel{\\text{IID}}{\\sim} N\\left(0, \\sigma^2\\right)$, where now we can make use of the \"identically distributed\" language.  In this presentation, we have introduced a new random variable, $\\varepsilon$.  Since the expression $\\bm{x}_i^\\top \\bs{\\beta}$ does not contain a random variable, it is deterministic in nature.  Therefore, the distribution of $Y_i$ is determined because we are simply shifting the distribution of $\\varepsilon_i$.  As with the simple linear regression model, we can relax the conditions we place on the distribution of $\\varepsilon_i$.\n\nRegardless of the conditions we impose on $\\varepsilon$, we are essentially specifying a model for the data generating process --- the set of statements we are willing to make regarding the variability in the response.  We know that since the response is a random variable it has some distribution.  The model for the data generating process is really a set of statements about that distribution.  We may only be characterizing the mean of the distribution of the response; we may be willing to characterize the mean and the variance; or, we may be willing to fully characterize the distributional form.\n\nJust as we did with the simple linear regression model, we can use the method of least squares to estimate the parameters in the model.  Specifically, we choose the parameter vector $\\bs{\\beta}$ to minimize the quantity\n\n$$\\sum_{i=1}^{n} \\left(Y_i - \\bm{x}_i^\\top \\bs{\\beta}\\right)^2.$$ {#eq-lsq}\n\nThe corresponding estimates are denoted by the vector $\\widehat{\\bs{\\beta}}.$  Unlike the simple linear regression case, we now have $p + 1$ parameters (assuming an intercept term) and therefore minimizing this quantity requires that we take $p + 1$ partial derivatives and simultaneously solve the $p + 1$ resulting equations.  This is where the power of matrix algebra makes the computations simpler.  \n\nLet $\\bm{Y}$ denote the vector of responses; that is,\n\n$$\\bm{Y} = \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}.$$\n\nAnd, let $\\bm{X}$ denote the __design matrix__\n\n$$\\bm{X} = \\begin{pmatrix} 1 & x_{1,1} & x_{2,1} & \\dotsb & x_{p, 1} \\\\\n1 & x_{1, 2} & x_{2, 2} & \\dotsb & x_{p, 2} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{1, n} & x_{2, n} & \\dotsb & x_{p, n} \\end{pmatrix} = \\begin{pmatrix} \\bm{x}_1^\\top \\\\ \\bm{x}_2^\\top \\\\ \\vdots \\\\ \\bm{x}_n^\\top \\end{pmatrix}.$$\n\n:::{.callout-note}\nThe first column being a column of 1's captures the intercept term; if there is no intercept in the model, this column is omitted.\n:::\n\nNow, we can express the least squares objective function (@eq-lsq) as\n\n$$(\\bm{Y} - \\bm{X}\\bs{\\beta})^\\top (\\bm{Y} - \\bm{X} \\bs{\\beta}).$$ {#eq-ls-matrix}\n\n:::{.callout-tip}\nBefore proceeding, convince yourself that this algebra makes sense.  A dot product of any vector with itself is simply a sum of squared terms (\"sum of squares\"), and the $i$-th element of each vector is simply the $i$-th component of the sum in @eq-lsq.\n:::\n\n\nLeast squares estimation is now about choosing the parameter vector $\\bs{\\beta}$ such that we minimize @eq-ls-matrix.  Taking a derivative (with respect to the _vector_ $\\bs{\\beta}$) results in choosing the parameter vector $\\bs{\\beta}$ such that \n\n$$\\left(\\bm{X}^\\top \\bm{X}\\right) \\bs{\\beta} = \\bm{X}^\\top \\bs{Y},$$ {#eq-norm-eq}\n\nwhich are known as the __normal equations__.  Again, we have not changed the original problem; we are just expressing it in matrices; there are still $p + 1$ equations with $p + 1$ unknowns (assuming an intercept term).  This leads to a compact expression for the least squares estimator:\n\n$$\\widehat{\\bs{\\beta}} = \\left(\\bm{X}^\\top \\bm{X}\\right)^{-1} \\bm{X}^\\top \\bs{Y}.$$ {#eq-reg-lse}\n\n:::{.callout-warning}\nIn practice, the inverse of the matrix $\\bm{X}^\\top \\bm{X}$ is never taken directly.  There are much more efficient algorithms for computing the least squares estimates.\n:::\n\n\n:::{#thm-least-squares-mlr}\n## Least Squares Estimates\nLet $Y_i$ represent the response for the $i$-th observation, and let $\\bm{x}_i$ represent the vector of observed predictors for the $i$-th observation in a sample of $n$ units, including the intercept term.  The least squares estimates for the multiple linear regression model relating the response and predictor are given by\n\n$$\\widehat{\\bs{\\beta}} = \\left(\\bm{X}^\\top \\bm{X}\\right)^{-1} \\bm{X}^\\top \\bs{Y}.$$\n\nwhere $\\bm{Y}$ is the vector of responses and \n\n$$\\bm{X} = \\begin{pmatrix} \\bm{x}_1^\\top \\\\ \\bm{x}_2^\\top \\\\ \\vdots \\\\ \\bm{x}_n^\\top \\end{pmatrix}.$$\n\nis known as the design matrix.\n:::\n\n:::{.proof}\nBy definition, the least squares estimate of the parameter vector $\\bs{\\beta}$ is the vector that minimizes the quantity\n\n$$Q(\\bs{\\beta}) = \\left(\\bm{Y} - \\bm{X} \\bs{\\beta}\\right)^\\top \\left(\\bm{Y} - \\bm{X} \\bs{\\beta}\\right).$$\n\nDefine $\\widehat{\\bs{\\beta}}$ to be the vector\n\n$$\\left(\\bm{X}^\\top \\bm{X}\\right)^{-1} \\bm{X}^\\top \\bs{Y}.$$\n\nObserve that\n\n$$\n\\begin{aligned}\n  Q(\\bs{\\beta})\n    &= \\left(\\bm{Y} - \\bm{X} \\bs{\\beta}\\right)^\\top \\left(\\bm{Y} - \\bm{X} \\bs{\\beta}\\right) \\\\\n    &= \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}} + \\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right)^\\top \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}} + \\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right)\n\\end{aligned}\n$$\n\nwhere the second line is obtained by adding and subtracting the term $\\bm{X}\\widehat{\\bs{\\beta}}$ from each vector.  We then expand the terms and obtain\n\n$$\n\\begin{aligned}\n  Q(\\bs{\\beta})\n    &= \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}}\\right)^\\top \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}}\\right) \\\\\n    &\\qquad + \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}}\\right)^\\top \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right) \\\\\n    &\\qquad + \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right)^\\top \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}}\\right) \\\\\n    &\\qquad + \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right)^\\top \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right).\n\\end{aligned}\n$$ {#eq-proof-q}\n\nLet's consider the second line of @eq-proof-q; observe that\n\n$$\n\\begin{aligned}\n  \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}}\\right)^\\top \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right)\n    &= \\bm{Y}^\\top \\bm{X}\\widehat{\\bs{\\beta}} - \\bm{Y}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &\\qquad - \\widehat{\\bs{\\beta}}^\\top \\bm{X}^\\top \\bm{X} \\widehat{\\bs{\\beta}} + \\widehat{\\bs{\\beta}}^\\top \\bm{X}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &= \\bm{Y}^\\top \\bm{X}\\widehat{\\bs{\\beta}} - \\bm{Y}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &\\qquad - \\left[\\left(\\bm{X}^\\top \\bm{X}\\right)^{-1} \\bm{X}^\\top \\bm{Y}\\right]^\\top \\bm{X}^\\top \\bm{X} \\widehat{\\bs{\\beta}} \\\\\n    &\\qquad + \\left[\\left(\\bm{X}^\\top \\bm{X}\\right)^{-1} \\bm{X}^\\top \\bm{Y}\\right]^\\top \\bm{X}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &= \\bm{Y}^\\top \\bm{X}\\widehat{\\bs{\\beta}} - \\bm{Y}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &\\qquad - \\bm{Y}^\\top \\bm{X} \\left(\\bm{X}^\\top \\bm{X}\\right)^{-1}  \\bm{X}^\\top \\bm{X} \\widehat{\\bs{\\beta}} \\\\\n    &\\qquad + \\bm{Y}^\\top \\bm{X} \\left(\\bm{X}^\\top \\bm{X}\\right)^{-1} \\bm{X}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &= \\bm{Y}^\\top \\bm{X}\\widehat{\\bs{\\beta}} - \\bm{Y}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &\\qquad - \\bm{Y}^\\top \\bm{X} \\widehat{\\bs{\\beta}} + \\bm{Y}^\\top \\bm{X} \\bs{\\beta} \\\\\n    &= 0.\n\\end{aligned}\n$$\n\nThat is, the cross product terms cancel out.  Further, since line 3 of @eq-proof-q is simply the transpose of line 2, we also have that line 3 is equivalent to 0.  That is, we rewrite @eq-proof-q as\n\n$$\n\\begin{aligned}\n  Q(\\bs{\\beta})\n    &= \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}}\\right)^\\top \\left(\\bm{Y} - \\bm{X}\\widehat{\\bs{\\beta}}\\right) \\\\\n    &\\qquad + \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right)^\\top \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right) \\\\\n    &= Q\\left(\\widehat{\\bs{\\beta}}\\right) + \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right)^\\top \\left(\\bm{X}\\widehat{\\bs{\\beta}} - \\bm{X}\\bs{\\beta}\\right).\n\\end{aligned}\n$$ {#eq-proof-q-simple}\n\nWe note that both terms are sums of squares and therefore must be non-negative.  Further, since only the second term is a function of $\\bs{\\beta}$, minimizing $Q(\\bs{\\beta})$ is equivalent to minimizing the second term.  Since the second term in @eq-proof-q-simple is non-negative, we can minimize it by equating it to 0, which happens if and only if $\\bs{\\beta} = \\widehat{\\bs{\\beta}}$.  This establishes the result.\n:::\n\n\n:::{#exm-special-case}\n## Least Squares Estimate in the Intercept Only Model\nSuppose we have only an intercept in the model, determine the least squares estimates of the intercept starting with the matrix representation.\n:::\n\n:::{.solution}\nWhile we have already determined the least squares estimates for the simple linear model in @sec-regression, and we could use that result to determine this estimate.  However, we establish this as a special case of the multiple regression model as well.  Specifically, note that the least squares estimates are given by\n\n$$\\widehat{\\bs{\\beta}} = \\left(\\bm{X}^top \\bm{X}\\right)^{-1} \\bm{X}^\\top \\bm{Y}.$$\n\nNow, we know that for an intercept-only model, the design matrix is given by\n\n$$\\bm{X} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}.$$\n\nTherefore, we have that\n\n$$\\bm{X}^\\top \\bm{X} = n$$\nand\n\n$$\n\\left(\\bm{X}^\\top \\bm{X}\\right)^{-1} = \\frac{1}{n}.\n$$\n\nWe also have that\n\n$$\\bm{X}^\\top \\bm{Y} = \\sum_{i=1}^{n} y_i = n\\bar{y}.$$\n\nTherefore, the least squares estimate is given by\n\n$$\n\\widehat{\\beta}_0 = \\frac{1}{n} n \\bar{y} = \\bar{y}\n$$\n\nthe sample mean response.\n:::\n\n\n:::{#thm-classical-ls-multiple}\n## Sampling Distribution for Least Squares Estimators\nUnder the conditions of the Classical Regression Model (@def-multiple-model), we have that, holding all other predictors fixed\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim t_{n - p - 1}$$\n\nwhere $Var\\left(\\widehat{\\beta}_j\\right)$ is the $(j,j)$-th element of the matrix\n\n$$\\widehat{\\sigma}^2 \\left(\\bm{X}^\\top \\bm{X}\\right)^{-1}$$\n\nand\n\n$$\\widehat{\\sigma}^2 = \\frac{1}{n - p - 1} \\sum_{i=1}^{n} \\left(Y_i - \\bm{x}_i^\\top \\widehat{\\bs{\\beta}}\\right)^2$$\n\nis our estimate of the unknown population variance $\\sigma^2$.\n:::\n\nOnce we have a model for the sampling distribution, we have the ability to make inference --- confidence intervals or p-values.\n\n\n:::{.callout-important}\nWhile subtle, the phrase \"holding all other predictors fixed\" is critical in the sampling distribution of the least squares estimates.\n:::\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["mystyles.css"],"output-file":"08-matrix.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","bibliography":["refs382notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"08-matrix.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["refs382notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}