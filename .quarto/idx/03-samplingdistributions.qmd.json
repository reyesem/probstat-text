{"title":"Sampling Distribution of a Statistic","markdown":{"headingText":"Sampling Distribution of a Statistic","headingAttr":{"id":"sec-samplingdistributions","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n{{< include _setupcode.qmd >}}\n\nIn the previous chapter, we related probability concepts associated with random variables to their statistical counterparts in data collection.  We introduced the idea of using a density function as a model of the distribution of a variable within the population.  This led to the observation that the population parameters govern the shape of these density functions.  Further, these parameters are the focal point of research objectives.  In a statistical analysis, the parameters would be estimated using statistics; in this chapter, we explore how probability theory helps us to characterize the variability in these statistics --- which is the key component of statistical inference.\n\n\n## Statistics and Expectations\nThe goal of statistics is to use a sample to say something about the underlying population.  Consider the following research objective:\n\n  > Estimate the cost (in US dollars) of a diamond for sale in the United States.\n  \nNo researcher would believe that measuring the cost of a single diamond would be sufficient to address the above research objective.  Instead, we would consider taking a sample of $n$ diamonds and measuring the cost of each.  We can represent the cost we will record (note the use of the future tense) as $X_1, X_2, \\dotsc, X_n$, where $X_i$ is the cost of the $i$-th diamond we will measure.  Collecting data on a sample requires that we deal with at least $n$ random variables (one measurement for each of the observations in our sample).\n\nIn almost every analysis, we compute a numerical summary of the data.  For example, the sample mean takes the form\n\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i.$$\n\n:::{.callout-note}\nWhile a capital letter denotes a random variable (not yet observed), the corresponding lowercase letter is used to denote a past observation (which is no longer random).\n:::\n\nThese numerical summaries, statistics, are functions of the data.  So, prior to when we collect that data, our statistics are functions of random variables.\n\n:::{#def-statistic}\n## Statistic\nA statistic is a numerical summary of a sample; it is a function of the data alone.  Prior to collecting data, a statistic is a function of the data to be collected.  \n:::\n\n@def-statistic eliminates the possibility of the statistic being a function of the underlying _parameters_; certainly, the behavior of the statistic is determined by the parameters, but the computation of a statistic should not require knowledge of the parameter once the data is collected.\n\n@def-statistic highlights the need to study functions and combinations of random variables.  If you recall, @thm-expectation introduced the idea of expectation as a linear operator.  The result, for a single random variable, is nearly intuitive.  If expectation is associated with integration (as defined in @def-mean), then expectations should adopt the properties of integration, including linearity.  The generalization of this result is extremely important within statistics.\n\n:::{#thm-linearity-of-expectations}\n## Linearity of Expectations\nFor random variables $X_1, X_2, \\dotsc, X_n$, constants $a_1, a_2, \\dotsc, a_n$ and real-valued functions $g_1, g_2, \\dotsc, g_n$, we have that\n\n$$E\\left[\\sum_{i=1}^{n} a_i g\\left(X_i\\right)\\right] = \\sum_{i=1}^{n} a_i E\\left[g\\left(X_i\\right)\\right].$$\n:::\n\nThe importance of @thm-linearity-of-expectations is seen in determining the expectation of the sample mean.\n\n:::{#exm-mean-xbar}\n## Mean of the Sample Mean\nLet $X_1, X_2, \\dotsc, X_n$ be random variables representing observations from a sample that will be collected.  Define the sample mean of that future sample to be\n\n$$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i.$$\n\nDetermine $E\\left(\\bar{X}\\right)$.\n:::\n\nBefore addressing the prompt in @exm-mean-xbar, we note that _before collecting the data_, statistics, like the sample mean, is a function of random variables and is therefore itself a random variable!  And, all random variables have distributions, and they have parameters that govern the shape of that distribution.  @exm-mean-xbar is focused on the mean of the distribution.\n\n:::{.solution}\nApplying @thm-linearity-of-expectations, we have\n\n$$\n\\begin{aligned}\n  E\\left(\\bar{X}\\right)\n    &= E\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) \\\\\n    &= \\frac{1}{n} \\sum_{i=1}^{n} E\\left(X_i\\right)\n\\end{aligned}\n$$\n\nIf we assume that each $X_i$ is taken from the same population, then $E\\left(X_i\\right) = \\mu$, the population mean, for some constant $\\mu$.  Therefore, we have that\n\n$$E\\left(\\bar{X}\\right) = \\frac{1}{n} \\sum_{i=1}^{n} \\mu = \\mu.$$\n:::\n\n@thm-linearity-of-expectations discusses how expectations work with linear combinations; we have a similar result for variances.  However, there is a particular caveat.\n\n:::{#thm-variance-independent-sum}\n## Variance of the Sum of Independent Random Variables\nLet $X_1, X_2, \\dotsc, X_n$ be independent random variables, and define constants $a_1, a_2, \\dotsc, a_n$ and real-valued functions $g_1, g_2, \\dotsc, g_n$.  Then,\n\n$$Var\\left[\\sum_{i=1}^{n} a_i g\\left(X_i\\right)\\right] = \\sum_{i=1}^{n} a^2_i Var\\left[g\\left(X_i\\right)\\right].$$\n:::\n\nComparing @thm-linearity-of-expectations to @thm-variance-independent-sum, we see that while the expectation always move through sums, the variance only does so when the random variables are independent.\n\n:::{#def-independence}\n## Independence\nRandom variables $X_1, X_2, \\dotsc, X_n$ are said to be mutually independent (or just \"independent\") if and only if\n\n$$Pr\\left(X_1 \\in A_1, X_2 \\in A_2, \\dotsb, X_n \\in A_n\\right) = \\prod_{i=1}^{n} Pr\\left(X_i \\in A_i\\right),$$\n\nwhere $A_1, A_2, \\dotsc, A_n$ are arbitrary sets.  \n:::\n\n:::{.callout-note}\nFor those not familiar, $\\prod_{i=1}^n a_i$ is the _product operator_.  It is analogous to $\\sum_{i=1}^{n} a_i$, but uses products instead of sums.\n:::\n\nEssentially, a random variable $X$ is said to be independent of $Y$ if the likelihood that $X$ takes a particular value is the same regardless of the value $Y$ takes.  \n\nConsider taking a sample of $n$ diamonds to address our above research objective.  It seems reasonable that the cost of one diamond does not depend on the cost of another.  That allows us to assume the values we collect will be independent; that is, the random variables we use to represent these costs are independent of one another.  It also seems natural to assume that each diamond we select comes from the same underlying population.  These two attributes together form the basis of what we mean by a \"random sample.\"\n\n:::{#def-random-sample}\n## Random Sample\nA random sample of size $n$ refers to a collection of $n$ random variables $X_1, X_2, \\dotsc, X_n$ such that the random variables are mutually independent, and the distribution of each random variable is identical.\n\nWe say $X_1, X_2, \\dotsc, X_n$ are independent and identically distributed, abbreviated IID.  We might also write this as $X_i \\stackrel{\\text{IID}}{\\sim} f$ for some density $f$.\n:::\n\n:::{.callout-warning}\nLet $X$ and $Y$ be identically distributed random variables.  This does not mean that $X = Y$.  That is, the two random variables need not take on the same value.  Instead, identically distributed means the density function of the two random variables are the same.  As a result, they share the same mean, variance, etc.  That is, their distributions are the same, not their value.\n:::\n\n:::{#exm-var-xbar}\n## Variance of the Sample Mean\nLet $X_1, X_2, \\dotsc, X_n$ be a random sample of size $n$ from a population with a variance of $\\sigma^2$.  Define the sample mean of that future sample to be\n\n$$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i.$$\n\nDetermine $Var\\left(\\bar{X}\\right)$.\n:::\n\n:::{.solution}\nSince $X_1, X_2, \\dotsc, X_n$ form a random sample; we know that they are mutually independent.  Therefore, @thm-variance-independent-sum gives\n\n$$\n\\begin{aligned}\n  Var\\left(\\bar{X}\\right)\n    &= Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) \\\\\n    &= \\frac{1}{n^2} \\sum_{i=1}^{n} Var\\left(X_i\\right) \\\\\n    &= \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 \\\\\n    &= \\sigma^2 / n.\n\\end{aligned}\n$$\n:::\n\n:::{.callout-warning}\nIt is important to remember that @exm-mean-xbar and @exm-var-xbar describe the mean and variance of the sample mean _prior to_ collecting data.  Once the data is collected, the sample mean has no distribution.  Once the data is collected, a statistic is simply a number.\n:::\n\nThere is one additional result for independent random variables that we should keep in mind.\n\n:::{#def-product-expectations}\n## Expectation of a Product of Independent Random Variables\nLet $X_1, X_2, \\dotsc, X_n$ be independent random variables, then\n\n$$E\\left(\\prod_{i=1}^n X_i\\right) = \\prod_{i=1}^{n} E\\left(X_i\\right).$$\n:::\n\n\n## Sampling Distribution of Sample Mean\nTogether, @exm-mean-xbar and @exm-var-xbar characterize the center and spread of the distribution of the sample mean.  Since each statistic is a random variable, it has a distribution, and we call that distribution a __sampling distribution__.\n\n:::{#def-sampling-distribution}\n## Sampling Distribution\nThe distribution of a statistic across repeated samples.\n:::\n\nNote that @def-sampling-distribution makes use of the idea of repeated sampling.  Once again, this leans on the frequentist interpretation of probability (@def-frequentist-interpretation).  We are thinking of the distribution of a statistic as the result of the different values that could potentially be observed if we were to repeatedly take samples of the same size.\n\nWhile @thm-linearity-of-expectations and @thm-variance-independent-sum allowed us to characterize the sampling distribution of the sample mean, these results did not provide information about the shape of the sampling distribution, much less provide a functional form of the density.  To make progress on this front, we return to another tool introduced in a probability course --- the __moment-generating function__.\n\n:::{#def-mgf}\n## Moment-Generating Function (MGF)\nFor a random variable $X$, let $M_X(t)$ be defined as\n\n$$M_X(t) = E\\left(e^{tX}\\right).$$\n\nIf $M_X(t)$ is defined for all values of $t$ in some interval about 0, then $M_X(t)$ is called the moment-generating function (MGF) of $X$.\n:::\n\n:::{.callout-note}\nWhen we are working with multiple random variables, it is common to use a subscript to denote the random variable we are referencing.  For example, $F_X$ may represent the CDF of the random variable $X$, $f_Y$ denote the density function of the random variable $Y$, and $M_Z(t)$ denote the MGF of the random variable $Z$.\n:::\n\nFirst, we note that the MGF is a function, but not a function of the random variable.  That is, $M_X(t)$ is not a random variable since it is the result of taking the expected value of a random variable.  Second, the definition does not guarantee the existence of the MGF for any particular random variable.  That is, there are distributions for which the MGF is not defined.  The power of the MGF is summarized in @thm-mgf-properties.\n\n:::{@thm-mgf-properties}\n## Properties of the Moment-Generating Function\nLet $X$ be a random variable with moment-generating function $M_X(t).$  Then, we have that\n\n  1. $E\\left(X^k\\right) = M_X^{(k)}(0)$ for all integers $k$, where $M_X(k)(0)$ is the $k$-th derivative of $M_X(t)$ evaluated at $t = 0$.\n  2. The MGF uniquely defines the random variable.  That is, if two random variables have the same MGF, then those random variables have the same density function.\n:::\n\nConsider again our sample of $n$ diamonds; let's assume it is a random sample from the population.  Suppose we are interested in the total number of diamonds within this sample that have a \"princess\" cut.  Define the random variable\n\n$$Y_i = \\begin{cases} 1 & \\text{if i-th diamond has a princess cut} \\\\ 0 & \\text{otherwise}. \\end{cases}$$\n\nIf we are intersted in the total number of diamonds within the sample that have a princess cut, we are interested in the statistic $\\sum_{i=1}^{n} Y_i$.  We can use @thm-mgf-properties, together with our previous results about expectations to derive the sampling distribution of this statistic.\n\n:::{#exm-bernoulli-sum}\n## Sampling Distribution of a Sum of Bernoulli Random Variables\nLet $Y_1, Y_2, \\dotsc, Y_n$ be IID random variables from a Bernoulli distribution with probability $\\theta$.  Define \n\n$$Z = \\sum_{i=1}^{n} Y_i,$$\n\nthe total number of \"successes\" in the random sample.  Determine the sampling distribution of $Z$.\n:::\n\n:::{.solution}\nIn order to determine the distribution of $Z$, we find its MGF.\n\n$$\n\\begin{aligned}\n  M_Z(t)\n    &= E\\left(e^{tZ}\\right) \\\\\n    &= E\\left(e^{t\\sum_{i=1}^{n} Y_i}\\right) \\\\\n    &= E\\left(\\prod_{i=1}^{n} e^{tY_i}\\right),\n\\end{aligned}\n$$\n\nwhere the third line results from recognizing that the product of exponential terms is the exponential of the sum of the powers.  Now, applying @thm-product-expectation, we have\n\n$$\n\\begin{aligned}\n  M_Z(t)\n    &= \\prod_{i=1}^{n} E\\left(e^{tY_i}\\right) \\\\\n    &= \\prod_{i=1}^{n} M_{Y_i}(t) \\\\\n    &= \\prod_{i=1}^{n} M_{Y_1}(t)\n\\end{aligned}\n$$\n\nwhere the last line is the result of the random variables being identically distributed.  Since they have the same distribution, they must have the same MGF's; therefore, $M_{Y_i}(t) = M_{Y_1}(t)$ for each $i$.  Again, we are _not_ saying $Y_i = Y_1$; we are saying the moment-generating functions of these random variables is equivalent.\n\nConsulting a table to determine the MGF of a Bernoulli random variable, we have that\n\n$$M_{Y_1}(t) = (1 - \\theta) + \\theta e^t.$$\n\nThus, we have that\n\n$$\n\\begin{aligned}\n  M_Z(t)\n    &= \\prod_{i=1}^{n} M_{Y_1}(t) \\\\\n    &= \\left[M_{Y_1}(t)\\right]^n \\\\\n    &= \\left[(1 - \\theta) + \\theta e^{t}\\right]^n.\n\\end{aligned}\n$$\n\nBut, consulting a table of common distributions, we recognize $M_Z(t)$ as the moment-generating function of a Binomial distribution with parameters $n$ and $\\theta$.  Since moment-generating functions uniquely define a distribution when they exist, we have that $Z \\sim Bin(n, \\theta)$.\n:::\n\nThe next chapter will illustrate how we can capitalize on this information.  For this chapter, we simply note that we are able to characterize how the sum of Bernoulli random variables behaves.  We note that this sampling distribution depends on the unknown parameter $\\theta$ that also governs the underlying population.  In addition, it depends on the sample size.\n\n:::{.callout-tip}\n## Big Idea\nThe sampling distribution of a statistic depends on both the parameters from the underlying population as well as the sample size.\n:::\n\nWhile the previous example illustrates the process, we admit that it simply reiterates a fact that we already knew (and noted in @def-binomial-distribution).  The utility of the next result may be a bit more apparent.\n\n:::{#exm-normal-mean}\n## Sampling Distribution of the Sample Mean from a Normal Population\nLet $Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{IID}}{\\sim} N\\left(\\mu,\\sigma^2\\right)$.  Define \n\n$$\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i,$$\n\nthe sample mean.  Determine the sampling distribution of $\\bar{Y}$.\n:::\n\n:::{.solution}\nIn order to determine the distribution of $\\bar{Y}$, we find its MGF.\n\n$$\n\\begin{aligned}\n  M_{\\bar{Y}}(t)\n    &= E\\left(e^{t\\bar{Y}}\\right) \\\\\n    &= E\\left(e^{tn^{-1}\\sum_{i=1}^{n} Y_i}\\right) \\\\\n    &= E\\left(\\prod_{i=1}^{n} e^{tn^{-1}Y_i}\\right) \n\\end{aligned}\n$$\n\nwhere the third line results from recognizing that the product of exponential terms is the exponential of the sum of the powers.  Now, applying @thm-product-expectation, we have\n\n$$\n\\begin{aligned}\n  M_{\\bar{Y}}(t)\n    &= \\prod_{i=1}^{n} E\\left(e^{tn^{-1}Y_i}\\right) \\\\\n    &= \\prod_{i=1}^{n} M_{Y_i}(t/n) \\\\\n    &= \\prod_{i=1}^{n} M_{Y_1}(t/n)\n\\end{aligned}\n$$\n\nwhere the last line is the result of the random variables being identically distributed, and the second line makes use of the definition of the MGF, which can be evaluated at any value, including $t/n$.  Since they have the same distribution, they must have the same MGF's; therefore, $M_{Y_i}(t) = M_{Y_1}(t)$ for each $i$ and all $t$.  Again, we are _not_ saying $Y_i = Y_1$; we are saying the moment-generating functions of these random variables is equivalent.\n\nConsulting a table to determine the MGF of a Normal random variable, we have that\n\n$$M_{Y_1}(t) = e^{\\mu t + (1/2) \\sigma^2 t^2}.$$\n\nThus, we have that\n\n$$\n\\begin{aligned}\n  M_{\\bar{Y}}(t)\n    &= \\prod_{i=1}^{n} M_{Y_1}(t/n) \\\\\n    &= \\left[M_{Y_1}(t/n)\\right]^n \\\\\n    &= \\left[e^{\\mu t/n + (1/2) \\sigma^2 t^2/n^2}\\right]^n \\\\\n    &= e^{\\mu t + (1/2)(\\sigma^2 / n) t^2}.\n\\end{aligned}\n$$\n\nBut, consulting a table of common distributions, we recognize $M_{\\bar{Y}}(t)$ as the moment-generating function of a Normal distribution with a mean of $\\mu$ and a variance of $\\sigma^2/n$.  Since moment-generating functions uniquely define a distribution when they exist, we have that $\\bar{Y} \\sim N\\left(\\mu, \\sigma^2/n\\right)$.\n:::\n\nWe note a difference between @exm-bernoulli-sum and @exm-normal-mean.  In @exm-bernoulli-sum, the statistic we examined did not estimate a parameter of interest.  While there is nothing wrong with examining the total number of diamonds in a sample, the statistic itself is dependent on the sample size --- we would expect a larger value with larger samples.  In @exm-normal-mean, however, the sample mean is a common estimate of the population mean.  It turns out the sampling distributions of most estimators (statistics chosen to estimate a parameter) tend to share similar characteristics.\n\n:::{.callout-note}\n## Common Characteristics of Sampling Distributions\nWhile not guaranteed, the sampling distribution of many statistics tend to have the following characteristics:\n\n  1. The sampling distribution is centered on the corresponding parameter of interest, or the center approaches the corresponding parameter as the sample size increases.\n  2. The spread of the sampling distribution is smaller than within the population, and the spread decreases as the sample size increases.\n  3. The shape of the sampling distribution differs from that of the underlying population, and the sampling distribution becomes more bell-shaped as the sample size increases.\n  \nOf the three characteristics above, the third is the one most likely to be broken.\n:::\n\nConsidering @exm-normal-mean, we see that all three characteristics hold.  First, we see (as we also saw in @exm-mean-xbar) that the expected value of the sample mean is the population mean.  When this occurs, we say the estimator is __unbiased__.\n\n:::{#def-unbiased}\n## Unbiased\nAn estimator (statistic) $\\widehat{\\theta}$ is said to be unbiased for the parameter $\\theta$ if\n\n$$E\\left(\\widehat{\\theta}\\right) = \\theta.$$\n:::\n\nWhile being unbiased is a good quality in an estimator, it is not required.  For example, the sample standard deviation is not an unbiased estimator of the population standard deviation, yet it is still a preferred estimator.\n\nIn @exm-normal-mean, we see that variance of the sample mean is smaller than the variance of the population by a factor of $n$; therefore, as the sample size increases, the variability of the sample mean decreases.  This implies that the sample mean of a large sample will tend not to stray as far from the population mean as that of a smaller sample.  This is where the notion of \"more data is better\" comes from.\n\n:::{.callout-tip}\n## Big Idea\nLarger samples result in more reliable statistics.\n:::\n\nFinally, we see in @exm-normal-mean that the sampling distribution of the sample mean is a Normal distribution, which is bell-shaped.  Again, being bell-shaped is not necessarily more advantageous than any other distribution, but it reinforces the idea that the statistic tends to be near the parameter of interest across repeated sampling.\n\n:::{.callout-warning}\nRemember, these discussions are about the distribution of a statistic across repeated samples, and so they apply prior to collecting data.  Once we have a sample, the statistic does not have a distribution.\n:::\n\n@exm-normal-mean is a really nice result because it tells us the behavior of a popular statistic; unfortunately, it only applies to a sample from a population which follows a Normal distribution.  More, it only applies when the population variance is known, which rarely happens in practice.  @thm-students-theorem generalizes the results to the case when the population variance is unknown.\n\n:::{#thm-students-theorem}\n## Student's Theorem\nLet $Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{IID}}{\\sim} N\\left(\\mu,\\sigma^2\\right)$.  Define \n\n$$\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i$$\n\nto be the sample mean and\n\n$$S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(Y_i - \\bar{Y}\\right)^2$$\n\nto be the sample variance.  Then, \n\n$$\\frac{\\sqrt{n}\\left(\\bar{Y} - \\mu\\right)}{S} \\sim t_{n-1}.$$\n:::\n\nTo see the impact of estimating the population variance, we recognize that @exm-normal-mean gave us that when the population variance is known\n\n$$\\bar{Y} \\sim N\\left(\\mu, \\sigma^2/n\\right),$$\n\nwhich can be rewritten as\n\n$$\\frac{\\sqrt{n}\\left(\\bar{Y} - \\mu\\right)}{\\sigma} \\sim N(0, 1).$$\n\nTherefore, when the population variance is estimated (using the typical sample variance), we have that the sampling distribution of this standardized ratio follows a t-distribution instead of a Standard Normal distribution.\n\n@thm-students-theorem highlights that we often characterize the distribution of some \"standardized statistic\" (instead of the statistic that estimates the parameter directly).  Exact results like @exm-normal-mean and @thm-students-theorem are quite rare.  It is more common for us to rely on approximations to the sampling distribution, the most famous of which is the Central Limit Theorem.\n\n:::{#thm-clt}\n## Central Limit Theorem (CLT)\nLet $X_1, X_2, \\dotsc, X_n$ be IID random variables such that $E\\left(X_i\\right) = \\mu$ and $Var\\left(X_i\\right) = \\sigma^2$ for all $i$.  As $n$ approaches infinity, the ratio\n\n$$\\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S}$$\n\nbehaves like a Standard Normal random variable.  That is,\n\n$$Pr\\left(\\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} \\leq q\\right) \\rightarrow Pr(Z \\leq q) \\qquad \\text{as } n \\rightarrow \\infty$$\n\nwhere $q$ is any real number, $Z \\sim N(0, 1)$, $\\bar{X}$ is the sample mean and $S^2$ is the sample standard deviation.\n:::\n\n:::{.callout-note}\n\"The\" Central Limit Theorem is a misnomer; there are actually several Central Limit Theorems which differ in their assumptions.  The one most commonly presented in texts uses the population standard deviation $\\sigma$ instead of the sample standard deviation $S$ as we have presented it.  The more common presentation is easier to prove, but it is far less useful in practice (as the population variance is rarely known).  The proof of the version we have presented is beyond the scope of the course but is more useful in practice.\n:::\n\nKnown as a \"limit\" (or \"asymptotic\") result, @thm-clt provides an approximation to the sampling distribution.  That is, the CLT states that as the sample size gets large, the Standard Normal distribution is a good approximation to the true sampling distribution of this standardized statistic.  Of course, that begs the question, \"how good is the approximation?\" as well as \"how large of a sample is large enough?\"  While we can never address these questions with certainty, there are some graphical techniques for assessing these questions in practice.\n\nThe huge draw of the CLT is that it applies under vary weak conditions --- the underlying population has a finite mean and variance.  For nearly any population, we have an approximation for the sampling distribution of the sample mean.  \n\n:::{.callout-note}\nThe above methods describe the actual sampling distribution.  Of course, because these describe how a statistic behaves across repeated samples, this is not something we get to observe directly.  Instead, the sampling distribution must be modeled.  This is often done by replacing the parameters in the sampling distribution with the corresponding estimates from the sample.  Therefore, the _model_ for the sampling distribution based on a given sample is really capturing the shape and spread of the sampling distribution.\n:::\n\n\n## Bootstrapping\nIn the above sections, we have discussed analytical methods (both exact and approximation through limit theorems) for the sampling distribution.  Given a set of data, we can also construct a model for the sampling distribution empirically.  As there is no single Central Limit Theorem, there is no single bootstrapping algorithm.  Instead, \"bootstrapping\" refers to the idea of using resampling methods to model a sampling distribution of a statistic, but the easiest algorithm is defined below.\n\n:::{#def-case-bootstrap}\n## Case-Resampling Bootstrap\nLet $Y_1, Y_2, \\dotsc, Y_n$ be a random sample from an underlying population, and let $\\theta$ represent a parameter of interest characterizing the underlying population.  Further, define $\\widehat{\\theta} = h(\\mathbf{Y})$ be a statistic which estimates the parameter.  The case-resampling bootstrap algorithm proceeds as follows:\n\n  1. Take a random sample, with replacement, from the set $\\left\\{Y_1, Y_2, \\dotsc, Y_n\\right\\}$ of size $n$.  Call these values $Y_1^*, Y_2^*, \\dotsc, Y_n^*$.  This is known as a bootstrap resample.\n  2. Compute $\\widehat{\\theta}^* = h\\left(\\mathbf{Y}^*\\right)$ and store this value.  This is known as a bootstrap statistic.\n  3. Repeat steps 1-2 $m$ times, for some large value of $m$ (say $m = 5000$).  Denote $\\widehat{\\theta}^*_j$ to be the bootstrap statistic from the $j$-th bootstrap resample.\n  \nThe empirical distribution of $\\widehat{\\theta}_1^*, \\widehat{\\theta}_2^*, \\dotsc, \\widehat{\\theta}_m^*$ will approximate the shape and spread of the sampling distribution of the statistic  $h(\\mathbf{Y})$.\n:::\n\nWhile the proof of the efficacy of a bootstrap algorithm is beyond the scope of this text, we can gain some intuition regarding the process.  Let's start by characterizing the distribution from which the algorithm resamples --- the distribution of the sample.  When we sample, with replacement, from the original sample, only $n$ values are possible $\\left(Y_1, Y_2, \\dotsc, Y_n\\right)$.  And, each value will be selected with probability $1/n$.  That is, we have that\n\n$$Pr\\left(Y_i^* = u\\right) = \\frac{1}{n} \\qquad u \\in \\left\\{Y_1, Y_2, \\dotsc, Y_n\\right\\}.$$\n\nThe mean of this distribution is represented by\n\n$$\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i,$$\n\nand the variance of this distribution is \n\n$$S^2_n = \\frac{1}{n} \\sum_{i=1}^{n} \\left(Y_i - \\bar{Y}\\right)^2.$$\n\nWe divide by $n$ instead of $n - 1$ because since we are treating the original sample as a population from which to be sampled, we rely on the formulas from @sec-randomvariables.\n\nIf $\\widehat{\\theta} = \\bar{Y}$, then we have that the sampling distribution of $\\widehat{\\theta}^* = n^{-1}\\sum_{i=1}^{n} Y_i^*$ will have a mean of \n\n$$E\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_i^*\\right] = \\bar{Y}$$\n\nand a variance of \n\n$$Var\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_i^*\\right] = \\frac{S_n^2}{n}.$$\n\nThese are the direct application of @exm-mean-xbar and @thm-variance-independent-sum.  And, if $n$ is large enough, we would expect the resulting empirical distribution to approximate that of a Normal distribution as a result of the CLT.  This highlights that _models_ of the sampling distribution tend to have similar characteristics to that of sampling distributions.\n\n:::{.callout-note}\n## Common Characteristics of Models for Sampling Distributions\nWhile not guaranteed, the model of a sampling distribution of many statistics tend to have the following characteristics:\n\n  1. The model of the sampling distribution is centered on the statistic from the original sample, or the center of the model approaches the statistic from the original sample as the sample size increases.\n  2. The spread of the model of the sampling distribution is smaller than within the sample, and the spread decreases as the sample size increases.\n  3. The shape of the model of the sampling distribution differs from that of the sample, and the model of the sampling distribution becomes more bell-shaped as the sample size increases.\n  \nOf the three characteristics above, the third is the one most likely to be broken.\n:::\n\n\n## Application\nConsider the following research objective:\n\n  > Estimate the cost (in US dollars) of a diamond for sale in the United States.\n  \nAs stated, this is an ill-posed objective as it is not centered on a parameter.  We might refine it to be\n\n  > Estimate the average cost (in US dollars) of a diamond for sale in the United States.\n  \nWe have a large ($n = `r nrow(ggplot2::diamonds)`$) sample of diamonds at our disposal that can be used to address this research objective.  A plot of the sample is shown in @fig-samplingdistributions-sample.\n\n```{r}\n#| label: fig-samplingdistributions-sample\n#| fig-cap: Distribution of the cost of a diamond sold in the United States.\n#| fig-alt: Histogram of the cost of a diamond; the distribution is skewed right.\n\ndiamonds |>\n  drop_na(price) |>\n  ggplot(mapping = aes(x = price)) +\n  geom_histogram(binwidth = 500,\n                 fill = 'grey75', \n                 color = 'black') +\n  labs(y = 'Number of Diamonds',\n       x = 'Cost of Diamond (US Dollars)')\n```\n\nUsing the sample, we conduct 5000 bootstrap replications, each time computing the sample mean.  The bootstrap sampling distribution is shown in @fig-samplingdistributions-bootstrap.  We have overlayed the model suggested by the CLT as well.  Observe that even though the sample was skewed to the right, the model for the sampling distribution is bell-shaped.  The center of our model is the observed sample mean of `r round(mean(diamonds$price))`, and the spread of the sampling distribution is much smaller than that observed in the sample.  With a large sample size, we see that the empirical model of the sampling distribution is very similar to that suggested by the CLT.\n\n:::{.callout-note}\nBootstrapping can be used to qualitatively assess whether the CLT is appropriate for a particular sample.  Of course, if we have gone through the effort of constructing an empirical model, we would likely rely on the empirical model.\n:::\n\n@fig-samplingdistributions-bootstrap does not include the results from @thm-students-theorem; the rationale for excluding this result is that a quick glance at @fig-samplingdistributions-sample is enough to convince us that the underlying population does not follow a Normal distribution; therefore, those results are inappropriate.\n\n```{r}\n#| label: fig-samplingdistributions-bootstrap\n#| fig-cap: Bootstrap sampling distribution of the sample mean cost of a diamond based on the available data.  5000 bootstrap replicates were used.\n#| fig-alt: Histogram of bootstrap statistics with a Normal distribution overlayed.\n\nset.seed(20230707) \n\nsamp.distn <- \n  diamonds |>\n  drop_na(price) |>\n  modelr::bootstrap(n = 5000) |>\n  mutate(xbar = map_dbl(strap, ~ mean(as_tibble(.x)$price)))\n\nggplot(data = samp.distn,\n       mapping = aes(x = xbar)) +\n  geom_histogram(mapping = aes(y = after_stat(density)),\n                 binwidth = 2, \n                 color = 'black',\n                 fill = 'grey75') +\n  geom_function(fun = dnorm, \n                args = list(mean = mean(diamonds$price),\n                            sd = sd(diamonds$price)/sqrt(nrow(diamonds))),\n                color = 'blue') +\n  labs(y = NULL,\n       x = 'Sample Mean Cost of 53940 Diamonds (US Dollars)') +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n```\n\nThe next chapter considers ways of using the above tools to perform inference.","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["mystyles.css"],"output-file":"03-samplingdistributions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","bibliography":["refs382notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"03-samplingdistributions.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["refs382notes.bib","packages.bib"],"comments":{"hypothesis":false},"fig-cap-location":"bottom","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}