{
  "hash": "b15fc53413c33da5263ef1f68eea3883",
  "result": {
    "markdown": "# More on the Classical Model {#sec-normality}\n\n\n\n\n\n\n\n\n\nIn the previous chapter, we introduced the classical simple linear regression model (@def-classical-simple-regression).  For a sample $\\left(Y_1, x_1\\right), \\left(Y_2, x_2\\right), \\dotsc, \\left(Y_n, x_n\\right)$, the distributional model for the response under the classical model is\n\n$$Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{Ind}}{\\sim} N\\left(\\beta_0 + \\beta_1 x_i, \\sigma^2\\right)$$\n\nwhere $\\beta_0$, $\\beta_1$, and $\\sigma^2$ are unknown parameters.  As we have seen, this embeds four conditions on the distribution --- that the mean is correctly specified, that the observations are independent of one another, that the variance is constant, and that the Normal distribution is appropriate.  While each of these plays a role in determining the sampling distribution of the parameter estimates, the impact of the Normality assumption is unique.  The moment we remove the assumption of Normality, the sampling distribution is no longer tractable analytically.  \n\nIn this chapter, we examine some additional results associated with the Normal distribution that are instrumental in classical theory of the linear model.\n\n\n## Linear Combinations\nIn general, combining random variables does not yield a predictable result; the Normal distribution is a notable exception.  The following theorem builds on an example seen earlier in the text.\n\n:::{#thm-normal-linear-combination}\n## Linear Combination of Independent Normal Random Variables\nLet $Y_1, Y_2, \\dotsc, Y_n$ be independent random variables such that\n\n$$Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{Ind}}{\\sim} N\\left(\\mu_i, \\sigma^2_i\\right).$$\n\nLet $a_1, a_2, \\dotsc, a_n \\in \\mathbb{R}$ be known constants.  Then,\n\n$$\\sum_{i=1}^{n} a_i Y_i \\sim N\\left(\\sum_{i=1}^{n} a_i \\mu_i, \\sum_{i=1}^{n} a_i \\sigma^2_i\\right).$$\n:::\n\n:::{.proof}\nLet $Z = \\sum_{i=1}^{n} a_i Y_i$.  Then, observe that\n\n$$\n\\begin{aligned}\n  M_Z(t) \n    &= E\\left(e^{tZ}\\right) \\\\\n    &= E\\left(e^{t \\sum_{i=1}^{n} a_i Y_i}\\right) \\\\\n    &= E\\left(\\prod_{i=1}^{n} e^{ta_i Y_i}\\right) \\\\\n    &= \\prod_{i=1}^{n} E\\left(e^{t a_i Y_i}\\right),\n\\end{aligned}\n$$\n\nwhere the last line is a result of the random variables being independent of one another.  Note that we do not get a further simplification here because we are _not_ assuming that the random variables are identically distributed.  Looking up the form of the moment generating function for a Normal random variable and substituting that here, we have\n\n$$\n\\begin{aligned}\n  M_Z(t) \n    &= \\prod_{i=1}^{n} E\\left(e^{t a_i Y_i}\\right) \\\\\n    &= \\prod_{i=1}^{n} e^{\\mu_i \\left(t a_i\\right) + \\sigma_i^2 \\left(t a_i\\right)^2 / 2} \\\\\n    &= \\exp\\left\\{\\sum_{i=1}^{n} \\mu_i t a_i + \\frac{1}{2} \\sum_{i=1}^{n} \\sigma^2 t^2 a_i^2\\right\\} \\\\\n    &= \\exp\\left\\{t \\left(\\sum_{i=1}^{n} a_i \\mu_i\\right) + \\frac{t^2}{2} \\left(\\sum_{i=1}^{n} a_i^2 \\sigma^2\\right)\\right\\}.\n\\end{aligned}\n$$\n\nWe now recognize this as the form of an MGF of a Normal distribution with a mean of $\\sum_{i=1}^{n} a_i \\mu_i$ and a variance of $\\sum_{i=1}^{n} a_i^2 \\sigma^2$.  By the uniqueness of MGF's, we have the result.\n:::\n\nWhy is @thm-normal-linear-combination so important?  It turns out that many of the statistics of interest that result from applying least squares are linear combinations of the response.  And, under the classical simple linear regression model, these responses are independent variates from a Normal distribution!\n\nWe have already seen one of these results in an earlier chapter when examining the sampling distribution of the sample mean from a Normal distribution.  \n\n:::{#exm-slope-linear-combination}\n## Form of Slope from Simple Linear Regression\nRecall that the least squares estimate for the slope in a simple linear regression model is given by \n\n$$\\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(Y_i - \\bar{Y}\\right)}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2}.$$\n\nShow that this can be written as a linear combination of the responses.\n:::\n\n:::{.solution}\nConsider expanding the numerator to observe that\n\n$$\n\\begin{aligned}\n  \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) \\left(Y_i - \\bar{Y}\\right)\n    &= \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) Y_i - \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) \\bar{Y} \\\\\n    &= \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) Y_i - n\\bar{x}\\bar{Y} + n\\bar{x}\\bar{Y} \\\\\n    &= \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) Y_i.\n\\end{aligned}\n$$\n\nNow, substituting this into the definition of $\\widehat{\\beta}_1$, we are able to rewrite the least squares estimator of the slope as\n\n$$\n\\begin{aligned}\n  \\widehat{\\beta}_1\n    &= \\frac{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(Y_i - \\bar{Y}\\right)}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2} \\\\\n    &= \\frac{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) Y_i}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2}. \n\\end{aligned}\n$$\n\nRecognizing that the denominator is a constant with respect to the sum in the numerator, we can define\n\n$$a_i = \\frac{x_i - \\bar{x}}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2}.$$\n\nThen, we have that\n\n$$\\widehat{\\beta}_1 = \\sum_{i=1}^{n} a_i Y_i.$$\n:::\n\nThe real power of recognizing that the least squares estimate of the slope is a linear combination of the responses is that we can apply @thm-normal-linear-combination.  Immediately, we have the sampling distribution of the least squares estimator of the slope:\n\n$$\\widehat{\\beta}_1 \\sim N\\left(\\sum_{i=1}^{n} a_i \\left(\\beta_0 + \\beta_1 x_i\\right), \\sigma^2 \\sum_{i=1}^{n} a_i^2\\right)$$\n\nwhere $a_i$ was defined in the solution to @exm-slope-linear-combination.  This reduces to\n\n$$\\widehat{\\beta}_1 \\sim N\\left(\\beta_1, \\sigma^2 \\sum_{i=1}^{n} a_i^2\\right).$$\n\n:::{.callout-note}\nNotice that since $\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) q = 0$ for any constant $q$, we have that $\\sum_{i=1}^{n} a_i = 0$ and $\\sum_{i=1}^{n} a_ix_i = 1$.  Further, we have that\n\n$$\\sum_{i=1}^{n} a_i^2 = \\frac{1}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2}.$$\n:::\n\nWe know from probability if $X \\sim N\\left(\\mu, \\sigma^2\\right)$, then $(X - \\mu)/\\sigma) \\sim N(0, 1)$.  So, it would seem that @exm-slope-linear-combination, and the resulting discussion, would suggest that\n\n$$\\frac{\\widehat{\\beta}_1 - \\beta_1}{\\sqrt{\\sigma^2 \\sum_{i=1}^{n} a_i^2}} \\sim N(0, 1),$$\n\nbut in the previous chapter, we suggested using the t-distribution.  The difference is that in the above, it is assumed that $\\sigma^2$ is known.  Estimating the parameter $\\sigma^2$ impacts the sampling distribution.  \n\n:::{.callout-tip}\n## Big Idea\nWhen the sampling distribution of an estimator depends on unknown nuisance parameters, the estimation of those nuisance parameters impacts the sampling distribution of the estimator.\n:::\n\n\n## Results for Squares\nThe previous section considered a linear combination of Normal random variables.  In this section, we explore a related result.\n\n:::{#thm-normal-chi-square}\n## Sum of Squared Normal Random Variables\nLet $Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{IID}}{\\sim} N(0, 1)$.  Then,\n\n$$\\sum_{i=1}^{n} Y_i^2 \\sim \\chi^2_n.$$\n:::\n\nAs with previous proofs, this is best addressed through a moment generating function argument.\n\n:::{.proof}\nLet $Z = \\sum_{i=1}^{n} Y_i^2$; then, observe that\n\n$$\n\\begin{aligned}\n  M_Z(t)\n    &= E\\left(e^{tZ}\\right) \\\\\n    &= E\\left(e^{t \\sum_{i=1}^{n} Y_i^2}\\right) \\\\\n    &= E\\left(\\prod_{i=1}^{n} e^{t Y_i^2}\\right) \\\\\n    &= \\prod_{i=1}^{n} E\\left(e^{t Y_i^2}\\right) \\\\\n    &= \\left[M_{Y_1^2}(t)\\right]^n\n\\end{aligned}\n$$\n\nwhere line 4 is a result of independence and line 5 is a result of the random variables being identically distributed.  Unfortunately, we do not know the MGF of $Y_1^2$; so, we must first determine its distribution before proceeding.  This is best done through a transformation.  Observe that\n\n$$\n\\begin{aligned}\n  F_{Y^2}(y) \n    &= Pr\\left(Y^2 \\leq y\\right) \\\\\n    &= Pr\\left(-\\sqrt{y} \\leq Y \\leq \\sqrt{y}\\right) \\\\\n    &= F_Y(\\sqrt{y}) - F_Y(-\\sqrt{y}).\n\\end{aligned}\n$$\n\nAnd, since $Y \\sim N(0, 1)$, we know the density function of $Y$; therefore,\n\n$$\n\\begin{aligned}\n  f_{Y^2}(y) \n    &= \\frac{\\partial}{\\partial y} F_{Y^2}(y) \\\\\n    &= \\frac{\\partial}{\\partial y} F_Y(\\sqrt{y}) - \\frac{\\partial}{\\partial y} F_Y(-\\sqrt{y}) \\\\\n    &= f_Y(\\sqrt{y}) (1/2) y^{-1/2} + f_Y(-\\sqrt{y}) (1/2) y^{-1/2} \\\\\n    &= y^{-1/2} f_Y(\\sqrt{y})\n\\end{aligned}\n$$\n\nwhere the last line is based on the symmetry of the Standard Normal distribution.  Thus, we have that the density of $Y^2$ is given by\n\n$$\\frac{1}{\\sqrt{2\\pi}} y^{-1/2} e^{-\\frac{1}{2} y^2} = \\frac{1}{2^{1/2} \\Gamma(1/2)} y^{1/2 - 1} e^{-(1/2) y^2},$$\n\nwhere we have made use of the fact that $\\Gamma(1/2) = \\sqrt{\\pi}$.  We recognize this as the density function of a $Gamma(1/2, 2)$ or equivalently a $\\chi^2_1$ distribution.  We therefore use the MGF for this distribution to continue our previous derivation, giving\n\n$$\n\\begin{aligned}\n  M_Z(t)\n    &= \\left[M_{Y_1^2}(t)\\right]^n \\\\\n    &= \\left[(1 - 2t)^{-1/2}\\right]^n \\\\\n    &= (1 - 2t)^{-n/2}\n\\end{aligned}\n$$\n\nwhich we recognize as the MGF of a $\\chi^2_n$ distribution.  By the uniqueness of moment generating functions, we have the result.\n:::\n\n\n## Relation to the F Distribution\nWhile this chapter has been focused on the Normal distribution, its presence in statistical analysis is often associated with other distributions (notably, the t-distribution, the Chi-Squared distribution, and the F-distribution).  We have already seen one way in which the Normal distribution is associated with the Chi-Squared distribution.  In this section, we establish the link between Chi-Squared distributions and the F-distribution.\n\n:::{#thm-F-distribution}\n## Relationship Between Chi-Squared Distribution and the F-Distribution\nLet $U$ and $V$ be independent Chi-Squared random variables with $\\nu$ and $\\eta$ degrees of freedom, respectively.  Then, $\\frac{U/\\nu}{V/\\eta} \\sim F_{\\nu, \\eta}$.\n:::\n\nUnlike previous results that relied on the MGF, this theorem requires that we perform a transformation.\n\n:::{.proof}\nLet $X = U/V$ and let $Y = V$.  We will find the joint density of $X$ and $Y$, and then integrate out $Y$ to derive the density of $X$.  Observe that\n\n$$\n\\begin{aligned}\n  F_{X,Y}(x,y)\n    &= Pr(X \\leq x, Y \\leq y) \\\\\n    &= Pr(U/V \\leq x, V \\leq y) \\\\\n    &= Pr(U \\leq Vx, V \\leq y) \\\\\n    &= \\int_{0}^{y} \\int_{0}^{vx} f_{U,V}(u,v) du dv.\n\\end{aligned}\n$$\n\nSince $U$ and $V$ are independent random variables, their joint density is easily given.  However, we are particularly interested in the joint density of $X$ and $Y$.  Therefore, we need only consider the derivative.  Observe that\n\n$$\n\\begin{aligned}\n  f_{X,Y}(x,y) \n    &= \\frac{\\partial^2}{\\partial x \\partial y} F_{X,Y}(x,y) \\\\\n    &= f_{U,V}(yx, y) y \\\\\n    &= \\frac{1}{2^{\\nu/2} \\Gamma(\\nu/2)} (yx)^{\\nu/2-1} e^{-yx/2} \\frac{1}{2^{\\eta/2} \\Gamma(\\eta/2)} (y)^{\\eta/2-1} e^{-y/2} y.\n\\end{aligned}\n$$\n\nTherefore, we have the joint density of $X$ and $Y$.  We now integrate out $y$ to obtain the marginal density of $X$.  We have\n\n$$\n\\begin{aligned}\n  f_X(x) \n    &= \\int_{0}^{\\infty} f_{X,Y}(x,y) dy \\\\\n    &= x^{\\nu/2 - 1} \\frac{1}{\\Gamma(\\nu/2)\\Gamma(\\eta/2) 2^{(\\eta + \\nu)/2}} \\int_{0}^{\\infty} y^{(\\nu + \\eta)/2 - 1} e^{-y (1 + x)/2} dy \\\\\n    &= \\frac{\\left(x^{\\nu/2 - 1}\\right) \\Gamma((\\nu+\\eta)/2)}{\\Gamma(\\nu/2)\\Gamma(\\eta/2) 2^{(\\eta + \\nu)/2} ((1+x)/2)^{(\\nu + \\eta)/2}} \\int_{0}^{\\infty} \\frac{((1+x)/2)^{(\\nu + \\eta)/2}}{\\Gamma((\\nu + \\eta)/2)} y^{(\\nu + \\eta)/2 - 1} e^{-y (1 + x)/2} dy.\n\\end{aligned}\n$$\n\nWe recognize that the integral is the density function of a Gamma distribution with shape parameter $(\\nu + \\eta)/2$ and rate parameter $(1 + x)/2$.  Since we are integrating over the entire support, the integral will go to 1.  This means the density of $X$ is given by\n\n$$\\frac{\\left(x^{\\nu/2 - 1}\\right) \\Gamma((\\nu+\\eta)/2)}{\\Gamma(\\nu/2)\\Gamma(\\eta/2) 2^{(\\eta + \\nu)/2} ((1+x)/2)^{(\\nu + \\eta)/2}}.$$\n\nWe can rewrite this density to be\n\n$$\\frac{\\Gamma((\\nu + \\eta)/2)}{\\Gamma(\\nu/2)\\Gamma(\\eta/2)} x^{\\nu/2 - 1} \\left(1 + x\\right)^{-(\\nu + \\eta)/2}.$$\n\nTherefore, the density of $Z = \\frac{U/\\nu}{V/\\eta} = (U/V)(\\eta/\\nu)$ is given by\n\n$$\n\\begin{aligned}\n  f_Z(z)\n    &= \\frac{\\partial}{\\partial z} F_Z(z) \\\\\n    &= \\frac{\\partial}{\\partial z} Pr(U/V \\leq (\\nu/\\eta) z) \\\\\n    &= \\frac{\\partial}{\\partial z} F_X((\\nu/\\eta) z) \\\\\n    &= (\\nu/\\eta) f_X((\\nu/\\eta) z).\n\\end{aligned}\n$$\n\nFinally, substituting in our expression for the density of $X$, we have\n\n$$\\frac{\\Gamma((\\nu + \\eta)/2)}{\\Gamma(\\nu/2)\\Gamma(\\eta/2)} \\left(\\frac{\\nu}{\\eta}\\right)^{\\nu/2} x^{\\nu/2 - 1} \\left(1 + x\\frac{\\nu}{\\eta}\\right)^{-(\\nu + \\eta)/2},$$\n\nwhich we recognize as the density of the F-distribution with $\\nu$ numerator and $\\eta$ denominator degrees of freedom.\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}