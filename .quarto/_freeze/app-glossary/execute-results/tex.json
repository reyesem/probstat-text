{
  "hash": "ecc40cc712ae072833285ec78aa465a2",
  "result": {
    "engine": "knitr",
    "markdown": "# Glossary\n\n\n\n\n\n\n\\providecommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\providecommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n\n\n\n\nThe following key terms were defined in the text; each term is presented with a link to where the term was first encountered in the text.\n\n\n\nAxioms of Probability (@def-axioms) \n: Let $\\mathcal{S}$ be the sample space of a random process.  Suppose that to each event $A$ within $\\mathcal{S}$, a number denoted by $Pr(A)$ is associated with $A$.  If the map $Pr(\\cdot)$ satisfies the following three axioms, then it is called a __probability__:\n\n  1. $Pr(A) \\geq 0$\n  2. $Pr(\\mathcal{S}) = 1$\n  3. If $\\left\\{A_1, A_2, \\dotsc\\right\\}$ is a sequence of mutually exclusive events in $\\mathcal{S}$, then\n  \n  $$Pr\\left(\\bigcup_{i = 1}^{\\infty} A_i\\right) = \\sum_{i = 1}^{\\infty} Pr\\left(A_i\\right).$$\n  \n\n\n$Pr(A)$ is said to be the \"probability of $A$\" or the \"probability $A$ occurs.\"\n\nBernoulli Distribution (@def-bernoulli-distribution) \n: Let $X$ be a discrete random variable taking the value 0 or 1.  $X$ is said to have a Bernoulli distribution with density\n\n$$f(x) = \\theta^x (1 - \\theta)^{1 - x} \\qquad x \\in \\{0, 1\\},$$\n\nwhere $0 < \\theta < 1$ is the probability that $X$ takes the value 1.\n\n- $E(X) = \\theta$\n- $Var(X) = \\theta(1 - \\theta)$\n\nWe write $X \\sim Ber(\\theta)$, which is read \"X follows a Bernoulli distribution with probability $\\theta$.\"\n\nBinomial Distribution (@def-binomial-distribution) \n: Let $X$ be a discrete random variable taking integer values between 0 and $n$, inclusive.  $X$ is said to have a Binomial distribution with density\n\n$$f(x) = \\begin{pmatrix}n \\\\ x\\end{pmatrix} \\theta^x (1 - \\theta)^{1 - x} \\qquad x \\in \\{0, 1, \\dotsc, n\\},$$\n\nwhere $0 < \\theta < 1$ is the probability of a success on an individual trial.\n\n- $E(X) = n\\theta$\n- $Var(X) = n\\theta(1 - \\theta)$\n\nWe write $X \\sim Bin(n, \\theta)$, which is read \"X follows a Binomial distribution with parameters $n$ and $\\theta$.\"\n\nCase-Resampling Bootstrap (@def-case-bootstrap) \n: Let $Y_1, Y_2, \\dotsc, Y_n$ be a random sample from an underlying population, and let $\\theta$ represent a parameter of interest characterizing the underlying population.  Further, define $\\widehat{\\theta} = h(\\mathbf{Y})$ be a statistic which estimates the parameter.  The case-resampling bootstrap algorithm proceeds as follows:\n\n  1. Take a random sample, with replacement, from the set $\\left\\{Y_1, Y_2, \\dotsc, Y_n\\right\\}$ of size $n$.  Call these values $Y_1^*, Y_2^*, \\dotsc, Y_n^*$.  This is known as a bootstrap resample.\n  2. Compute $\\widehat{\\theta}^* = h\\left(\\mathbf{Y}^*\\right)$ and store this value.  This is known as a bootstrap statistic.\n  3. Repeat steps 1-2 $m$ times, for some large value of $m$ (say $m = 5000$).  Denote $\\widehat{\\theta}^*_j$ to be the bootstrap statistic from the $j$-th bootstrap resample.\n  \nThe empirical distribution of $\\widehat{\\theta}_1^*, \\widehat{\\theta}_2^*, \\dotsc, \\widehat{\\theta}_m^*$ will approximate the shape and spread of the sampling distribution of the statistic  $h(\\mathbf{Y})$.\n\nChi-Square Distribution (@def-chi-square-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a Chi-Square distribution if the density is given by\n\n$$f(x) = \\frac{1}{2^{\\nu/2}\\Gamma (\\nu/2)}\\;x^{\\nu/2-1}e^{-x/2} \\qquad x > 0,$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim \\chi^2_{\\nu}$, which is read \"X follows a Chi-Square distribution with $\\nu$ degrees of freedom.\"  The Chi-Square distribution is a special case of the Gamma distribution where $\\alpha = \\nu/2$ and $\\beta = 2$.\n\nClassical Regression Model (@def-multiple-model) \n: Let $Y_i$ represent the response for the $i$-th observation, and let $\\bm{x}_i$ represent the vector of observed predictors for the $i$-th observation in a sample of $n$ units, including the intercept term.  Under the classical linear regression model, the distributional model for the response among the population is given by\n\n$$Y_i \\stackrel{\\text{Ind}}{\\sim} N\\left(\\bm{x}_i^\\top \\bs{\\beta}, \\sigma^2\\right).$$\n\nClassical Simple Linear Regression (@def-classical-simple-regression) \n: Let $\\left(Y_1, x_1\\right), \\left(Y_2, x_2\\right), \\dotsc, \\left(Y_n, x_n\\right)$ be observations made on a sample of $n$ units.  Under the classical simple linear regression model, the distributional model for the response among the population is given by\n\n$$Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{Ind}}{\\sim} N\\left(\\beta_0 + \\beta_1 x_i, \\sigma^2\\right)$$\n\nwhere $\\beta_0$, $\\beta_1$, and $\\sigma^2$ are unknown parameters.\n\nConditional Density (@def-conditional-density) \n: Let $X$ and $Y$ be continuous random variables.  Then, the density function of $X$ given the value of $Y$, written as $f_{X \\mid Y}(x \\mid y)$ is given by\n\n$$f_{X \\mid Y}(x \\mid y) = \\frac{f_{X,Y}(x, y)}{f_Y(y)}$$\n\nwhere $f_{X,Y}(x,y)$ is the joint density function.\n\nConditional Probability (@def-conditional-probability) \n: Let $A$ and $B$ be events with non-zero probability.  Then, the probability that event $A$ occurs given that event $B$ occurs (written $A \\mid B$) is given by\n\n$$Pr(A \\mid B) = \\frac{Pr(A \\cap B)}{Pr(B)}.$$\n\nConfidence Interval (@def-confidence-interval) \n: Consider repeatedly taking samples $\\mathbf{Y}$ of size $n$ from a population characterized by the parameter $\\theta$.  The interval $\\left(h_1(\\mathbf{Y}), h_2(\\mathbf{Y})\\right)$ is said to be a $100c$% confidence interval if \n\n$$Pr\\left(h_1(\\mathbf{Y}) \\leq \\theta \\leq h_2(\\mathbf{Y})\\right) = c.$$\n\nContinuous and Discrete Random Variable (@def-rvtypes) \n: The random variable $X$ is said to be a discrete random variable if its corresponding support is countable.  The random variable $X$ is said to be a continuous random variable if the corresponding support is uncountable (such as an interval or a union of intervals on the real line).\n\nCovariance (@def-covariance) \n: Let $X$ and $Y$ be random variables.  The covariance of $X$ and $Y$, written $Cov(X, Y)$ is defined as\n\n$$Cov(X,Y) = E\\left[(X - E(X))(Y - E(Y))\\right] = E(XY) - E(X) E(Y).$$\n\nCumulative Distribution Function (CDF) (@def-cdf) \n: Let $X$ be a random variable; the cumulative distribution function (CDF) is defined as\n\n$$F(u) = Pr(X \\leq u).$$\n\nFor a continuous random variable, we have that\n\n$$F(u) = \\int_{-\\infty}^{u} f(x) dx$$\n\nimplying that the density function is the derivative of the CDF.  For a discrete random variable\n\n$$F(u) = \\sum_{x \\leq u} f(x).$$\n\nDensity Function (@def-density-function) \n: A density function $f$ relates the values in the support of a random variable with the probability of observing those values.  \n\nLet $X$ be a continuous random variable, then its density function $f$ is the function such that\n\n$$Pr(a \\leq X \\leq b) = \\int_a^b f(x) dx$$\n\nfor any real numbers $a$ and $b$ in the support.\n\nLet $X$ be a discrete random variable, then its density function $f$ is the function such that\n\n$$Pr(X = u) = f(u)$$\n\nfor any real number $u$ in the support.\n\nEvent (@def-event) \n: A subset of the sample space that is of particular interest.\n\nExpectation of a Function (@def-expectation) \n: Let $X$ be a random variable with density function $f$ over the support $\\mathcal{S}$, and let $g$ be a real-valued function.  Then, \n\n$$E\\left[g(X)\\right] = \\int_{\\mathcal{S}} g(x) f(x) dx$$\n\nfor continuous random variables and\n\n$$E\\left[g(X)\\right] = \\sum_{\\mathcal{S}} g(x) f(x)$$\n\nfor discrete random variables.\n\nExpectation of a Product of Independent Random Variables (@def-product-expectations) \n: Let $X_1, X_2, \\dotsc, X_n$ be independent random variables, then\n\n$$E\\left(\\prod_{i=1}^n X_i\\right) = \\prod_{i=1}^{n} E\\left(X_i\\right).$$\n\nExpected Value (Mean) (@def-mean) \n: Let $X$ be a random variable with density function $f$ defined over the support $\\mathcal{S}$.  The expected value of a random variable, also called the mean and denoted $E(X)$, is given by\n\n$$E(X) = \\int_{\\mathcal{S}} x f(x) dx$$\n\nfor continuous random variables and \n\n$$E(X) = \\sum_{\\mathcal{S}} x f(x)$$\n\nfor discrete random variables.\n\nF-Distribution (@def-f-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have an F-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma((r + s)/2)}{(\\Gamma(r/2) \\Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \\qquad x > 0,$$\n\nwhere $r,s > 0$ are the numerator and denominator degrees of freedom, respectively.\n\nWe write $X \\sim F_{r, s}$, which is read \"X has an F-distribution with r numerator degrees of freedom and s denominator degrees of freedom.\"\n\nFrequentist Interpretation of Probability (@def-frequentist-interpretation) \n: In this perspective, the probability of $A$ describes the long-run behavior of the event.  Specifically, consider repeating the random process $m$ times, and let $f(A)$ represent the number of times the event $A$ occurs out of those $m$ replications.  Then,\n\n$$Pr(A) = \\lim_{m \\rightarrow \\infty} \\frac{f(A)}{m}.$$\n\nGamma Distribution (@def-gamma-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a Gamma distribution if the density is given by\n\n$$f(x) = \\frac{1}{\\beta^{\\alpha} \\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x/\\beta} \\qquad x > 0,$$\n\nwhere $\\alpha > 0$ is the shape parameter and $\\beta > 0$ is the scale parameter.\n\n- $E(X) = \\alpha\\beta$\n- $Var(X) = \\alpha\\beta^2$\n\nWe write $X \\sim Gamma\\left(\\alpha, \\beta\\right)$, which is read \"X follows a Gamma distribution with shape $\\alpha$ and scale $\\beta$.\"  This short-hand implies the density above.  When $\\alpha = 1$, we refer to this as the Exponential distribution with scale $\\beta$.\n\nWe note that, in general, there is no closed form solution for $\\Gamma(\\alpha)$, but \n\n- $\\Gamma(\\alpha) = (\\alpha - 1) \\Gamma(\\alpha - 1)$\n- $\\Gamma(k) = (k - 1)!$ for non-negative integer $k$\n\nHierarchical Model (@def-hierarchical-model) \n: A distributional model for a response that is constructed in layers.  The top-most layer is conditional on components that are expressed in lower layers.\n\nIndependence (@def-independence) \n: Random variables $X_1, X_2, \\dotsc, X_n$ are said to be mutually independent (or just \"independent\") if and only if\n\n$$Pr\\left(X_1 \\in A_1, X_2 \\in A_2, \\dotsb, X_n \\in A_n\\right) = \\prod_{i=1}^{n} Pr\\left(X_i \\in A_i\\right),$$\n\nwhere $A_1, A_2, \\dotsc, A_n$ are arbitrary sets.\n\nJoint Density (@def-joint-density) \n: Let $X$ and $Y$ be continuous random variables.  Then, the joint density function of $X$ and $Y$, written $f_{X,Y}(x,y)$ is the function such that\n\n$$Pr(a < X < b, c < Y < d) = \\int_{c}^{d} \\int_{a}^{b} f_{X,Y}(x,y) dx dy.$$\n\nLocation-Scale Family (@def-location-scale) \n: For $a \\in \\mathbb{R}$ and $b > 0$, let $X = a + bZ$ for some random variable $Z$.  $X$ is said to be the location-scale family associated with the distribution of $Z$ with location parameter $a$ and scale parameter $b$.\n\nMethod of Distribution Functions (@def-method-of-distribution-functions) \n: Let $X$ be a continuous random variable with density $f$ and cumulative distribution function $F$.  Consider $Y = h(X)$.  The following process provides the density function $g$ of $Y$ by first finding its cumulative distribution function $G$.\n\n  1. Find the set $A$ for which $h(X) \\leq t$ if and only if $X \\in A$.\n  2. Recognize that $G(y) = Pr(Y \\leq y) = Pr\\left(h(X) \\leq y\\right) = Pr(X \\in A)$.\n  3. If interested in $g(y)$, note that $g(y) = \\frac{\\partial}{\\partial y} G(y)$.\n\nMethod of Least Squares (@def-least-squares) \n: The least squares estimates of the parameters $\\beta_0$ and $\\beta_1$ in a simple linear regression model are the values that minimize the quantity\n\n$$\\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2.$$\n\nThe estimates are often denoted $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$.\n\nMoment-Generating Function (MGF) (@def-mgf) \n: For a random variable $X$, let $M_X(t)$ be defined as\n\n$$M_X(t) = E\\left(e^{tX}\\right).$$\n\nIf $M_X(t)$ is defined for all values of $t$ in some interval about 0, then $M_X(t)$ is called the moment-generating function (MGF) of $X$.\n\nNormal (Gaussian) Distribution (@def-normal-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a Normal (or Guassian) distribution if the density is given by\n\n$$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\qquad -\\infty < x < \\infty,$$\n\nwhere $\\mu$ is any real number and $\\sigma^2 > 0$.  \n\n- $E(X) = \\mu$\n- $Var(X) = \\sigma^2$\n\nWe write $X \\sim N\\left(\\mu, \\sigma^2\\right)$, which is read \"X follows a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\"  This short-hand implies the density above.  When $\\mu = 0$ and $\\sigma^2 = 1$, this is referred to as the Standard Normal distribution.\n\nNull Distribution (@def-null-distribution) \n: Distribution of a statistic under a hypothesized value of the population parameter(s).\n\nOrder Statistic (@def-order-statistic) \n: Let $X_1, X_2, \\dotsc, X_n$ be a random sample of size $n$.  The $j$-th order statistic, denoted $X_{(j)}$ is the $j$-th smallest value in the sample.  Special cases include\n\n  - $X_{(1)}$, the sample minimum, and\n  - $X_{(n)}$, the sample maximum.\n\nP-value (@def-pvalue) \n: The probability, assuming the null hypothesis is true, that we would observe a statistic, by chance alone, as extreme or more so than that observed in the sample.\n\nParameter (@def-parameter) \n: Numeric quantity which summarizes the distribution of a variable within the _population_ of interest.  Generally denoted by Greek letters in statistical formulas.\n\nPercentile (@def-percentile) \n: Let $X$ be a random variable with density function $f$.  The $100k$ percentile is the value $q$ such that\n\n$$Pr(X \\leq q) = k.$$\n\nRandom Sample (@def-random-sample) \n: A random sample of size $n$ refers to a collection of $n$ random variables $X_1, X_2, \\dotsc, X_n$ such that the random variables are mutually independent, and the distribution of each random variable is identical.\n\nWe say $X_1, X_2, \\dotsc, X_n$ are independent and identically distributed, abbreviated IID.  We might also write this as $X_i \\stackrel{\\text{IID}}{\\sim} f$ for some density $f$.\n\nRandom Variable (@def-random-variable) \n: Let $\\mathcal{S}$ be the sample space corresponding to a random process; a random variable $X$ is a function mapping elements of the sample space to the real line.\n\nRandom variables represent a measurement that will be collected during the course of a study.  Random variables are typically represented by a capital letter.\n\nRegression (@def-regression) \n: Allowing the parameters characterizing the distribution of a random variable to depend, through some specified function, on the value of additional variables.\n\nSample Quantile (@def-sample-quantile) \n: Let $x_{(j)}$ denote the $j$-th observed order statistic in a sample.  Then, the $q$-th quantile of the sample is given by $x_{(j)}$ for $\\frac{j-1}{n} < q \\leq \\frac{j}{n}$ and $x_{(1)}$ if $q = 0$, for $j=1,2,\\dotsc,n$.\n\nSample Space (@def-sample-space) \n: The sample space for a random process is the collection of all possible results that we might observe.\n\nSampling Distribution (@def-sampling-distribution) \n: The distribution of a statistic across repeated samples.\n\nStatistic (@def-statistic) \n: A statistic is a numerical summary of a sample; it is a function of the data alone.  Prior to collecting data, a statistic is a function of the data to be collected.\n\nSubjective Interpretation of Probability (@def-subjective-interpretation) \n: In this perspective, the probability of $A$ describes the individual's uncertainty about event $A$.\n\nSupport (@def-support) \n: The support of a random variable is the set of all possible values the random variable can take.\n\nUnbiased (@def-unbiased) \n: An estimator (statistic) $\\widehat{\\theta}$ is said to be unbiased for the parameter $\\theta$ if\n\n$$E\\left(\\widehat{\\theta}\\right) = \\theta.$$\n\nVariance (@def-variance) \n: Let $X$ be a random variable with density function $f$ defined over the support $\\mathcal{S}$.  The variance of a random variable, denoted $Var(X)$, is given by\n\n$$Var(X) = E\\left[X - E(X)\\right]^2 = E\\left(X^2\\right) - E^2(X).$$\n\nIf we let $\\mu = E(X)$, then this is equivalent to\n\n$$\\int_{\\mathcal{S}} (x - \\mu)^2 f(x) dx$$\n\nfor continuous random variables and \n\n$$\\sum_{\\mathcal{S}} (x - \\mu)^2 f(x)$$\n\nfor discrete random variables.\n\nt-Distribution (@def-t-distribution) \n: Let $X$ be a continuous random variable.  $X$ is said to have a (standardized) t-distribution, sometimes called the Student's t-distribution, if the density is given by\n\n$$f(x) = \\frac{\\Gamma \\left(\\frac{\\nu+1}{2} \\right)} {\\sqrt{\\nu\\pi}\\,\\Gamma \\left(\\frac{\\nu}{2} \\right)} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}} \\qquad -\\infty < x < \\infty$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim t_{\\nu}$, which is read \"X follows a t-distribution with $\\nu$ degrees of freedom.\"\n",
    "supporting": [
      "app-glossary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}