{
  "hash": "d0e50c35a5f4578acc4a34d69821e150",
  "result": {
    "engine": "knitr",
    "markdown": "# Autocorrelation {#sec-autocorrelation}\n\n\n\n\n\n\n\\providecommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\providecommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n\n\n\nA critical assumption throughout the text has been that of independence.  For example, we might assume that \n\n$$Y_i = \\mu + \\varepsilon_i$$\n\nwhere the $\\varepsilon_i$ are independent random variables.  For a more concrete example, suppose we say that $\\mu = 0$ and $\\varepsilon \\sim N(0, 1)$, again, all independent.  And, suppose we take a sample of size 100.  @fig-time-series illustrates several possible samples, where the observations are made sequentially.  That is, @fig-time-series shows several time-series plots (plot of the observations over time).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The time-series plot for several random samples of size 100 taken from a Standard Normal distribution.  One sample is highlighted for illustration.  No visible patterns are present.](./images/fig-time-series-1.png){#fig-time-series width=80%}\n:::\n:::\n\n\nThere are no visible trends in the location or spread of the responses as we move across the graphic.  While any one time-series plot _may_ show some small trend, overall, across repeated samples, no trend is observed.  This lack of trend is the result of the independence between observations.  Any one observation does not help us predict the location of the next.\n\n\nIn contrast, suppose we continue to take $\\mu = 0$, but we say that $\\varepsilon_1 \\sim N(0, 1)$ and \n\n$$\\varepsilon_i \\mid \\varepsilon_{i-1} \\sim N\\left(\\varepsilon_{i-1}, 1\\right)$$\n\nfor $i = 2, 3, \\dotsc, n$.  Notice that the distribution of one error term depends on the value of the previous error.  We again consider taking a sample of size 100.  @fig-time-series-auto shows several time-series plots for this updated situation.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The time-series plot for several samples of size 100 taken sequentially.  The first observation is taken from a Standard Normal distribution; each subsequent observation is taken from a Normal distribution centered on the previous observation with a variance of 1.  One sample is highlighted for illustration.](./images/fig-time-series-auto-1.png){#fig-time-series-auto width=80%}\n:::\n:::\n\n\nUnlike @fig-time-series, we notice a clear trend in the location of the series highlighted in @fig-time-series-auto.  The location of the response tends to decrease over time.  The lack of independence is revealing itself in a trend in the location of the response over time.\n\nThis is known as auto-correlation.  While the specifics of this phenomena would be studied in a course on regression modeling, what we see is that the relationship between one observation and the next is time-dependent.  That is, observations close together in time (indices that are near one another) are related to one another, while observations further apart in time are less related to one another.\n\n:::{#exm-covariance-auto}\n## Covariance in Autocorrelation Model\nConsider the simple autocorrelation model\n\n$$Y_i = \\varepsilon_i$$\n\nwhere $\\varepsilon_1 \\sim N(0, 1)$ and \n\n$$\\varepsilon_i \\mid \\varepsilon_{i-1} \\sim N\\left(\\varepsilon_{i-1}, 1\\right)$$\n\nfor $i > 1$.  Determine the covariance between two observations.\n:::\n\n:::{.soln}\nWithout loss of generality, consider $Y_1$ and $Y_2$.  Observe that\n\n$$Cov\\left(Y_1, Y_2\\right) = Cov\\left(\\varepsilon_1, \\varepsilon_2\\right) = E\\left(\\varepsilon_{1} \\varepsilon_{2}\\right) - E\\left(\\varepsilon_{1}\\right)E\\left(\\varepsilon_{2}\\right).$$\n\nIn order to evaluate these expectations, we need an interim result: for any two random variables $X$ and $Y$, we have that $E(X) = E(E(X \\mid Y))$.  That is, in the inner expectation, we take the conditional expectation of $X$ given $Y$; the result will be only a function of the random variable $Y$.  In the outer expectation, we take the expectation with respect to $Y$.\n\nReturning to our problem at hand, we know that $E\\left(\\varepsilon_1\\right) = 0$.  Applying our latest result, we have\n\n$$\n\\begin{aligned}\n  E\\left(\\varepsilon_2\\right) \n    &= E\\left[E\\left(\\varepsilon_2 \\mid \\varepsilon_1\\right)\\right] \\\\\n    &= E\\left[\\varepsilon_1\\right] \\\\\n    &= 0.\n\\end{aligned}\n$$\n\nWe also have that\n\n$$\n\\begin{aligned}\n  E\\left(\\varepsilon_1 \\varepsilon_2\\right) \n    &= E\\left[E\\left(\\varepsilon_1 \\varepsilon_2 \\mid \\varepsilon_1\\right)\\right] \\\\\n    &= E\\left[ \\varepsilon_1 E\\left(\\varepsilon_2 \\mid \\varepsilon_1\\right)\\right] \\\\\n    &= E\\left[\\varepsilon_1^2\\right] \\\\\n    &= Var\\left(\\varepsilon_1\\right) + E^2\\left(\\varepsilon_1\\right) \\\\\n    &= 1.\n\\end{aligned}\n$$\n\nLine 2 comes from recognizing that if we are \"given\" $\\varepsilon_1$, then it is constant in terms of the inner expectation and comes out of the expectation.  Now, we have shown that\n\n$$Cov\\left(Y_1, Y_2\\right) = 1$$\n\nmeaning that there is a correlation between terms that are next to one another.  \n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}