{
  "hash": "373fb184dd6f36cdf9ec9b9cbd6738d5",
  "result": {
    "engine": "knitr",
    "markdown": "# Inference for a Population Mean {#sec-inference}\n\n\n\n\n\n\n\\providecommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\providecommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n\n\n\n\nIn the previous chapter, we examined properties of sampling distribution, with particular attention paid to the sampling distribution of the sample mean.  We also discussed both analytical and empirical methods for modeling the sampling distribution of a statistic.  In this chapter, we build on those results to conduct inference on a parameter.\n\n## Confidence Intervals\nSampling distributions characterize the variability in a statistic across repeated samples.  That is, we expect the statistic to differ from one sample to another.  However, what the sampling distribution reveals is that the degree to which these statistics vary, and the degree to which they vary from the corresponding parameter, is quantifiable.  This is how sampling distributions are used in a probability course --- given an underlying population, determine the probability that the sample mean exceeds a particular value.  In statistics, however, we want to go in the other direction --- given a set of data, determine a suitable interval of estimates for the parameter.\n\n@fig-inference-samplingdistribution accompanies the \"forward\" direction that would accompany a probability problem.  Given the sampling distribution, we would be 95% sure the statistic would fall within $d$ units of the parameter $\\theta$, as illustrated by the shaded region.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of using a sampling distribution to determine the values of a statistic we would likely observe in a new sample.](./images/fig-inference-samplingdistribution-1.pdf){#fig-inference-samplingdistribution fig-alt='Density curve with the middle 95% of values highlighted.' width=80%}\n:::\n:::\n\n\n\nGiven a sample, we want to \"reverse engineer\" the problem.  That is, the sampling distribution tells us that the statistic would likely not fall more than $d$ units from the parameter.  Therefore, if we take a sample from the underlying population and compute a statistic, we would expect the parameter to be within $d$ units of this statistic.  This is illustrated in @fig-inference-model, where the difference is that the model for the sampling distribution is centered on the statistic.  We call this a __confidence interval__.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of using a model for the sampling distribution to determine the values of the parameter that are consistent with the observed sample.](./images/fig-inference-model-1.pdf){#fig-inference-model fig-alt='Density curve with the middle 95% of values highlighted.' width=80%}\n:::\n:::\n\n\n\n:::{#def-confidence-interval}\n## Confidence Interval\nConsider repeatedly taking samples $\\mathbf{Y}$ of size $n$ from a population characterized by the parameter $\\theta$.  The interval $\\left(h_1(\\mathbf{Y}), h_2(\\mathbf{Y})\\right)$ is said to be a $100c$% confidence interval if \n\n$$Pr\\left(h_1(\\mathbf{Y}) \\leq \\theta \\leq h_2(\\mathbf{Y})\\right) = c.$$\n:::\n\n:::{.callout-warning}\nIt is important to note that in the definition of a confidence interval, the statistics $h_1(\\mathbf{Y})$ and $h_2(\\mathbf{Y})$ are the random variables; the parameter $\\theta$ is fixed.  That is, it is the _interval_ that is moving across the repeated samples, not the parameter.  Instead of saying \"the probability the parameter is within the interval,\" we would say \"the probability the interval captures the parameter.\"  While this may seem subtle, it is important for correctly interpreting the interval.\n:::\n\nNotice that in the definition of a confidence interval depends on repeated sampling.  Once we have a sample, probability no longer makes sense; the interval either captures the parameter or it does not, but our ignorance of the result does not warrant a probability statement.  This is a direct result of our interpretation of probability (@def-frequentist-interpretation).\n\n:::{#exm-ci-clt}\n## CI for Sample Mean Using CLT\nLet $Y_1, Y_2, \\dotsc, Y_n$ be a large random sample from a population with a finite mean $\\mu$ and variance $\\sigma^2$.  Develop a $100c$% confidence interval for the population mean $\\mu$.\n:::\n\n:::{.solution}\nDefine $z_{0.5(1 + c)}$ to be the value such that \n\n$$Pr\\left(Z \\leq z_{0.5(1+c)}\\right) = 0.5(1 + c)$$\n\nfor any $0 < c < 1$ where $Z \\sim N(0, 1)$.  Then, we know that\n\n$$c = Pr\\left(-z_{0.5(1+c)} \\leq Z \\leq z_{0.5(1+c)}\\right)$$\n\nbecause of the symmetry of the Normal distribution.  Defining $\\bar{Y}$ and $S$ to be the sample mean and standard deviation from the sample, from the CLT, we know that the ratio $\\frac{\\sqrt{n}\\left(\\bar{Y} - \\mu\\right)}{S}$ can be approximated by a Standard Normal distribution. That is, probability statements about this ratio are equivalent to probability statements about a Standard Normal random variable.  Therefore, we have\n\n$$c = Pr\\left(-z_{0.5(1+c)} \\leq \\frac{\\sqrt{n}\\left(\\bar{Y} - \\mu\\right)}{S} \\leq z_{0.5(1+c)}\\right).$$\n\nWe now rearranging terms, we have\n\n$$\n\\begin{aligned}\n  c \n    &= Pr\\left(-z_{0.5(1+c)} \\leq \\frac{\\sqrt{n}\\left(\\bar{Y} - \\mu\\right)}{S} \\leq z_{0.5(1+c)}\\right) \\\\\n    &= Pr\\left(-z_{0.5(1+c)}\\frac{S}{\\sqrt{n}} \\leq \\left(\\bar{Y} - \\mu\\right) \\leq z_{0.5(1+c)}\\frac{S}{\\sqrt{n}}\\right) \\\\\n    &= Pr\\left(\\bar{Y} - z_{0.5(1+c)} \\frac{S}{\\sqrt{n}} \\leq \\mu \\leq \\bar{Y} + z_{0.5(1+c)} \\frac{S}{\\sqrt{n}}\\right).\n\\end{aligned}\n$$\n\nDefining $h_1(\\mathbf{Y}) = \\bar{Y} - z_{0.5(1+c)}\\frac{S}{\\sqrt{n}}$ and $h_2(\\mathbf{Y}) = \\bar{Y} + z_{0.5(1+c)}\\frac{S}{\\sqrt{n}}$, we have identified an interval $\\left(h_1(\\mathbf{Y}), h_2(\\mathbf{Y})\\right)$ that satisfies the definition of a $100c$% confidence interval.\n:::\n\nConsider the following research objective:\n\n  > Estimate the average cost (in US dollars) of a diamond for sale in the United States.\n  \nIn the previous chapter, we saw that the CLT could be used to model the sampling distribution of the sample mean using the available data.  Applying the result of @exm-ci-clt, we have that a 95% confidence interval for the average cost of a diamond is given by\n\n$$\n\\begin{aligned}\n  \\bar{y} &\\pm z_{0.925} \\left(s / \\sqrt{n}\\right) \\\\\n  3932.8 &\\pm (1.96) (3989.4 / \\sqrt{53940}) \\\\\n  (3899&,\\ 3966).\n\\end{aligned}\n$$\n\nWhile @exm-ci-clt provided a formula for a confidence interval using the CLT, we can also develop a confidence interval in the same spirit from an empirical model.  Simply define $h_1(\\mathbf{Y})$ and $h_2(\\mathbf{Y})$ to be the 2.5-th and 97.5-th percentiles from the empirical sampling distribution.  Then, we will have a valid 95% confidence interval. @fig-inference-bootstrap illustrates the 95% CI using the same data.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Bootstrap sampling distribution of the sample mean cost of a diamond based on the available data.  5000 bootstrap replicates were used.](./images/fig-samplingdistributions-bootstrap-1.pdf){#fig-samplingdistributions-bootstrap fig-alt='Density plot of bootstrap statistics with the area under the curve corresponding to the middle 95% of values shaded.' width=80%}\n:::\n:::\n\n\n\nNot surprisingly, since we saw in the last chapter that the empirical model and the CLT were extremely similar, the 95% CI from the empirical model matches the 95% CI given by the CLT.\n\n\n## Null Distributions and P-Values\nThe disciplines of probability and statistics blend together most naturally when performing a hypothesis test.  In @sec-samplingdistributions, we examined sampling distributions of a statistic, with particular attention paid to the sample mean.  We had to transition to modeling these sampling distributions using data since the population parameters, which also characterize the sampling distribution of the statistic, are unknown.  When we are performing a hypothesis test, however, the null distribution specifies a particular value of the unknown parameter(s); this then allows us to make use of our models directly!  This is known as a __null distribution__.\n\n:::{#def-null-distribution}\n## Null Distribution\nDistribution of a statistic under a hypothesized value of the population parameter(s).\n:::\n\nLet's return to our investigation of diamond prices.  Suppose we are interested in the following research question:\n\n  > Is it reasonable that the average cost (in US dollars) of a diamond is \\$3900?  Or, does the sample provide evidence that the average cost of a diamond exceeds \\$3900?\n  \nLetting $\\theta$ represent the average cost (in US dollars) of a diamond, the above research question can be captured using the following hypotheses:\n\n$$H_0: \\theta \\leq 3900 \\qquad \\text{vs.} \\qquad H_1: \\theta > 3900.$$\n\nFrom the CLT, we have that the sampling distribution of the ratio\n\n$$\\frac{\\sqrt{n}\\left(\\bar{Y} - \\theta\\right)}{S}$$\n\ncan be approximated by a Standard Normal distribution, where $\\bar{Y}$ and $S$ represent the sample mean and standard deviation, respectively.  In the above ratio, $\\theta$ is unknown.  However, _if we are willing to assume the above null hypothesis is true_, then we know that $\\theta = 3900$.\n\n:::{.callout-note}\nWhen working with a one-sided hypothesis, \"assuming the null hypothesis\" always considers the boundary of the interval.  The idea here is that if we can establish evidence against this boundary, then we can establish evidence against any other value represented in the null hypothesis.\n:::\n\nThat is, _if the null hypothesis is true_, then we have that the sampling distribution of the ratio\n\n$$\\frac{\\sqrt{n}\\left(\\bar{Y} - 3900\\right)}{S}$$\n\ncan be approximated by a Standard Normal distribution.  Essentially, we are using the CLT combined with the knowledge provided by the null hypothesis about the parameters.  That is, we really are discussing the _null distribution_.  Now, we can use this distribution to make probabilistic statements.  For example, we might ask the following question:\n\n  > Assuming the null hypothesis is true, what is the probability that the ratio $\\frac{\\sqrt{n}\\left(\\bar{Y} - 3900\\right)}{S}$ would meet or exceed 1.909?\n  \nThis is a straight-forward probability problem.  Observe that\n\n$$\n\\begin{aligned}\n  Pr\\left(\\frac{\\sqrt{n}\\left(\\bar{Y} - 3900\\right)}{S} \\geq 1.909\\right)\n    &= Pr(Z \\geq 1.909) \\\\\n    &= 0.0281\n\\end{aligned}\n$$\n\nwhere $Z \\sim N(0, 1)$.  That is, assuming the mean cost of a diamond is \\$3900, there is only a 2.81% chance that we would observe a ratio of at least 1.909.  However, using the data we obtained, we find that the ratio we observed in our sample was 1.909, where we use the observed sample mean and standard deviation in the computation.  That is, if the average cost of a diamond is \\$3900, if we were to repeat the study, there is only a 2.81% chance that we would observe data that would provide a ratio as extreme or more so than that observed.  Further, larger values of this ratio are more consistent with the alternative hypothesis (average costs larger than \\$3900).  So, if the average cost of a diamond is \\$3900, there is only a 2.81% chance that we would observe data as consistent or more so with the alternative hypothesis as that observed.  This is known as a __p-value__.\n\n:::{#def-pvalue}\n## P-value\nThe probability, assuming the null hypothesis is true, that we would observe a statistic, by chance alone, as extreme or more so than that observed in the sample.\n:::\n\n:::{.callout-note}\nOften times, our analytic results imply a sampling distribution (or an approximation of the sampling distribution) for a ratio instead of the statistic of interest.  This ratio is often called the \"standardized (test) statistic\" in hypothesis testing because the ratio, when evaluated with the observed data, provides a metric quantifying the difference between our expectations and our observations in the sample.\n:::",
    "supporting": [
      "04-inference_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}