{
  "hash": "c672f407badcc0625a6fffce3843a3a7",
  "result": {
    "markdown": "# Inference for Regression Models {#sec-regression}\n\n\n\n\n\n\n\n\n\n\nThe previous chapters introduced the primary components of inference.  Particular attention was paid to making inference for the mean of a quantitative variable.  Most interesting questions involve examining a relationship between two variables.  For example, consider the following question:\n\n  > Does the average cost of a diamond (in US dollars) tend to change as the carat (a measure of the size) changes?\n  \nThe above question depends on the relationship between the cost of a diamond and the carat of the diamond.  In this chapter, we examine some of the probabilistic components associated with such problems, known as __regression__.\n\n:::{#def-regression}\n## Regression\nAllowing the parameters characterizing the distribution of a random variable to depend, through some specified function, on the value of additional variables.\n:::\n\n## Simple Linear Regression Model\nIn previous chapters, we considered distributional models for the population of the form\n\n$$Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{Ind}}{\\sim} N\\left(\\mu, \\sigma^2\\right),$$\n\nfor some unknown mean $\\mu$ and variance $\\sigma^2$.  Note that under such a distributional model, if we knew the value of the parameters $\\mu$ and $\\sigma$, we know everything we need to know to fully characterize the variability in the response $Y$.  However, knowing the parameters does not explain _why_ there is variability in the population.  Why doesn't every unit share the exact same value of the response?  We know that variability is inherent in any process; perhaps all of the variability in this response is due to measurement error.  If that were true, then if we had infinitely precise measuring tools, then the variability would be eliminated.  \n\nIt is more likely that the various units are not meant to be completely identical.  That is, there are additional characteristics that might explain why each unit has a different response.  For example, in our sample of diamonds, the units (the diamonds) are not identical.  Some diamonds are larger than others; some have superior color or clarity.  These differences in the characteristics of the diamonds might explain their different prices.  Unfortunately, the distributional model above does not incorporate additional characteristics.\n\nThe research question posed above suggested that the average response (cost) might depend upon another variable (carat), a predictor.  The simple linear regression model allows the average response to depend upon the predictor through a linear function.  \n\n:::{#def-classical-simple-regression}\n## Classical Simple Linear Regression\nLet $\\left(Y_1, x_1\\right), \\left(Y_2, x_2\\right), \\dotsc, \\left(Y_n, x_n\\right)$ be observations made on a sample of $n$ units.  Under the classical simple linear regression model, the distributional model for the response among the population is given by\n\n$$Y_1, Y_2, \\dotsc, Y_n \\stackrel{\\text{Ind}}{\\sim} N\\left(\\beta_0 + \\beta_1 x_i, \\sigma^2\\right)$$\n\nwhere $\\beta_0$, $\\beta_1$, and $\\sigma^2$ are unknown parameters.\n:::\n\n:::{.callout-note}\nIn @def-classical-simple-regression, note that we only consider the response to be a random variable; the predictor is considered fixed (hence the use of a lowercase $x$ instead of a capital $X$).  This is consistent with the idea of a designed experiment for which the values of the predictor can be determined in advance by the researchers.  \n\nHowever, for observational studies, the values of the predictor cannot be fixed.  That is, in practice, the predictor is also unknown in advance and is therefore a random variable as well.  In such cases, we can proceed in the same manner, considering the _conditional_ distribution of the response _given_ the predictor.\n:::\n\nNotice that @def-classical-simple-regression simply extends the form of the distributional model we have previously considered.  It makes it clear that\n\n$$E\\left(Y_i\\right) = \\beta_0 + \\beta_1 x_i$$\n\nfor each unit.  Notice that we do not say that the observations are \"identically distributed,\" because they are not!  The mean response differs (at least potentially) for each observation as it depends on the corresponding value of the predictor.  \n\nWhile the above presentation connects the process for the inference of a single mean with that of regression, it is not the common presentation.  Instead, the model is traditionally presented as saying that\n\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\n\nwhere $\\varepsilon_i \\stackrel{\\text{IID}}{\\sim} N\\left(0, \\sigma^2\\right)$, where now we can make use of the \"identically distributed\" language.  In this presentation, we have introduced a new random variable, $\\varepsilon$.  Since the expression $\\beta_0 + \\beta_1 x_i$ does not contain a random variable, it is deterministic in nature.  Therefore, the distribution of $Y_i$ is determined because we are simply shifting the distribution of $\\varepsilon_i$.\n\nWhenever we posit a distributional model for the population, as we do in @def-classical-simple-regression, we are making a fairly strong assumption.  In practice, we may not feel comfortable positing the form of the distribution.  We can reduce the conditions on $\\varepsilon$ and still retain some of the key characteristics of the model.\n\nRegardless of the conditions we impose on $\\varepsilon$, we are essentially specifying a model for the data generating process --- the set of statements we are willing to make regarding the variability in the response.  We know that since the response is a random variable it has some distribution.  The model for the data generating process is really a set of statements about that distribution.  We may only be characterizing the mean of the distribution of the response; we may be willing to characterize the mean and the variance; or, we may be willing to fully characterize the distributional form.\n\n\n## Least Squares Estimation\nWhile the previous section defines a model for the data generating process, it does not specify how to estimate the unknown parameters.  Prior to this section, we were able to estimate the parameters by making analogous computations within the sample.  As we replace a singular parameter with a function, corresponding computations are no longer obvious.  A general process for estimating the unknown parameters that define the mean response is the __method of least squares__.\n\n:::{#def-least-squares}\n## Method of Least Squares\nThe least squares estimates of the parameters $\\beta_0$ and $\\beta_1$ in a simple linear regression model are the values that minimize the quantity\n\n$$\\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2.$$\n\nThe estimates are often denoted $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$.\n:::\n\n:::{#thm-least-squares-slr}\n## Least Squares Estimates\nLet $\\left(Y_1, x_1\\right), \\left(Y_2, x_2\\right), \\dotsc, \\left(Y_n, x_n\\right)$ be observations made on a sample of $n$ units.  The least squares estimates for the simple linear regression model relating the response and predictor are given by\n\n$$\n\\begin{aligned}\n  \\widehat{\\beta}_1 \n    &= \\frac{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(Y_i - \\bar{Y}\\right)}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2} \\\\\n  \\widehat{\\beta}_0\n    &= \\bar{Y} - \\widehat{\\beta}_1 \\bar{x}.\n\\end{aligned}\n$$\n:::\n\n:::{.proof}\nBy definition, the least squares estimates are the values of the parameters that minimize the quantity\n\n$$\\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2.$$\n\nTo minimize this quantity, we can consider taking partial derivatives with respect to each quantity:\n\n$$\n\\begin{aligned}\n  \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n    &= -2 \\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right) \\\\\n  \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n    &= -2 \\sum_{i=1}^{n} x_i \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right).\n\\end{aligned}\n$$\n\nSetting each derivative equal to zero, we have\n\n$$\n\\begin{aligned}\n  0 \n    &= n\\bar{Y} - n\\beta_0 - n\\beta_1 \\bar{x} \\\\\n  0\n    &= \\sum_{i=1}^{n} x_i Y_i - n\\beta_0 \\bar{x} - \\beta_1 \\sum_{i=1}^{n} x_i^2. \n\\end{aligned}\n$$\n\nWe now have two equations and two unknown terms.  Solving the first equation, we have that\n\n$$\\widehat{\\beta}_0 = \\bar{Y} - \\widehat{\\beta}_1 \\bar{x}.$$\n\nPlugging this into the second expression, we have\n\n$$\n\\begin{aligned}\n  0 \n    &= \\sum_{i=1}^{n} x_i Y_i - n\\bar{Y}\\bar{x} + n \\beta_1 \\bar{x}^2 - \\beta_1 \\sum_{i=1}^{n} x_i^2 \\\\\n    &= \\sum_{i=1}^{n} x_i \\left(Y_i - \\bar{Y}\\right) - \\beta_1 \\sum_{i=1}^{n} \\left(x_i^2 - \\bar{x}^2\\right) \\\\\n    &= \\sum_{i=1}^{n} \\left(x_i - \\bar{x} + \\bar{x}\\right) \\left(Y_i - \\bar{Y}\\right) - \\beta_1 \\sum_{i=1}^{n} \\left(x_i^2 - \\bar{x}^2\\right),\n\\end{aligned}\n$$\n\nwhere the second line rearranges the terms by bringing things inside the sums, and the third line \"does nothing\" by adding and subtracting the same term, $\\bar{x}$.  Note that the second term can be written as\n\n$$\n\\begin{aligned}\n  \\beta_1 \\sum_{i=1}^{n} \\left(x_i^2 - \\bar{x}^2\\right)\n    &= \\beta_1 \\sum_{i=1}^{n} \\left[\\left(x_i - \\bar{x}\\right)^2 + 2x_i\\bar{x} - 2\\bar{x}^2\\right] \\\\\n    &= \\beta_1\\sum_{i=1}^{n}\\left(x_i - \\bar{x}\\right)^2 + \\beta_1 2 n\\bar{x}^2 - 2n\\bar{x}^2 \\\\\n    &= \\beta_1 \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2.\n\\end{aligned}\n$$\n\nAnd, the first term simplifies to \n\n$$\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(Y_i - \\bar{Y}\\right) + \\bar{x} \\sum_{i=1}^{n} \\left(Y_i - \\bar{Y}\\right) = \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(Y_i - \\bar{Y}\\right).$$\n\nPutting these components together, we have that the least squares estimate of $\\beta_1$ must satisfy the equality\n\n$$0 = \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(Y_i - \\bar{Y}\\right) - \\beta_1 \\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2.$$\n\nThis gives us that\n\n$$\\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)\\left(Y_i - \\bar{Y}\\right)}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2}.$$\n\nOf course, we have simply found the limits; to establish these are actually minimums, we must consider the Hessian matrix, which consists of second-order partial derivatives.  For our model, we have\n\n$$\n\\begin{aligned}\n  \\frac{\\partial^2}{\\partial \\beta_0^2} \\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n    &= 2n \\\\\n  \\frac{\\partial^2}{\\partial \\beta_1^2} \\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n    &= 2\\sum_{i=1}^{n} x_i^2\\\\\n  \\frac{\\partial^2}{\\partial \\beta_1 \\partial \\beta_0} \\sum_{i=1}^{n} \\left(Y_i - \\beta_0 - \\beta_1 x_i\\right)^2  \n    &= 2n\\bar{x}.\n\\end{aligned}\n$$\n\nThe determinant of the Hessian matrix is then \n\n$$\n\\begin{aligned}\n  4n\\sum_{i=1}^{n} x_i^2 - 4n\\bar{x}^2 \n    &= 4n\\left[\\sum_{i=1}^{n} x_i^2 - \\bar{x}^2\\right] \\\\\n    &= 4n\\left\\{\\sum_{i=1}^{n} \\left[\\left(x_i - \\bar{x}\\right)^2 + 2x_i\\bar{x} - \\bar{x}^2\\right] - \\bar{x}^2\\right\\} \\\\\n    &= 4n \\left[\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2 + n\\bar{x}^2 - \\bar{x}^2\\right] \\\\\n    &= 4n \\left[\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2 + (n - 1)\\bar{x}^2\\right] \\\\\n    &> 0.\n\\end{aligned}\n$$\n\nSince the determinant is always positive, we have that our least squares estimates minimize the quantity of interest.\n:::\n\n:::{.callout-note}\nIn the proof above, we essentially derive a very helpful relation that pops up often in statistical proofs:\n\n$$\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2 = \\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2.$$\n:::\n\nThe method of least squares can be used to estimate the parameters in the mean model, but this is not _statistics_; it is _mathematics_.  In particular, the method of least squares is just one of several optimization problems we might have defined to estimate the parameters.  Statistics is the discipline of characterizing variability.  Characterizing the uncertainty in these estimates --- characterizing their sampling distribution --- that is where we move into a statistical analysis.\n\n:::{#exm-unbiased-ls}\n## Expected Value of the Intercept\nThe least squares estimate of the intercept is an unbiased estimator.  Establish this, assuming that the least squares estimate of the slope is also an unbiased estimator.\n:::\n\n:::{.solution}\nTo establish that this statement is correct, observe that\n\n$$\n\\begin{aligned}\n  E\\left(\\widehat{\\beta}_0\\right)\n    &= E\\left(\\bar{Y} - \\widehat{\\beta}_1 \\bar{x}\\right) \\\\\n    &= E\\left(n^{-1} \\sum_{i=1}^{n} \\left(\\beta_0 + \\beta_1 x_i + \\varepsilon_i\\right) - \\widehat{\\beta}_1 \\bar{x}\\right) \\\\\n    &= E\\left(n^{-1} \\sum_{i=1}^{n} \\left(\\beta_0 + \\beta_1 x_i + \\varepsilon_i\\right)\\right) - \\bar{x}E\\left(\\widehat{\\beta}_1\\right) \\\\\n    &= n^{-1}\\sum_{i=1}^{n} \\left(\\beta_0 + \\beta_1 x_i + E\\left(\\epsilon_i\\right)\\right) - \\bar{x} \\beta_1 \\\\\n    &= \\beta_0 + \\beta_1 \\bar{x} - \\bar{x} \\beta_1 \\\\\n    &= \\beta_0.\n\\end{aligned}\n$$\n\nNote that in line 2, we insert the value of $Y_i$ from the model for the data generating process; in line 3, we make use of the linearity of expectations; in line 4, we assume that the least squares estimate of the slope is an unbiased estimator.  Finally, in line 5, we assume that the error term has an average of 0.\n:::\n\nNote that in our solution, the only assumption we needed on the error term was that it had an average of 0.  While the other conditions on the error term typically assumed (classical regression model) are helpful in characterizing the sampling distribution of the least squares estimates, only this one condition is needed to establish where that sampling distribution is centered.\n\n:::{#thm-classical-ls}\n## Sampling Distribution for Least Squares Estimators\nUnder the conditions of the Classical Regression Model (@def-classical-simple-regression), we have that\n\n$$\\frac{\\widehat{\\beta}_j - \\beta_j}{\\sqrt{Var\\left(\\widehat{\\beta}_j\\right)}} \\sim t_{n - 2}$$\n\nwhere\n\n$$\n\\begin{aligned}\n  Var\\left(\\widehat{\\beta}_0\\right)\n    &= \\widehat{\\sigma}^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2}\\right)\\\\\n  Var\\left(\\widehat{\\beta}_1\\right)\n    &= \\frac{\\widehat{\\sigma}^2}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2}\n\\end{aligned}\n$$\n\nand\n\n$$\\widehat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^{n} \\left(Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 x_i\\right)^2$$\n\nis our estimate of the unknown population variance $\\sigma^2$.\n:::\n\nOnce we have a model for the sampling distribution, we have the ability to make inference --- confidence intervals or p-values.\n\n\n",
    "supporting": [
      "05-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}