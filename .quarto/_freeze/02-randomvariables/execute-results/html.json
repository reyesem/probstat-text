{
  "hash": "954d1419fa661655d4db757881cdec9e",
  "result": {
    "engine": "knitr",
    "markdown": "# Random Variables and Distributions {#sec-randomvariables}\n\n\n\n\n\n\n\\providecommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\providecommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\providecommand{\\dist}[1]{\\stackrel{\\text{#1}}{\\sim}}\n\\providecommand{\\ind}[1]{\\mathbb{I}\\left(#1\\right)}\n\\providecommand{\\bm}[1]{\\mathbf{#1}}\n\\providecommand{\\bs}[1]{\\boldsymbol{#1}}\n\\providecommand{\\Ell}{\\mathcal{L}}\n\\providecommand{\\indep}{\\perp\\negthickspace\\negmedspace\\perp}\n\n\n\n\nIn @sec-fundamentals, we discussed the probability of an \"event.\"  For statisticians, the events of interests center on measurements, or functions of those measurements, that we plan to take.  In this chapter, we begin to connect probability to data analysis.  Our goal is to reexamine concepts introduced in a probability course, relating them to their data-centric analogues. \n\n## Random Variables\nConsider collecting data; before the data is collected, we cannot predict with certainty what we will observe.  Therefore, we can think of each observation as the result of a random process.  These observations are recorded as variables in our dataset.  In probability, a __random variable__ is used to represent a measurement that results from a random process.  \n\n:::{#def-random-variable}\n## Random Variable\nLet $\\mathcal{S}$ be the sample space corresponding to a random process; a random variable $X$ is a function mapping elements of the sample space to the real line.\n\nRandom variables represent a measurement that will be collected during the course of a study.  Random variables are typically represented by a capital letter.\n:::\n\nWhile for our purposes, it suffices to think of a random variable as a measurement, mathematically, it is a _function_.  The image (or range) of this function is used to broadly classify random variables as __continuous__ or __discrete__; we refer to this image as the __support__ of the random variable.\n\n:::{#def-support}\n## Support\nThe support of a random variable is the set of all possible values the random variable can take.\n:::\n\n:::{#def-rvtypes}\n## Continuous and Discrete Random Variable\nThe random variable $X$ is said to be a discrete random variable if its corresponding support is countable.  The random variable $X$ is said to be a continuous random variable if the corresponding support is uncountable (such as an interval or a union of intervals on the real line).\n:::\n\nDiscrete random variables are analogous to categorical (or qualitative) variables in data analysis; that is, discrete random variables are used to model the result of a random process which categorizes each unit of observation into a group.  Continuous random variables are analogous to numeric (or quantitative) variables in data analysis; continuous random variables are used to model the result of a random process which produces a number for which arithmetic makes sense.\n\n:::{.callout-warning}\nWhether we use a continuous or discrete random variable to represent a measurement is not always obvious.  Suppose we consider recording the age of a student selected from a class at a university that typically enrolls \"traditional\" students (those coming directly from high school).  Let the random variable $X$ denote the age of the student.\n\nIf we record the student's age in years since birth, $X$ can take on only a finite number of values (most likely $\\{18, 19, 20, 21, 22, 23\\}$), making it a discrete random variable.  However, if we record the student's age as the number of seconds since birth, we might well consider the support of $X$ to be a rather large interval, leading to a continuous random variable.  \n:::\n\nThe goal of statistics is to use a sample to say something about the underlying population.  Consider taking a sample of size $n$ and measuring a single variable on each unit of observation.  Then, we might represent the measurements we will obtain (note the use of the future tense) as $X_1, X_2, \\dots, X_n$.  While the majority of probability courses focus on a single, or maybe two, random variables, note that collecting data on a sample requires that we deal with at least $n$ random variables (one measurement for each of the observations in our sample).\n\n\n## Characterizing a Distribution\nAgain, the goal of statistics is to use a sample to say something about the underlying population.  Consider the following research objective:\n\n  > Estimate the cost (in US dollars) of a diamond for sale in the United States.\n  \nFor this research objective, our population of interest is all diamonds for sale in the United States.  We would not expect every diamond for sale to have the same price; variability is inherent in any process.  As a result, the sale price of diamonds has a distribution across this population.  This is our primary use of probability theory in a statistical analysis --- to model distributions.  \n\nConsider taking a sample of size 1 from the population; let $Y$ represent the cost of the diamond that is selected.  Since we have not yet observed the cost of this diamond, $Y$ is a random variable.  And, since this diamond is sampled from the population of interest, the support of $Y$ is determined by the cost of diamonds in the United States.  Further, the likelihood that $Y$ falls within any interval is determined by the distribution of the cost across the population.  That is, the distribution of $Y$ is the distribution of the population.\n\n:::{.callout-tip}\n## Big Idea\nIf a random variable $X$ represents a measurement for a single observation from a population, the distribution of the random variable corresponds to the distribution of the variable across the population.\n:::\n\nA key realization in statistical analysis is that we will never fully observe the distribution of the population; however, we can posit a model for this distribution.  In probability, the most common way to characterize the distribution of a random variable is through its density function.\n\n:::{#def-density-function}\n## Density Function\nA density function $f$ relates the values in the support of a random variable with the probability of observing those values.  \n\nLet $X$ be a continuous random variable, then its density function $f$ is the function such that\n\n$$Pr(a \\leq X \\leq b) = \\int_a^b f(x) dx$$\n\nfor any real numbers $a$ and $b$ in the support.\n\nLet $X$ be a discrete random variable, then its density function $f$ is the function such that\n\n$$Pr(X = u) = f(u)$$\n\nfor any real number $u$ in the support.\n:::\n\n:::{.callout-note}\n## Properties of a Density Function\nLet $X$ be a random variable with density function $f$ defined over support $\\mathcal{S}$.  Then, \n\n  1. $f(x) \\geq 0$ for all $x \\in \\mathcal{S}$.  That is, the density is non-negative for all values in the support.\n  2. If $X$ is a continuous random variable, then $\\int_{\\mathcal{S}} f(x) dx = 1$; similarly, if $X$ is a discrete random variable, then $\\sum_{\\mathcal{S}} f(x) = 1$.  That is, $X$ must take a value in its support; so, $Pr(X \\in \\mathcal{S}) = 1$, similar to the second Axiom of Probability (@def-axioms).\n  3. $f(x) = 0$ for all values of $x \\notin \\mathcal{S}$.  The density takes the value of 0 for all values outside the support.\n:::\n\n:::{.callout-note}\nIn a probability course, there is often a distinction made between probability \"density\" functions (used for continuous random variables) and probability \"mass\" functions (used for discrete random variables).  We do not make this distinction and instead rely on the context to determine whether we are dealing with a continuous or discrete random variable.  Throughout, we will note when the operations differ between these two types of variables.  Measure theory provides a unifying framework to these issues.\n:::\n\nWith few exceptions, we will be working with continuous random variables.  As a result, the density function is a smooth function over some region, and the actual value of the function is not interpretable; instead, we get at a probability by considering the area under the curve.  Again, drawing connections to data analysis, we can think of a density function as a mathematical formula representing a smooth histogram.  The area under the curve for any region gives the proportion of the population which has a value in that region.  That is, we get the probability that a random variable will be in an interval by integrating the density function over that interval.  \n\nFigure @fig-randomvariables-density illustrates this idea; we have data from a sample of diamonds from the population of interest.  The sample is summarized with a histogram; we have overlayed a posited density (with the corresponding mathematical function that describes this density) for the population.  The sample (summarized with the histogram) is approximating the population (modeled using the density function).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustration of a density function representing the posited distribution of the population alongside a histogram summarizing the cost of diamonds using a sample of 53940 diamonds.](./images/fig-randomvariables-density-1.png){#fig-randomvariables-density fig-alt='Histogram with a density function overlayed.' width=80%}\n:::\n:::\n\n\nYou may recognize the particular form of the density function in @fig-randomvariables-density.  The general form is\n\n$$f(x) = \\frac{1}{\\sigma} e^{-x / \\sigma} \\qquad \\text{for } x > 0$$\n\nwhere $\\sigma$ is the _scale_ parameter that defines the distribution (set at 4000 in @fig-randomvariables-density).  This is known as the Exponential distribution with scale parameter $\\sigma$.  This illuminates another connection between probability and statistics.\n\nNote that our research objective describe above is an ill-posed question as stated.  The answer is \"it depends\" since each individual diamond in the population has a different value.  Well-posed questions in statistics are centered on an appropriately chosen __parameter__.\n\n:::{#def-parameter}\n## Parameter\nNumeric quantity which summarizes the distribution of a variable within the _population_ of interest.  Generally denoted by Greek letters in statistical formulas.\n:::\n\nIn probability, the parameters are values that are tuned or set within a problem; we then work forward to compute the probability of an event of interest.  In practice, however, when we posit a functional form for a density function to describe the distribution of the population, the parameters are unknown.  We plan to use the data to estimate or characterize the parameter; but, the parameter itself will remain unknown.  In both cases, however, the parameter is a _fixed quantity_, even if we are ignorant of that value.\n\n:::{.callout-tip}\n## Big Idea\nWhen a probability model is specified for a population, it is generally specified up to some unknown parameter(s).  Making inference on the unknown parameter(s) therefore characterizes the entire distribution.\n:::\n\n\n### Common Parameters\nMost scientific questions are focused on the location or spread of a distribution.  For example, we are interested in estimating the average cost of a diamond sold in the United States.  Introductory statistics introduces summaries of location and spread within the sample (e.g., sample mean for location and sample variance for spread).  Analogous summaries exist for density functions.  As stated above, parameters are unknown constants that govern the form of the density function.  Because they govern the form of the density, the parameters are also related to those summarizing the location or spread of the distribution.\n\n:::{#def-mean}\n## Expected Value (Mean)\nLet $X$ be a random variable with density function $f$ defined over the support $\\mathcal{S}$.  The expected value of a random variable, also called the mean and denoted $E(X)$, is given by\n\n$$E(X) = \\int_{\\mathcal{S}} x f(x) dx$$\n\nfor continuous random variables and \n\n$$E(X) = \\sum_{\\mathcal{S}} x f(x)$$\n\nfor discrete random variables.\n:::\n\nNotice the similarity between the form of the sample mean and the population mean.  A sample mean takes the sum of each value in the sample, weighting each value by $1/n$ (where $n$ is the sample size).  Without information about the underlying population, the sample must treat each value observed as equally likely; values become more likely if they appear multiple times.  In the population, however, when the form of $f$ is known, the density provides information about the likelihood of each value giving us a better weight than $1/n$.  That is, the population mean is a sum of the values in the support, weighting each value by the corresponding value of the density function.\n\n:::{#def-variance}\n## Variance\nLet $X$ be a random variable with density function $f$ defined over the support $\\mathcal{S}$.  The variance of a random variable, denoted $Var(X)$, is given by\n\n$$Var(X) = E\\left[X - E(X)\\right]^2 = E\\left(X^2\\right) - E^2(X).$$\n\nIf we let $\\mu = E(X)$, then this is equivalent to\n\n$$\\int_{\\mathcal{S}} (x - \\mu)^2 f(x) dx$$\n\nfor continuous random variables and \n\n$$\\sum_{\\mathcal{S}} (x - \\mu)^2 f(x)$$\n\nfor discrete random variables.\n:::\n\n:::{.callout-warning}\nPay careful attention to the notation.  $E^2(X)$ represents the square of the expected value; that is, \n\n$$E^2(X) = \\left[E(X)\\right]^2.$$\n\nHowever, $E(X)^2$ represents the expected value of the square of $X$; that is,\n\n$$E(X)^2 = E\\left(X^2\\right).$$\n:::\n\nThe variance provides a measure of spread; in particular, it is capturing distance from the mean.  Notice that the form of the variance involves taking the expectation of a squared term; in general, we will need to consider expectations of functions.\n\n:::{#def-expectation}\n## Expectation of a Function\nLet $X$ be a random variable with density function $f$ over the support $\\mathcal{S}$, and let $g$ be a real-valued function.  Then, \n\n$$E\\left[g(X)\\right] = \\int_{\\mathcal{S}} g(x) f(x) dx$$\n\nfor continuous random variables and\n\n$$E\\left[g(X)\\right] = \\sum_{\\mathcal{S}} g(x) f(x)$$\n\nfor discrete random variables.\n:::\n\nA result of @def-expectation is the following, very useful theorem, which states that expectations are linear operators.\n\n:::{#thm-expectation}\n## Expectation of a Linear Combination\nLet $X$ be a random variable, and let $a_1, a_2, \\dotsc, a_m$ be real-valued constants and $g_1, g_2, \\dotsc, g_m$ be real-valued functions; then,\n\n$$E\\left[\\sum_{i=1}^{m} a_i g_i(X)\\right] = \\sum_{i=1}^{m} a_i E\\left[g_i(X)\\right].$$\n:::\n\nThe mean and variance play an important role in characterizing a distribution, especially within statistical theory (as we will see in future chapters).  However, there is another set of parameters which are important.\n\n:::{#def-percentile}\n## Percentile\nLet $X$ be a random variable with density function $f$.  The $100k$ percentile is the value $q$ such that\n\n$$Pr(X \\leq q) = k.$$\n:::\n\n:::{#exm-parameters}\n## Parameters of Exponential Distribution\nLet $X$ be an Exponential distribution with scale parameter $\\sigma$; that is, the density function $f$ is given by\n\n$$f(x) = \\frac{1}{\\sigma} e^{-x/\\sigma} \\qquad x > 0$$\n\nwhere $\\sigma > 0.$  Compute the mean, variance, and median of this distribution, as a function of the unknown scale parameter.\n:::\n\nThe solution to this problem is particularly important as it illustrates a very useful technique when working with known distributions in statistical theory.\n\n:::{.solution}\nWe note that the function\n\n$$g(y) = \\frac{1}{\\beta^{\\alpha} \\Gamma(\\alpha)} y^{\\alpha - 1} e^{-y/\\beta}$$\n\nis a valid density function over the positive real line provided that $\\alpha,\\beta > 0$; in particular, this is known as a Gamma distribution.  Since $g$ is a valid density function, then we know that \n\n$$\\int_{0}^{\\infty} g(y) dy = 1$$\n\nfor all values of $\\alpha,\\beta > 0$. \n\nNow, let $X$ be an Exponential random random variable with scale parameter $\\sigma$.  Then, the expected value of $X$ is given by\n\n$$\n\\begin{aligned}\n  E(X)\n    &= \\int_{0}^{\\infty} x \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\int_{0}^{\\infty} \\frac{1}{\\sigma} x^{2-1} e^{-x/\\sigma} dx\n\\end{aligned}\n$$\n\nwhere we have simply rewritten the exponent in the second line.  Notice that expression within the integral shares a striking similarity to the form of the density function of a Gamma distribution; however, they are not exactly the same.  To coerce the expression into that of the Gamma density function, we \"do nothing\" --- multiplying and dividing the expression by the quantity $\\sigma\\Gamma(2)$.  This gives\n\n$$\n\\begin{aligned}\n  E(X)\n    &= \\int_{0}^{\\infty} x \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\int_{0}^{\\infty} \\frac{1}{\\sigma} x^{2-1} e^{-x/\\sigma} dx \\\\\n    &= \\int_{0}^{\\infty} \\sigma \\Gamma(2) \\frac{1}{\\sigma^2 \\Gamma(2)} x^{2-1} e^{-x/\\sigma} dx \\\\\n    &= \\sigma \\Gamma(2) \\int_{0}^{\\infty} \\frac{1}{\\sigma^2 \\Gamma(2)} x^{2-1} e^{-x/\\sigma} dx \\\\\n    &= \\sigma \\Gamma(2) \\\\\n    &= \\sigma.\n\\end{aligned}\n$$\n\nIn line 3, we have multiplied and divided by $\\sigma \\Gamma(2)$, which does not change the problem.  In line 4, we have pulled out the terms $\\sigma \\Gamma(2)$ since it is a constant with respect to the integral; what is left inside the integral is the form of the density function for a Gamma distribution where $\\alpha = 2$ and $\\beta = \\sigma$.  In line 5, we make use of the fact that the integral of any density function over the entire support for which it is defined must be 1.  Finally, in line 6, we recognize that $\\Gamma(k) = (k-1)!$ if $k$ is a natural number.  \n\nApplying the same process, we also have that\n\n$$\n\\begin{aligned}\n  E\\left(X^2\\right)\n    &= \\int_{0}^{\\infty} x^2 \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\sigma^2 \\Gamma(3)\\int_{0}^{\\infty} \\frac{1}{\\sigma^3 \\Gamma(3)} x^{3-1} e^{-x/\\sigma} dx \\\\\n    &= 2\\sigma^2.\n\\end{aligned}\n$$\n\nTherefore, \n\n$$Var(X) = E\\left(X^2\\right) - E^2(X) = 2\\sigma^2 - \\sigma^2 = \\sigma^2.$$\n\nFinally, the median is the value $q$ such that $Pr(X \\leq q) = 0.5$; but, we recognize that\n\n$$\n\\begin{aligned}\n  Pr(X \\leq q) \n    &= \\int_{0}^{q} \\frac{1}{\\sigma} e^{-x/\\sigma} dx \\\\\n    &= \\left. -e^{-x/\\sigma} \\right|_{0}^{q} \\\\\n    &= -e^{-q/\\sigma} + 1.\n\\end{aligned}\n$$\n\nSetting this expression equal to 0.5 and solving for $q$ yields $q = -\\sigma \\log(0.5)$, where $\\log(\\cdot)$ represents the _natural_ logarithm.\n:::\n\n:::{.callout-tip}\n## Big Idea\nSuppose the density $f$ is a function of the parameters $\\boldsymbol{\\theta}$; then, the mean, variance, and median (as well as any other parameters of interest in a research objective) will be functions of $\\boldsymbol{\\theta}$.\n:::\n\n@exm-parameters highlighted a useful technique for simplifying integrals in statistical applications, which makes use of the \"do nothing\" strategy discussed in the previous chapter.  The solution also shows that there is more than one way to characterize a distribution.\n\n\n### Distribution Function\nEspecially for visualization, the density function is the most common way of characterizing a probability model.  However, computing the probability using the density is problematic due to the integration required.  Many software address this by working with the cumulative distribution function (CDF).\n\n:::{#def-cdf}\n## Cumulative Distribution Function (CDF)\nLet $X$ be a random variable; the cumulative distribution function (CDF) is defined as\n\n$$F(u) = Pr(X \\leq u).$$\n\nFor a continuous random variable, we have that\n\n$$F(u) = \\int_{-\\infty}^{u} f(x) dx$$\n\nimplying that the density function is the derivative of the CDF.  For a discrete random variable\n\n$$F(u) = \\sum_{x \\leq u} f(x).$$\n:::\n\nWorking with the CDF improves computation because it avoids the need to integrate each time; instead, the integral is computed once (and stored internally in the computer) and we use the result to compute probabilities directly.\n\n:::{.callout-tip}\n## Big Idea\nDensity functions are the mathematical models for distributions; they link values of the variable with the likelihood of occurrence.  However, for computational reasons, we often work with the cumulative distribution function which provides the probability of being less than or equal to a value.\n:::\n\n\n## Common Probability Distributions\nWhile we could posit any non-negative function as a model for a density function (properly scaled of course), there are some models that are very common.  While the following list is not exhaustive, it does include the most commonly used distributions that we will encounter in the text.\n\nWhen a response is binary (assumes one of two values), it is a Bernoulli distribution.  In order to make use of this distribution, we typically define one of the two possible outcomes as a \"success\" and the other as a \"failure.\"  For example,\n\n$$X = \\begin{cases} 1 & \\text{if a success is observed} \\\\ 0 & \\text{if a success is not observed.} \\end{cases}$$\n\n:::{#def-bernoulli-distribution}\n## Bernoulli Distribution\nLet $X$ be a discrete random variable taking the value 0 or 1.  $X$ is said to have a Bernoulli distribution with density\n\n$$f(x) = \\theta^x (1 - \\theta)^{1 - x} \\qquad x \\in \\{0, 1\\},$$\n\nwhere $0 < \\theta < 1$ is the probability that $X$ takes the value 1.\n\n- $E(X) = \\theta$\n- $Var(X) = \\theta(1 - \\theta)$\n\nWe write $X \\sim Ber(\\theta)$, which is read \"X follows a Bernoulli distribution with probability $\\theta$.\"\n:::\n\nWe can generalize the Bernoulli distribution to count the number of successes out of $n$ independent trials.\n\n:::{#def-binomial-distribution}\n## Binomial Distribution\nLet $X$ be a discrete random variable taking integer values between 0 and $n$, inclusive.  $X$ is said to have a Binomial distribution with density\n\n$$f(x) = \\begin{pmatrix}n \\\\ x\\end{pmatrix} \\theta^x (1 - \\theta)^{1 - x} \\qquad x \\in \\{0, 1, \\dotsc, n\\},$$\n\nwhere $0 < \\theta < 1$ is the probability of a success on an individual trial.\n\n- $E(X) = n\\theta$\n- $Var(X) = n\\theta(1 - \\theta)$\n\nWe write $X \\sim Bin(n, \\theta)$, which is read \"X follows a Binomial distribution with parameters $n$ and $\\theta$.\"\n:::\n\nWhile there are many other discrete distributions that play important roles in categorical data analyses, the majority of our text will focus on quantitative response variables.  So, we list several key distributions for continuous random variables.\n\n:::{#def-normal-distribution}\n## Normal (Gaussian) Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have a Normal (or Guassian) distribution if the density is given by\n\n$$f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\qquad -\\infty < x < \\infty,$$\n\nwhere $\\mu$ is any real number and $\\sigma^2 > 0$.  \n\n- $E(X) = \\mu$\n- $Var(X) = \\sigma^2$\n\nWe write $X \\sim N\\left(\\mu, \\sigma^2\\right)$, which is read \"X follows a Normal distribution with mean $\\mu$ and variance $\\sigma^2$.\"  This short-hand implies the density above.  When $\\mu = 0$ and $\\sigma^2 = 1$, this is referred to as the Standard Normal distribution.\n:::\n\nThis model is a bell-shaped distribution centered at the mean $\\mu$.  While this is a common model, it should not be assumed by default.  In future chapters, we will consider methods for assessing whether, given a sample, assuming the population follows a Normal distribution is reasonable.\n\n:::{#def-gamma-distribution}\n## Gamma Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have a Gamma distribution if the density is given by\n\n$$f(x) = \\frac{1}{\\beta^{\\alpha} \\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x/\\beta} \\qquad x > 0,$$\n\nwhere $\\alpha > 0$ is the shape parameter and $\\beta > 0$ is the scale parameter.\n\n- $E(X) = \\alpha\\beta$\n- $Var(X) = \\alpha\\beta^2$\n\nWe write $X \\sim Gamma\\left(\\alpha, \\beta\\right)$, which is read \"X follows a Gamma distribution with shape $\\alpha$ and scale $\\beta$.\"  This short-hand implies the density above.  When $\\alpha = 1$, we refer to this as the Exponential distribution with scale $\\beta$.\n\nWe note that, in general, there is no closed form solution for $\\Gamma(\\alpha)$, but \n\n- $\\Gamma(\\alpha) = (\\alpha - 1) \\Gamma(\\alpha - 1)$\n- $\\Gamma(k) = (k - 1)!$ for non-negative integer $k$\n:::\n\nThe Gamma distribution is useful for modeling time-to events.  \n\n:::{.callout-warning}\nWe have presented the Gamma (and Exponential) distribution in terms of the _scale_ parameter.  It is sometimes easier to parameters the distribution in terms of the _rate_ parameter, where the rate is the inverse of the scale.  When consulting tables of distributions[^tables], be sure to note the parameterization of the distribution provided.\n:::\n\n:::{.callout-note}\nThe Exponential distribution being a special case of the Gamma distribution is not the only relationship between common distributions.  There are many relationships[^relationships] that are useful; we will describe these as needed.\n:::\n\nThe (standardized) t-distribution is a bell-shaped distribution, similar to the Normal distribution but with wider tails.  It has a single parameter, known as the degrees of freedom.  Note that unlike many other distributions, this parameter (the degrees of freedom) is not associated with the location of the distribution.  Instead, it governs the spread (but is not equivalent to the variance).\n\n:::{#def-t-distribution}\n## t-Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have a (standardized) t-distribution, sometimes called the Student's t-distribution, if the density is given by\n\n$$f(x) = \\frac{\\Gamma \\left(\\frac{\\nu+1}{2} \\right)} {\\sqrt{\\nu\\pi}\\,\\Gamma \\left(\\frac{\\nu}{2} \\right)} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}} \\qquad -\\infty < x < \\infty$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim t_{\\nu}$, which is read \"X follows a t-distribution with $\\nu$ degrees of freedom.\"\n:::\n\n:::{.callout-note}\nAs the degrees of freedom approach infinity, the density function of the t-distribution approaches that of a Standard Normal random variable.\n:::\n\nThe Chi-Square distribution is a skewed distribution (looks like a giant slide).  It has a single parameter, known as the degrees of freedom.  The degrees of freedom for this distribution characterize both the location and spread simultaneously.\n\n:::{#def-chi-square-distribution}\n## Chi-Square Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have a Chi-Square distribution if the density is given by\n\n$$f(x) = \\frac{1}{2^{\\nu/2}\\Gamma (\\nu/2)}\\;x^{\\nu/2-1}e^{-x/2} \\qquad x > 0,$$\n\nwhere $\\nu > 0$ is the degrees of freedom.\n\nWe write $X \\sim \\chi^2_{\\nu}$, which is read \"X follows a Chi-Square distribution with $\\nu$ degrees of freedom.\"  The Chi-Square distribution is a special case of the Gamma distribution where $\\alpha = \\nu/2$ and $\\beta = 2$.\n:::\n\nThe F-distribution is a skewed distribution.  It has two parameters, known as the numerator and denominator degrees of freedom.  While neither variable is directly the mean or variance, together these two parameters characterize both the location and the spread.\n\n:::{#def-f-distribution}\n## F-Distribution\nLet $X$ be a continuous random variable.  $X$ is said to have an F-distribution if the density is given by\n\n$$f(x) = \\frac{\\Gamma((r + s)/2)}{(\\Gamma(r/2) \\Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \\qquad x > 0,$$\n\nwhere $r,s > 0$ are the numerator and denominator degrees of freedom, respectively.\n\nWe write $X \\sim F_{r, s}$, which is read \"X has an F-distribution with r numerator degrees of freedom and s denominator degrees of freedom.\"\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of various common distributions for continuous random variables.](./images/fig-randomvariables-comparisons-1.png){#fig-randomvariables-comparisons fig-alt='Four density functions overlayed on same axes for comparison.' width=80%}\n:::\n:::\n\n\nThe formulas above are ugly, but we will not be working with them directly.  Instead, statistical software have these distributions embedded.  The key idea here is that when we know the model for a distribution, we can make use of several results about this distribution.\n\n:::{.callout-tip}\n## Big Idea\nSome probability models occur so frequently that we give them names for easy reference.  Some models are common for modeling the population, in which case they are defined in terms of unknown parameters to be estimated.  Some models are used, not to model a population, but to model other distributions that occur in statistical practice.\n:::\n\n\n## Transformations of a Random Variable\nOccasionally, we are interested in a transformation of a particular characteristic.  That is, we have a model for the distribution of $X$, but we are interested in $Y = g(X)$.  In this section, we examine one method for determining the density of $Y$ from the density of $X$.\n\nWhile there various approaches to this problem, we find this method the most reliable.  Further, it does not require the memorization of a formula, but instead builds on fundamental ideals.  This is known as the __Method of Distribution Functions__.\n\n:::{#def-method-of-distribution-functions}\n## Method of Distribution Functions\nLet $X$ be a continuous random variable with density $f$ and cumulative distribution function $F$.  Consider $Y = h(X)$.  The following process provides the density function $g$ of $Y$ by first finding its cumulative distribution function $G$.\n\n  1. Find the set $A$ for which $h(X) \\leq t$ if and only if $X \\in A$.\n  2. Recognize that $G(y) = Pr(Y \\leq y) = Pr\\left(h(X) \\leq y\\right) = Pr(X \\in A)$.\n  3. If interested in $g(y)$, note that $g(y) = \\frac{\\partial}{\\partial y} G(y)$.\n:::\n\nWhen $h$ is a strictly monotone function (unique inverse exists), then step 1-2 is much easier because we can apply $h^{-1}$.  In step 2 of the above process, the final expression is often left in terms of $F$, the CDF of $X$; then, when we find the density in step 3, we can apply the chain rule (avoiding the need to actually have an expression for $F$).\n\n:::{#exm-transformations}\n## Transformation of a Random Variable\nPreviously, we posited the following model for the distribution of the cost of a diamond sold in the US:\n\n$$f(x) = \\frac{1}{\\sigma} e^{-x/\\sigma} \\qquad x > 0$$\n\nfor some $\\sigma > 0$.  As cost is generally a heavily skewed variable, we may be interested in taking the (natural) logarithm before proceeding with an analysis.  Find the density of $Y = \\log(X)$.\n:::\n\n:::{.solution}\nWe note that $\\log(x)$ is a strictly monotone function.  Therefore, we have that\n\n$$\n\\begin{aligned}\n  G(y) &= Pr(Y \\leq y) \\\\\n    &= Pr(\\log(X) \\leq y) \\\\\n    &= Pr\\left(X \\leq e^y\\right).\n\\end{aligned}\n$$\n\nJust to place this within the method described above, since $\\log(x) \\leq y$ if and only if $x \\leq e^y$, then $A = \\{t: x \\leq e^t\\}$.  Of course, we didn't really need to identify this because we were able to apply the inverse of $\\log(x)$ directly within the probability expression.  We now recognize that we have a probability of the form \"$X$ less than or equal to something.\"  And, this matches the form of the CDF of $X$.  That is, we have that\n\n$$G(y) = F\\left(e^y\\right).$$\n\nThis completes step 2 of the procedure; we have expressed the CDF of $Y$ as a function of the CDF of $X$.  Now, to find the density, we apply the chain rule.\n\n$$\n\\begin{aligned}\n  g(y) \n    &= \\frac{\\partial}{\\partial y} G(y) \\\\\n    &= \\left[\\left.\\frac{\\partial}{\\partial x} F(x)\\right|_{x = e^y}\\right] \\frac{\\partial}{\\partial y} e^y \\\\\n    &= \\left[\\left. f(x) \\right|_{x = e^y}\\right] e^y \\\\\n    &= f\\left(e^y\\right) e^y \\\\\n    &= \\frac{1}{\\sigma} e^{-e^y/\\sigma} e^y\n\\end{aligned}\n$$\n\nwhich will be valid for all real values of $y$; that is, the support of $Y$ is all real numbers.  In line 2 above, we applied the chain rule to compute the derivative, avoiding the need to explicitly state the CDF of $X$.\n:::\n\n:::{.callout-warning}\nWhile mathematicians distinguish between a derivative $\\frac{d}{dx}$ and a partial derivative $\\frac{\\partial}{\\partial x}$, we do not make that distinction.\n:::\n\n[^tables]: A good [table of common distributions](https://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf) is given in Casella and Berger, a popular text for statistical theory at the graduate level.\n\n[^relationships]: An excellent [summary of the relationships between Distributions](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html) was developed by faculty at the College of William and Mary.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}