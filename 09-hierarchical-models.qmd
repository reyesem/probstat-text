# Hierarchical Models {#sec-hierarchical-models}

{{< include _setupcode.qmd >}}

We have primarily addressed models where the only random variable was the response.  All terms in the model were either fixed predictors or fixed (albeit unknown) parameters.  In this model, we consider a hierarchical models.

## Motivating Example
Consider obtaining the weight of an individual.  We might randomly select the individual from a population; then, we place them on a scale, which is subject to measurement error.  Since the scale is subject to measurement error, we do not observe their actual weight; instead, we see some jittered version of their weight.  So, we might want to take multiple weight readings.  As a result, the $j$-th weight we observe $Y_j$ on the subject is the sum of the individual's true weight $\theta$ and the measurement error $\varepsilon_j$ in the scale on the $j$-th reading:

$$Y_j = \theta + \varepsilon_j.$$

This looks a lot like a linear model with only an intercept.  However, the primary difference is that this is for multiple observations from a single unit!  We would of course not only measure the weight of one person from the population but a sample of size $n$ from the population.  Therefore, our model would become

$$Y_{i,j} = \theta_i + \varepsilon_{i,j}$$ {#eq-hier-one}

where 

  - $Y_{i,j}$ represents the $j$-th weight measurement taken on the $i$-th person (remember, we record each person's weight multiple times),
  - $\theta_i$ represents the true weight for person $i$, and
  - $\varepsilon_{i,j}$ represents the measurement error in the $j$-th weight measurement taken on the $i$-th person.
  
And, since the measurement error is random, we might posit a model for its distribution.  For example, we might say that

$$\varepsilon_{i,j} \stackrel{\text{IID}}{\sim} N\left(0, \sigma^2\right).$$
  
Notice that unlike our previous models, the unknown terms $\theta_1, \theta_2, \dotsc, \theta_n$ change with each person.  More, since the individuals are from some underlying population, we might think about their weights as coming from some underlying distribution.  That is, we might posit that

$$\theta_i \stackrel{\text{IID}}{\sim} N\left(\mu, \eta^2\right)$$

for some unknown parameters $\mu$ and $\eta^2$.  Here, $\mu$ represents the average weight of individuals in the population.  

We now have two distributional models; putting this together, we can write

$$
\begin{aligned}
  Y_{i,j} \mid \theta_j &\stackrel{\text{Ind}}{\sim} N\left(\theta_j, \sigma^2\right) \\
  \theta_j &\stackrel{\text{IID}}{\sim} N\left(\mu, \eta^2\right).
\end{aligned}
$$

That is, the distributional model for the observed weight is _conditional_ on knowing the true weight of the individual, and this true weight has its own distribution.  This is an example of a __hierarchical model__.

:::{#def-hierarchical-model}
## Hierarchical Model
A distributional model for a response that is constructed in layers.  The top-most layer is conditional on components that are expressed in lower layers.
:::

Hierarchical models rely on the notion of conditional probability.

## Quick Review of Conditional Probability

:::{#def-conditional-probability}
## Conditional Probability
Let $A$ and $B$ be events with non-zero probability.  Then, the probability that event $A$ occurs given that event $B$ occurs (written $A \mid B$) is given by

$$Pr(A \mid B) = \frac{Pr(A \cap B)}{Pr(B)}.$$
:::

The notion of conditional probability constrains the sample space to consider only that region where the event $B$ does occur.  We are essentially looking for the proportional area that $A$ takes up within $B$.  We generalize this definition to random variables as well.  In order to do so, we need the notion of a joint density function.

:::{#def-joint-density}
## Joint Density
Let $X$ and $Y$ be continuous random variables.  Then, the joint density function of $X$ and $Y$, written $f_{X,Y}(x,y)$ is the function such that

$$Pr(a < X < b, c < Y < d) = \int_{c}^{d} \int_{a}^{b} f_{X,Y}(x,y) dx dy.$$
:::

While we have defined a joint density function for two continuous random variables, we could create an analogous definition for two discrete random variables or a mix of the two.  We can now define a conditional density.

:::{#def-conditional-density}
## Conditional Density
Let $X$ and $Y$ be continuous random variables.  Then, the density function of $X$ given the value of $Y$, written as $f_{X \mid Y}(x \mid y)$ is given by

$$f_{X \mid Y}(x \mid y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}$$

where $f_{X,Y}(x,y)$ is the joint density function.
:::

This is always a challenging part of a course in probability theory.  Here, we are saying that for each value the random variable $Y$ takes, a new _distribution_ for $X$ is created.  Knowing information about one variable informed the distribution of the other.

Just as we talk about how a random variable "varies" (the variability of the distribution), we can talk about how two random variables "co-vary."  

:::{#def-covariance}
## Covariance
Let $X$ and $Y$ be random variables.  The covariance of $X$ and $Y$, written $Cov(X, Y)$ is defined as

$$Cov(X,Y) = E\left[(X - E(X))(Y - E(Y))\right] = E(XY) - E(X) E(Y).$$
:::


## Properties for the Simple Hierarchical Model
Consider the simple hierarchical model from our motivating example:

$$Y_{i,j} = \theta_i + \varepsilon_{i,j}.$$

Since the response is a sum of random variables, we trivially have that

$$E\left(Y_{i,j}\right) = E\left(\theta_i\right) + E\left(\varepsilon_{i,j}\right) = \mu.$$

So, the mean response across all observed observations is the average weight in the population.  Since we assume that the $\theta_i$ terms are independent of the $\varepsilon_{i,j}$ terms, we also readily obtain that

$$Var\left(Y_{i,j}\right) = Var\left(\theta_i\right) + Var\left(\varepsilon_{i,j}\right) = \eta^2 + \sigma^2.$$

That is, the variability in the response is the result of the variability across individuals as well as the variability due to measurement error.  And, if the measurement error did not exist ($\sigma^2 = 0$), then we would have that $Y_{i,1} = Y_{i,2} = \dotsb = Y_{i,m}$.

:::{#exm-covariance-blocks}
## Covariance in Hierarchical Model
Consider the simple hierarchical model

$$Y_{i,j} = \theta_i + \varepsilon_{i,j}$$

where $\theta_i \stackrel{\text{IID}}{\sim} N\left(\mu, \eta^2\right)$, $\varepsilon_{i,j} \stackrel{\text{IID}}{\sim} N\left(0, \sigma^2\right)$, and $\theta_i$ independent of $\varepsilon_{i,j}$ for all $i$ and $j$.  Determine the covariance between two observations made on the same unit, and determine the covariance between two observations made on different units.
:::

:::{.soln}
First, consider two observations made on different units.  Without loss of generality, we consider $Y_{1,j}$ and $Y_{2,j}$.  We are interested in finding the covariance between $Y_{1,j}$ and $Y_{2,j}$.  By definition

$$Cov\left(Y_{1,j}, Y_{2,j}\right) = E\left(Y_{1,j} Y_{2,j}\right) - E\left(Y_{1,j}\right)E\left(Y_{2,j}\right).$$

We have already established the mean of each observation, $\mu$.  Plugging in the form of the linear model for $Y_{1,j}$ and $Y_{2,j}$ and expanding, we have that

$$
\begin{aligned}
  Cov\left(Y_{1,j}, Y_{2,j}\right)
    &= E\left(Y_{1,j} Y_{2,j}\right) - E\left(Y_{1,j}\right)E\left(Y_{2,j}\right) \\
    &= E\left(\theta_1\theta_2 + \theta_1 \varepsilon_{2,j} + \theta_2 \varepsilon_{1,j} + \varepsilon_{1,j}\varepsilon_{2,j}\right) - \mu^2 \\
    &= E\left(\theta_1\right)E\left(\theta_2\right) + E\left(\theta_1\right) E\left(\varepsilon_{2,j}\right) + E\left(\theta_2\right) E\left(\varepsilon_{1,j}\right) + E\left(\varepsilon_{1,j}\right)E\left(\varepsilon_{2,j}\right) - \mu^2 
\end{aligned}
$$

where the expectations separate due to the independence between the $\theta$ and $\varepsilon$ terms.  Now, plugging in, we have that

$$Cov\left(Y_{1,j}, Y_{2,j}\right) = \mu^2 + 0 + 0 + 0 - \mu^2 = 0.$$

We now consider two observations from the same unit; without loss of generality, we consider $Y_{i,1}$ and $Y_{i,2}$ (the same $i$ ensures they are from the same subject). Following a similar process as before, we have

$$
\begin{aligned}
  Cov\left(Y_{i,1}, Y_{i,2}\right)
    &= E\left(Y_{i,1} Y_{i,2}\right) - E\left(Y_{i,1}\right)E\left(Y_{i,2}\right) \\
    &= E\left(\theta_i\theta_i + \theta_i \varepsilon_{i,1} + \theta_i \varepsilon_{i,2} + \varepsilon_{i,1}\varepsilon_{i,2}\right) - \mu^2 \\
    &= E\left(\theta_i^2\right) + E\left(\theta_i\right) E\left(\varepsilon_{i,1}\right) + E\left(\theta_i\right) E\left(\varepsilon_{i,2}\right) + E\left(\varepsilon_{i,1}\right)E\left(\varepsilon_{i,2}\right) - \mu^2 \\
    &= E\left(\theta_i^2\right) - \mu^2
\end{aligned}
$$

where the expectations separate due to the independence.  However, we still have one term left to resolve.  Remembering the definition of variance, we have that

$$Cov\left(Y_{i,1}, Y_{i,2}\right) = Var\left(\theta_i\right) + E^2\left(\theta_i\right) - \mu^2 = \eta^2 + \mu^2 - \mu^2 = \eta^2.$$

That is, observations from the same subject are correlated with one another.
:::


## Repeated Measures as a Hierarchical Model
The repeated measures ANOVA model views the mean response as a function of a factor of interest and a block term.  Specifically, we write

$$Y_i = \sum_{j=1}^{k} \mu_j (\text{Group } j)_i + \sum_{b=1}^{m} \beta_b (\text{Block } b)_i + \varepsilon_i$$

where

$$
\begin{aligned}
  (\text{Group } j)_i 
    &= \begin{cases}
      1 & \text{if i-th observation comes from group j} \\
      0 & \text{otherwise}
      \end{cases} \\
  (\text{Block } b)_i
    &= \begin{cases}
      1 & \text{if i-th observation comes from block b} \\
      0 & \text{otherwise}
      \end{cases}
\end{aligned}
$$

are indicator variables.  It is common to assume the block effects, $\beta_1, \beta_2, \dotsc, \beta_b$ are random variables that are independent of the treatment groups and the error in the response $\varepsilon_i$; further, it is common to assume that

$$\beta_j \stackrel{\text{IID}}{\sim} N\left(0, \sigma^2_b\right).$$

That is, the repeated measures ANOVA model is a hierarchical model.  The distribution of the response can only be specified if we know the block effect, and the block effect itself is a random variable.

Viewing it as a hierarchical model allows us to see where the correlation structure comes from.  All observations that share a similar value of $\beta_b$ are associated in some way.