<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Statistical Theory - 3&nbsp; Sampling Distribution of a Statistic</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-inference.html" rel="next">
<link href="./02-randomvariables.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="mystyles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03-samplingdistributions.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Sampling Distribution of a Statistic</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Intro Statistical Theory</a> 
        <div class="sidebar-tools-main">
    <a href="./ma382-text.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Essential Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-randomvariables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Variables and Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-samplingdistributions.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Sampling Distribution of a Statistic</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Inference for a Population Mean</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Inference for Regression Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-normality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">More on the Classical Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-location-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Location Scale Families</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#statistics-and-expectations" id="toc-statistics-and-expectations" class="nav-link active" data-scroll-target="#statistics-and-expectations"><span class="header-section-number">3.1</span> Statistics and Expectations</a></li>
  <li><a href="#sampling-distribution-of-sample-mean" id="toc-sampling-distribution-of-sample-mean" class="nav-link" data-scroll-target="#sampling-distribution-of-sample-mean"><span class="header-section-number">3.2</span> Sampling Distribution of Sample Mean</a></li>
  <li><a href="#properties-of-the-moment-generating-function" id="toc-properties-of-the-moment-generating-function" class="nav-link" data-scroll-target="#properties-of-the-moment-generating-function"><span class="header-section-number">3.3</span> Properties of the Moment-Generating Function</a></li>
  <li><a href="#bootstrapping" id="toc-bootstrapping" class="nav-link" data-scroll-target="#bootstrapping"><span class="header-section-number">3.4</span> Bootstrapping</a></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application"><span class="header-section-number">3.5</span> Application</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-samplingdistributions" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Sampling Distribution of a Statistic</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In the previous chapter, we related probability concepts associated with random variables to their statistical counterparts in data collection. We introduced the idea of using a density function as a model of the distribution of a variable within the population. This led to the observation that the population parameters govern the shape of these density functions. Further, these parameters are the focal point of research objectives. In a statistical analysis, the parameters would be estimated using statistics; in this chapter, we explore how probability theory helps us to characterize the variability in these statistics — which is the key component of statistical inference.</p>
<section id="statistics-and-expectations" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="statistics-and-expectations"><span class="header-section-number">3.1</span> Statistics and Expectations</h2>
<p>The goal of statistics is to use a sample to say something about the underlying population. Consider the following research objective:</p>
<blockquote class="blockquote">
<p>Estimate the cost (in US dollars) of a diamond for sale in the United States.</p>
</blockquote>
<p>No researcher would believe that measuring the cost of a single diamond would be sufficient to address the above research objective. Instead, we would consider taking a sample of <span class="math inline">\(n\)</span> diamonds and measuring the cost of each. We can represent the cost we will record (note the use of the future tense) as <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span>, where <span class="math inline">\(X_i\)</span> is the cost of the <span class="math inline">\(i\)</span>-th diamond we will measure. Collecting data on a sample requires that we deal with at least <span class="math inline">\(n\)</span> random variables (one measurement for each of the observations in our sample).</p>
<p>In almost every analysis, we compute a numerical summary of the data. For example, the sample mean takes the form</p>
<p><span class="math display">\[\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i.\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>While a capital letter denotes a random variable (not yet observed), the corresponding lowercase letter is used to denote a past observation (which is no longer random).</p>
</div>
</div>
<p>These numerical summaries, statistics, are functions of the data. So, prior to when we collect that data, our statistics are functions of random variables.</p>
<div id="def-statistic" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 (Statistic) </strong></span>A statistic is a numerical summary of a sample; it is a function of the data alone. Prior to collecting data, a statistic is a function of the data to be collected.</p>
</div>
<p><a href="#def-statistic">Definition&nbsp;<span>3.1</span></a> eliminates the possibility of the statistic being a function of the underlying <em>parameters</em>; certainly, the behavior of the statistic is determined by the parameters, but the computation of a statistic should not require knowledge of the parameter once the data is collected.</p>
<p><a href="#def-statistic">Definition&nbsp;<span>3.1</span></a> highlights the need to study functions and combinations of random variables. If you recall, <a href="02-randomvariables.html#thm-expectation">Theorem&nbsp;<span>2.1</span></a> introduced the idea of expectation as a linear operator. The result, for a single random variable, is nearly intuitive. If expectation is associated with integration (as defined in <a href="02-randomvariables.html#def-mean">Definition&nbsp;<span>2.6</span></a>), then expectations should adopt the properties of integration, including linearity. The generalization of this result is extremely important within statistics.</p>
<div id="thm-linearity-of-expectations" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Linearity of Expectations) </strong></span>For random variables <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span>, constants <span class="math inline">\(a_1, a_2, \dotsc, a_n\)</span> and real-valued functions <span class="math inline">\(g_1, g_2, \dotsc, g_n\)</span>, we have that</p>
<p><span class="math display">\[E\left[\sum_{i=1}^{n} a_i g\left(X_i\right)\right] = \sum_{i=1}^{n} a_i E\left[g\left(X_i\right)\right].\]</span></p>
</div>
<p>The importance of <a href="#thm-linearity-of-expectations">Theorem&nbsp;<span>3.1</span></a> is seen in determining the expectation of the sample mean.</p>
<div id="exm-mean-xbar" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Mean of the Sample Mean) </strong></span>Let <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> be random variables representing observations from a sample that will be collected. Define the sample mean of that future sample to be</p>
<p><span class="math display">\[\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i.\]</span></p>
<p>Determine <span class="math inline">\(E\left(\bar{X}\right)\)</span>.</p>
</div>
<p>Before addressing the prompt in <a href="#exm-mean-xbar">Example&nbsp;<span>3.1</span></a>, we note that <em>before collecting the data</em>, statistics, like the sample mean, is a function of random variables and is therefore itself a random variable! And, all random variables have distributions, and they have parameters that govern the shape of that distribution. <a href="#exm-mean-xbar">Example&nbsp;<span>3.1</span></a> is focused on the mean of the distribution.</p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>Applying <a href="#thm-linearity-of-expectations">Theorem&nbsp;<span>3.1</span></a>, we have</p>
<p><span class="math display">\[
\begin{aligned}
  E\left(\bar{X}\right)
    &amp;= E\left(\frac{1}{n} \sum_{i=1}^{n} X_i\right) \\
    &amp;= \frac{1}{n} \sum_{i=1}^{n} E\left(X_i\right)
\end{aligned}
\]</span></p>
<p>If we assume that each <span class="math inline">\(X_i\)</span> is taken from the same population, then <span class="math inline">\(E\left(X_i\right) = \mu\)</span>, the population mean, for some constant <span class="math inline">\(\mu\)</span>. Therefore, we have that</p>
<p><span class="math display">\[E\left(\bar{X}\right) = \frac{1}{n} \sum_{i=1}^{n} \mu = \mu.\]</span></p>
</div>
<p><a href="#thm-linearity-of-expectations">Theorem&nbsp;<span>3.1</span></a> discusses how expectations work with linear combinations; we have a similar result for variances. However, there is a particular caveat.</p>
<div id="thm-variance-independent-sum" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Variance of the Sum of Independent Random Variables) </strong></span>Let <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> be independent random variables, and define constants <span class="math inline">\(a_1, a_2, \dotsc, a_n\)</span> and real-valued functions <span class="math inline">\(g_1, g_2, \dotsc, g_n\)</span>. Then,</p>
<p><span class="math display">\[Var\left[\sum_{i=1}^{n} a_i g\left(X_i\right)\right] = \sum_{i=1}^{n} a^2_i Var\left[g\left(X_i\right)\right].\]</span></p>
</div>
<p>Comparing <a href="#thm-linearity-of-expectations">Theorem&nbsp;<span>3.1</span></a> to <a href="#thm-variance-independent-sum">Theorem&nbsp;<span>3.2</span></a>, we see that while the expectation always move through sums, the variance only does so when the random variables are independent.</p>
<div id="def-independence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (Independence) </strong></span>Random variables <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> are said to be mutually independent (or just “independent”) if and only if</p>
<p><span class="math display">\[Pr\left(X_1 \in A_1, X_2 \in A_2, \dotsb, X_n \in A_n\right) = \prod_{i=1}^{n} Pr\left(X_i \in A_i\right),\]</span></p>
<p>where <span class="math inline">\(A_1, A_2, \dotsc, A_n\)</span> are arbitrary sets.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For those not familiar, <span class="math inline">\(\prod_{i=1}^n a_i\)</span> is the <em>product operator</em>. It is analogous to <span class="math inline">\(\sum_{i=1}^{n} a_i\)</span>, but uses products instead of sums.</p>
</div>
</div>
<p>Essentially, a random variable <span class="math inline">\(X\)</span> is said to be independent of <span class="math inline">\(Y\)</span> if the likelihood that <span class="math inline">\(X\)</span> takes a particular value is the same regardless of the value <span class="math inline">\(Y\)</span> takes.</p>
<p>Consider taking a sample of <span class="math inline">\(n\)</span> diamonds to address our above research objective. It seems reasonable that the cost of one diamond does not depend on the cost of another. That allows us to assume the values we collect will be independent; that is, the random variables we use to represent these costs are independent of one another. It also seems natural to assume that each diamond we select comes from the same underlying population. These two attributes together form the basis of what we mean by a “random sample.”</p>
<div id="def-random-sample" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 (Random Sample) </strong></span>A random sample of size <span class="math inline">\(n\)</span> refers to a collection of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> such that the random variables are mutually independent, and the distribution of each random variable is identical.</p>
<p>We say <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> are independent and identically distributed, abbreviated IID. We might also write this as <span class="math inline">\(X_i \stackrel{\text{IID}}{\sim} f\)</span> for some density <span class="math inline">\(f\)</span>.</p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be identically distributed random variables. This does not mean that <span class="math inline">\(X = Y\)</span>. That is, the two random variables need not take on the same value. Instead, identically distributed means the density function of the two random variables are the same. As a result, they share the same mean, variance, etc. That is, their distributions are the same, not their value.</p>
</div>
</div>
<div id="exm-var-xbar" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (Variance of the Sample Mean) </strong></span>Let <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a population with a variance of <span class="math inline">\(\sigma^2\)</span>. Define the sample mean of that future sample to be</p>
<p><span class="math display">\[\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i.\]</span></p>
<p>Determine <span class="math inline">\(Var\left(\bar{X}\right)\)</span>.</p>
</div>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>Since <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> form a random sample; we know that they are mutually independent. Therefore, <a href="#thm-variance-independent-sum">Theorem&nbsp;<span>3.2</span></a> gives</p>
<p><span class="math display">\[
\begin{aligned}
  Var\left(\bar{X}\right)
    &amp;= Var\left(\frac{1}{n} \sum_{i=1}^{n} X_i\right) \\
    &amp;= \frac{1}{n^2} \sum_{i=1}^{n} Var\left(X_i\right) \\
    &amp;= \frac{1}{n^2} \sum_{i=1}^{n} \sigma^2 \\
    &amp;= \sigma^2 / n.
\end{aligned}
\]</span></p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is important to remember that <a href="#exm-mean-xbar">Example&nbsp;<span>3.1</span></a> and <a href="#exm-var-xbar">Example&nbsp;<span>3.2</span></a> describe the mean and variance of the sample mean <em>prior to</em> collecting data. Once the data is collected, the sample mean has no distribution. Once the data is collected, a statistic is simply a number.</p>
</div>
</div>
<p>There is one additional result for independent random variables that we should keep in mind.</p>
<div id="def-product-expectations" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.4 (Expectation of a Product of Independent Random Variables) </strong></span>Let <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> be independent random variables, then</p>
<p><span class="math display">\[E\left(\prod_{i=1}^n X_i\right) = \prod_{i=1}^{n} E\left(X_i\right).\]</span></p>
</div>
</section>
<section id="sampling-distribution-of-sample-mean" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sampling-distribution-of-sample-mean"><span class="header-section-number">3.2</span> Sampling Distribution of Sample Mean</h2>
<p>Together, <a href="#exm-mean-xbar">Example&nbsp;<span>3.1</span></a> and <a href="#exm-var-xbar">Example&nbsp;<span>3.2</span></a> characterize the center and spread of the distribution of the sample mean. Since each statistic is a random variable, it has a distribution, and we call that distribution a <strong>sampling distribution</strong>.</p>
<div id="def-sampling-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.5 (Sampling Distribution) </strong></span>The distribution of a statistic across repeated samples.</p>
</div>
<p>Note that <a href="#def-sampling-distribution">Definition&nbsp;<span>3.5</span></a> makes use of the idea of repeated sampling. Once again, this leans on the frequentist interpretation of probability (<a href="01-fundamentals.html#def-frequentist-interpretation">Definition&nbsp;<span>1.5</span></a>). We are thinking of the distribution of a statistic as the result of the different values that could potentially be observed if we were to repeatedly take samples of the same size.</p>
<p>While <a href="#thm-linearity-of-expectations">Theorem&nbsp;<span>3.1</span></a> and <a href="#thm-variance-independent-sum">Theorem&nbsp;<span>3.2</span></a> allowed us to characterize the sampling distribution of the sample mean, these results did not provide information about the shape of the sampling distribution, much less provide a functional form of the density. To make progress on this front, we return to another tool introduced in a probability course — the <strong>moment-generating function</strong>.</p>
<div id="def-mgf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.6 (Moment-Generating Function (MGF)) </strong></span>For a random variable <span class="math inline">\(X\)</span>, let <span class="math inline">\(M_X(t)\)</span> be defined as</p>
<p><span class="math display">\[M_X(t) = E\left(e^{tX}\right).\]</span></p>
<p>If <span class="math inline">\(M_X(t)\)</span> is defined for all values of <span class="math inline">\(t\)</span> in some interval about 0, then <span class="math inline">\(M_X(t)\)</span> is called the moment-generating function (MGF) of <span class="math inline">\(X\)</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we are working with multiple random variables, it is common to use a subscript to denote the random variable we are referencing. For example, <span class="math inline">\(F_X\)</span> may represent the CDF of the random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(f_Y\)</span> denote the density function of the random variable <span class="math inline">\(Y\)</span>, and <span class="math inline">\(M_Z(t)\)</span> denote the MGF of the random variable <span class="math inline">\(Z\)</span>.</p>
</div>
</div>
<p>First, we note that the MGF is a function, but not a function of the random variable. That is, <span class="math inline">\(M_X(t)\)</span> is not a random variable since it is the result of taking the expected value of a random variable. Second, the definition does not guarantee the existence of the MGF for any particular random variable. That is, there are distributions for which the MGF is not defined. The power of the MGF is summarized in <span class="quarto-unresolved-ref">?thm-mgf-properties</span>.</p>
</section>
<section id="properties-of-the-moment-generating-function" class="level2 {@thm-mgf-properties}" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="properties-of-the-moment-generating-function"><span class="header-section-number">3.3</span> Properties of the Moment-Generating Function</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable with moment-generating function <span class="math inline">\(M_X(t).\)</span> Then, we have that</p>
<ol type="1">
<li><span class="math inline">\(E\left(X^k\right) = M_X^{(k)}(0)\)</span> for all integers <span class="math inline">\(k\)</span>, where <span class="math inline">\(M_X(k)(0)\)</span> is the <span class="math inline">\(k\)</span>-th derivative of <span class="math inline">\(M_X(t)\)</span> evaluated at <span class="math inline">\(t = 0\)</span>.</li>
<li>The MGF uniquely defines the random variable. That is, if two random variables have the same MGF, then those random variables have the same density function.</li>
</ol>
</section>
<p>Consider again our sample of <span class="math inline">\(n\)</span> diamonds; let’s assume it is a random sample from the population. Suppose we are interested in the total number of diamonds within this sample that have a “princess” cut. Define the random variable</p>
<p><span class="math display">\[Y_i = \begin{cases} 1 &amp; \text{if i-th diamond has a princess cut} \\ 0 &amp; \text{otherwise}. \end{cases}\]</span></p>
<p>If we are intersted in the total number of diamonds within the sample that have a princess cut, we are interested in the statistic <span class="math inline">\(\sum_{i=1}^{n} Y_i\)</span>. We can use <span class="quarto-unresolved-ref">?thm-mgf-properties</span>, together with our previous results about expectations to derive the sampling distribution of this statistic.</p>
<div id="exm-bernoulli-sum" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (Sampling Distribution of a Sum of Bernoulli Random Variables) </strong></span>Let <span class="math inline">\(Y_1, Y_2, \dotsc, Y_n\)</span> be IID random variables from a Bernoulli distribution with probability <span class="math inline">\(\theta\)</span>. Define</p>
<p><span class="math display">\[Z = \sum_{i=1}^{n} Y_i,\]</span></p>
<p>the total number of “successes” in the random sample. Determine the sampling distribution of <span class="math inline">\(Z\)</span>.</p>
</div>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>In order to determine the distribution of <span class="math inline">\(Z\)</span>, we find its MGF.</p>
<p><span class="math display">\[
\begin{aligned}
  M_Z(t)
    &amp;= E\left(e^{tZ}\right) \\
    &amp;= E\left(e^{t\sum_{i=1}^{n} Y_i}\right) \\
    &amp;= E\left(\prod_{i=1}^{n} e^{tY_i}\right),
\end{aligned}
\]</span></p>
<p>where the third line results from recognizing that the product of exponential terms is the exponential of the sum of the powers. Now, applying <span class="quarto-unresolved-ref">?thm-product-expectation</span>, we have</p>
<p><span class="math display">\[
\begin{aligned}
  M_Z(t)
    &amp;= \prod_{i=1}^{n} E\left(e^{tY_i}\right) \\
    &amp;= \prod_{i=1}^{n} M_{Y_i}(t) \\
    &amp;= \prod_{i=1}^{n} M_{Y_1}(t)
\end{aligned}
\]</span></p>
<p>where the last line is the result of the random variables being identically distributed. Since they have the same distribution, they must have the same MGF’s; therefore, <span class="math inline">\(M_{Y_i}(t) = M_{Y_1}(t)\)</span> for each <span class="math inline">\(i\)</span>. Again, we are <em>not</em> saying <span class="math inline">\(Y_i = Y_1\)</span>; we are saying the moment-generating functions of these random variables is equivalent.</p>
<p>Consulting a table to determine the MGF of a Bernoulli random variable, we have that</p>
<p><span class="math display">\[M_{Y_1}(t) = (1 - \theta) + \theta e^t.\]</span></p>
<p>Thus, we have that</p>
<p><span class="math display">\[
\begin{aligned}
  M_Z(t)
    &amp;= \prod_{i=1}^{n} M_{Y_1}(t) \\
    &amp;= \left[M_{Y_1}(t)\right]^n \\
    &amp;= \left[(1 - \theta) + \theta e^{t}\right]^n.
\end{aligned}
\]</span></p>
<p>But, consulting a table of common distributions, we recognize <span class="math inline">\(M_Z(t)\)</span> as the moment-generating function of a Binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>. Since moment-generating functions uniquely define a distribution when they exist, we have that <span class="math inline">\(Z \sim Bin(n, \theta)\)</span>.</p>
</div>
<p>The next chapter will illustrate how we can capitalize on this information. For this chapter, we simply note that we are able to characterize how the sum of Bernoulli random variables behaves. We note that this sampling distribution depends on the unknown parameter <span class="math inline">\(\theta\)</span> that also governs the underlying population. In addition, it depends on the sample size.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>The sampling distribution of a statistic depends on both the parameters from the underlying population as well as the sample size.</p>
</div>
</div>
<p>While the previous example illustrates the process, we admit that it simply reiterates a fact that we already knew (and noted in <a href="02-randomvariables.html#def-binomial-distribution">Definition&nbsp;<span>2.12</span></a>). The utility of the next result may be a bit more apparent.</p>
<div id="exm-normal-mean" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 (Sampling Distribution of the Sample Mean from a Normal Population) </strong></span>Let <span class="math inline">\(Y_1, Y_2, \dotsc, Y_n \stackrel{\text{IID}}{\sim} N\left(\mu,\sigma^2\right)\)</span>. Define</p>
<p><span class="math display">\[\bar{Y} = \frac{1}{n}\sum_{i=1}^{n} Y_i,\]</span></p>
<p>the sample mean. Determine the sampling distribution of <span class="math inline">\(\bar{Y}\)</span>.</p>
</div>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>In order to determine the distribution of <span class="math inline">\(\bar{Y}\)</span>, we find its MGF.</p>
<p><span class="math display">\[
\begin{aligned}
  M_{\bar{Y}}(t)
    &amp;= E\left(e^{t\bar{Y}}\right) \\
    &amp;= E\left(e^{tn^{-1}\sum_{i=1}^{n} Y_i}\right) \\
    &amp;= E\left(\prod_{i=1}^{n} e^{tn^{-1}Y_i}\right)
\end{aligned}
\]</span></p>
<p>where the third line results from recognizing that the product of exponential terms is the exponential of the sum of the powers. Now, applying <span class="quarto-unresolved-ref">?thm-product-expectation</span>, we have</p>
<p><span class="math display">\[
\begin{aligned}
  M_{\bar{Y}}(t)
    &amp;= \prod_{i=1}^{n} E\left(e^{tn^{-1}Y_i}\right) \\
    &amp;= \prod_{i=1}^{n} M_{Y_i}(t/n) \\
    &amp;= \prod_{i=1}^{n} M_{Y_1}(t/n)
\end{aligned}
\]</span></p>
<p>where the last line is the result of the random variables being identically distributed, and the second line makes use of the definition of the MGF, which can be evaluated at any value, including <span class="math inline">\(t/n\)</span>. Since they have the same distribution, they must have the same MGF’s; therefore, <span class="math inline">\(M_{Y_i}(t) = M_{Y_1}(t)\)</span> for each <span class="math inline">\(i\)</span> and all <span class="math inline">\(t\)</span>. Again, we are <em>not</em> saying <span class="math inline">\(Y_i = Y_1\)</span>; we are saying the moment-generating functions of these random variables is equivalent.</p>
<p>Consulting a table to determine the MGF of a Normal random variable, we have that</p>
<p><span class="math display">\[M_{Y_1}(t) = e^{\mu t + (1/2) \sigma^2 t^2}.\]</span></p>
<p>Thus, we have that</p>
<p><span class="math display">\[
\begin{aligned}
  M_{\bar{Y}}(t)
    &amp;= \prod_{i=1}^{n} M_{Y_1}(t/n) \\
    &amp;= \left[M_{Y_1}(t/n)\right]^n \\
    &amp;= \left[e^{\mu t/n + (1/2) \sigma^2 t^2/n^2}\right]^n \\
    &amp;= e^{\mu t + (1/2)(\sigma^2 / n) t^2}.
\end{aligned}
\]</span></p>
<p>But, consulting a table of common distributions, we recognize <span class="math inline">\(M_{\bar{Y}}(t)\)</span> as the moment-generating function of a Normal distribution with a mean of <span class="math inline">\(\mu\)</span> and a variance of <span class="math inline">\(\sigma^2/n\)</span>. Since moment-generating functions uniquely define a distribution when they exist, we have that <span class="math inline">\(\bar{Y} \sim N\left(\mu, \sigma^2/n\right)\)</span>.</p>
</div>
<p>We note a difference between <a href="#exm-bernoulli-sum">Example&nbsp;<span>3.3</span></a> and <a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a>. In <a href="#exm-bernoulli-sum">Example&nbsp;<span>3.3</span></a>, the statistic we examined did not estimate a parameter of interest. While there is nothing wrong with examining the total number of diamonds in a sample, the statistic itself is dependent on the sample size — we would expect a larger value with larger samples. In <a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a>, however, the sample mean is a common estimate of the population mean. It turns out the sampling distributions of most estimators (statistics chosen to estimate a parameter) tend to share similar characteristics.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Common Characteristics of Sampling Distributions
</div>
</div>
<div class="callout-body-container callout-body">
<p>While not guaranteed, the sampling distribution of many statistics tend to have the following characteristics:</p>
<ol type="1">
<li>The sampling distribution is centered on the corresponding parameter of interest, or the center approaches the corresponding parameter as the sample size increases.</li>
<li>The spread of the sampling distribution is smaller than within the population, and the spread decreases as the sample size increases.</li>
<li>The shape of the sampling distribution differs from that of the underlying population, and the sampling distribution becomes more bell-shaped as the sample size increases.</li>
</ol>
<p>Of the three characteristics above, the third is the one most likely to be broken.</p>
</div>
</div>
<p>Considering <a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a>, we see that all three characteristics hold. First, we see (as we also saw in <a href="#exm-mean-xbar">Example&nbsp;<span>3.1</span></a>) that the expected value of the sample mean is the population mean. When this occurs, we say the estimator is <strong>unbiased</strong>.</p>
<div id="def-unbiased" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.7 (Unbiased) </strong></span>An estimator (statistic) <span class="math inline">\(\widehat{\theta}\)</span> is said to be unbiased for the parameter <span class="math inline">\(\theta\)</span> if</p>
<p><span class="math display">\[E\left(\widehat{\theta}\right) = \theta.\]</span></p>
</div>
<p>While being unbiased is a good quality in an estimator, it is not required. For example, the sample standard deviation is not an unbiased estimator of the population standard deviation, yet it is still a preferred estimator.</p>
<p>In <a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a>, we see that variance of the sample mean is smaller than the variance of the population by a factor of <span class="math inline">\(n\)</span>; therefore, as the sample size increases, the variability of the sample mean decreases. This implies that the sample mean of a large sample will tend not to stray as far from the population mean as that of a smaller sample. This is where the notion of “more data is better” comes from.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Larger samples result in more reliable statistics.</p>
</div>
</div>
<p>Finally, we see in <a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a> that the sampling distribution of the sample mean is a Normal distribution, which is bell-shaped. Again, being bell-shaped is not necessarily more advantageous than any other distribution, but it reinforces the idea that the statistic tends to be near the parameter of interest across repeated sampling.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Remember, these discussions are about the distribution of a statistic across repeated samples, and so they apply prior to collecting data. Once we have a sample, the statistic does not have a distribution.</p>
</div>
</div>
<p><a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a> is a really nice result because it tells us the behavior of a popular statistic; unfortunately, it only applies to a sample from a population which follows a Normal distribution. More, it only applies when the population variance is known, which rarely happens in practice. <a href="#thm-students-theorem">Theorem&nbsp;<span>3.3</span></a> generalizes the results to the case when the population variance is unknown.</p>
<div id="thm-students-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (Student’s Theorem) </strong></span>Let <span class="math inline">\(Y_1, Y_2, \dotsc, Y_n \stackrel{\text{IID}}{\sim} N\left(\mu,\sigma^2\right)\)</span>. Define</p>
<p><span class="math display">\[\bar{Y} = \frac{1}{n}\sum_{i=1}^{n} Y_i\]</span></p>
<p>to be the sample mean and</p>
<p><span class="math display">\[S^2 = \frac{1}{n-1} \sum_{i=1}^{n} \left(Y_i - \bar{Y}\right)^2\]</span></p>
<p>to be the sample variance. Then,</p>
<p><span class="math display">\[\frac{\sqrt{n}\left(\bar{Y} - \mu\right)}{S} \sim t_{n-1}.\]</span></p>
</div>
<p>To see the impact of estimating the population variance, we recognize that <a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a> gave us that when the population variance is known</p>
<p><span class="math display">\[\bar{Y} \sim N\left(\mu, \sigma^2/n\right),\]</span></p>
<p>which can be rewritten as</p>
<p><span class="math display">\[\frac{\sqrt{n}\left(\bar{Y} - \mu\right)}{\sigma} \sim N(0, 1).\]</span></p>
<p>Therefore, when the population variance is estimated (using the typical sample variance), we have that the sampling distribution of this standardized ratio follows a t-distribution instead of a Standard Normal distribution.</p>
<p><a href="#thm-students-theorem">Theorem&nbsp;<span>3.3</span></a> highlights that we often characterize the distribution of some “standardized statistic” (instead of the statistic that estimates the parameter directly). Exact results like <a href="#exm-normal-mean">Example&nbsp;<span>3.4</span></a> and <a href="#thm-students-theorem">Theorem&nbsp;<span>3.3</span></a> are quite rare. It is more common for us to rely on approximations to the sampling distribution, the most famous of which is the Central Limit Theorem.</p>
<div id="thm-clt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4 (Central Limit Theorem (CLT)) </strong></span>Let <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> be IID random variables such that <span class="math inline">\(E\left(X_i\right) = \mu\)</span> and <span class="math inline">\(Var\left(X_i\right) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>. As <span class="math inline">\(n\)</span> approaches infinity, the ratio</p>
<p><span class="math display">\[\frac{\sqrt{n}\left(\bar{X} - \mu\right)}{S}\]</span></p>
<p>behaves like a Standard Normal random variable. That is,</p>
<p><span class="math display">\[Pr\left(\frac{\sqrt{n}\left(\bar{X} - \mu\right)}{S} \leq q\right) \rightarrow Pr(Z \leq q) \qquad \text{as } n \rightarrow \infty\]</span></p>
<p>where <span class="math inline">\(q\)</span> is any real number, <span class="math inline">\(Z \sim N(0, 1)\)</span>, <span class="math inline">\(\bar{X}\)</span> is the sample mean and <span class="math inline">\(S^2\)</span> is the sample standard deviation.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>“The” Central Limit Theorem is a misnomer; there are actually several Central Limit Theorems which differ in their assumptions. The one most commonly presented in texts uses the population standard deviation <span class="math inline">\(\sigma\)</span> instead of the sample standard deviation <span class="math inline">\(S\)</span> as we have presented it. The more common presentation is easier to prove, but it is far less useful in practice (as the population variance is rarely known). The proof of the version we have presented is beyond the scope of the course but is more useful in practice.</p>
</div>
</div>
<p>Known as a “limit” (or “asymptotic”) result, <a href="#thm-clt">Theorem&nbsp;<span>3.4</span></a> provides an approximation to the sampling distribution. That is, the CLT states that as the sample size gets large, the Standard Normal distribution is a good approximation to the true sampling distribution of this standardized statistic. Of course, that begs the question, “how good is the approximation?” as well as “how large of a sample is large enough?” While we can never address these questions with certainty, there are some graphical techniques for assessing these questions in practice.</p>
<p>The huge draw of the CLT is that it applies under vary weak conditions — the underlying population has a finite mean and variance. For nearly any population, we have an approximation for the sampling distribution of the sample mean.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The above methods describe the actual sampling distribution. Of course, because these describe how a statistic behaves across repeated samples, this is not something we get to observe directly. Instead, the sampling distribution must be modeled. This is often done by replacing the parameters in the sampling distribution with the corresponding estimates from the sample. Therefore, the <em>model</em> for the sampling distribution based on a given sample is really capturing the shape and spread of the sampling distribution.</p>
</div>
</div>
<section id="bootstrapping" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="bootstrapping"><span class="header-section-number">3.4</span> Bootstrapping</h2>
<p>In the above sections, we have discussed analytical methods (both exact and approximation through limit theorems) for the sampling distribution. Given a set of data, we can also construct a model for the sampling distribution empirically. As there is no single Central Limit Theorem, there is no single bootstrapping algorithm. Instead, “bootstrapping” refers to the idea of using resampling methods to model a sampling distribution of a statistic, but the easiest algorithm is defined below.</p>
<div id="def-case-bootstrap" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.8 (Case-Resampling Bootstrap) </strong></span>Let <span class="math inline">\(Y_1, Y_2, \dotsc, Y_n\)</span> be a random sample from an underlying population, and let <span class="math inline">\(\theta\)</span> represent a parameter of interest characterizing the underlying population. Further, define <span class="math inline">\(\widehat{\theta} = h(\mathbf{Y})\)</span> be a statistic which estimates the parameter. The case-resampling bootstrap algorithm proceeds as follows:</p>
<ol type="1">
<li>Take a random sample, with replacement, from the set <span class="math inline">\(\left\{Y_1, Y_2, \dotsc, Y_n\right\}\)</span> of size <span class="math inline">\(n\)</span>. Call these values <span class="math inline">\(Y_1^*, Y_2^*, \dotsc, Y_n^*\)</span>. This is known as a bootstrap resample.</li>
<li>Compute <span class="math inline">\(\widehat{\theta}^* = h\left(\mathbf{Y}^*\right)\)</span> and store this value. This is known as a bootstrap statistic.</li>
<li>Repeat steps 1-2 <span class="math inline">\(m\)</span> times, for some large value of <span class="math inline">\(m\)</span> (say <span class="math inline">\(m = 5000\)</span>). Denote <span class="math inline">\(\widehat{\theta}^*_j\)</span> to be the bootstrap statistic from the <span class="math inline">\(j\)</span>-th bootstrap resample.</li>
</ol>
<p>The empirical distribution of <span class="math inline">\(\widehat{\theta}_1^*, \widehat{\theta}_2^*, \dotsc, \widehat{\theta}_m^*\)</span> will approximate the shape and spread of the sampling distribution of the statistic <span class="math inline">\(h(\mathbf{Y})\)</span>.</p>
</div>
<p>While the proof of the efficacy of a bootstrap algorithm is beyond the scope of this text, we can gain some intuition regarding the process. Let’s start by characterizing the distribution from which the algorithm resamples — the distribution of the sample. When we sample, with replacement, from the original sample, only <span class="math inline">\(n\)</span> values are possible <span class="math inline">\(\left(Y_1, Y_2, \dotsc, Y_n\right)\)</span>. And, each value will be selected with probability <span class="math inline">\(1/n\)</span>. That is, we have that</p>
<p><span class="math display">\[Pr\left(Y_i^* = u\right) = \frac{1}{n} \qquad u \in \left\{Y_1, Y_2, \dotsc, Y_n\right\}.\]</span></p>
<p>The mean of this distribution is represented by</p>
<p><span class="math display">\[\bar{Y} = \frac{1}{n} \sum_{i=1}^{n} Y_i,\]</span></p>
<p>and the variance of this distribution is</p>
<p><span class="math display">\[S^2_n = \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \bar{Y}\right)^2.\]</span></p>
<p>We divide by <span class="math inline">\(n\)</span> instead of <span class="math inline">\(n - 1\)</span> because since we are treating the original sample as a population from which to be sampled, we rely on the formulas from <a href="02-randomvariables.html"><span>Chapter&nbsp;2</span></a>.</p>
<p>If <span class="math inline">\(\widehat{\theta} = \bar{Y}\)</span>, then we have that the sampling distribution of <span class="math inline">\(\widehat{\theta}^* = n^{-1}\sum_{i=1}^{n} Y_i^*\)</span> will have a mean of</p>
<p><span class="math display">\[E\left[\frac{1}{n} \sum_{i=1}^{n} Y_i^*\right] = \bar{Y}\]</span></p>
<p>and a variance of</p>
<p><span class="math display">\[Var\left[\frac{1}{n} \sum_{i=1}^{n} Y_i^*\right] = \frac{S_n^2}{n}.\]</span></p>
<p>These are the direct application of <a href="#exm-mean-xbar">Example&nbsp;<span>3.1</span></a> and <a href="#thm-variance-independent-sum">Theorem&nbsp;<span>3.2</span></a>. And, if <span class="math inline">\(n\)</span> is large enough, we would expect the resulting empirical distribution to approximate that of a Normal distribution as a result of the CLT. This highlights that <em>models</em> of the sampling distribution tend to have similar characteristics to that of sampling distributions.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Common Characteristics of Models for Sampling Distributions
</div>
</div>
<div class="callout-body-container callout-body">
<p>While not guaranteed, the model of a sampling distribution of many statistics tend to have the following characteristics:</p>
<ol type="1">
<li>The model of the sampling distribution is centered on the statistic from the original sample, or the center of the model approaches the statistic from the original sample as the sample size increases.</li>
<li>The spread of the model of the sampling distribution is smaller than within the sample, and the spread decreases as the sample size increases.</li>
<li>The shape of the model of the sampling distribution differs from that of the sample, and the model of the sampling distribution becomes more bell-shaped as the sample size increases.</li>
</ol>
<p>Of the three characteristics above, the third is the one most likely to be broken.</p>
</div>
</div>
</section>
<section id="application" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="application"><span class="header-section-number">3.5</span> Application</h2>
<p>Consider the following research objective:</p>
<blockquote class="blockquote">
<p>Estimate the cost (in US dollars) of a diamond for sale in the United States.</p>
</blockquote>
<p>As stated, this is an ill-posed objective as it is not centered on a parameter. We might refine it to be</p>
<blockquote class="blockquote">
<p>Estimate the average cost (in US dollars) of a diamond for sale in the United States.</p>
</blockquote>
<p>We have a large (<span class="math inline">\(n = 53940\)</span>) sample of diamonds at our disposal that can be used to address this research objective. A plot of the sample is shown in <a href="#fig-samplingdistributions-sample">Figure&nbsp;<span>3.1</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-samplingdistributions-sample" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./images/fig-samplingdistributions-sample-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Histogram of the cost of a diamond; the distribution is skewed right."></p>
<figcaption class="figure-caption">Figure&nbsp;3.1: Distribution of the cost of a diamond sold in the United States.</figcaption>
</figure>
</div>
</div>
</div>
<p>Using the sample, we conduct 5000 bootstrap replications, each time computing the sample mean. The bootstrap sampling distribution is shown in <a href="04-inference.html#fig-samplingdistributions-bootstrap">Figure&nbsp;<span>4.3</span></a>. We have overlayed the model suggested by the CLT as well. Observe that even though the sample was skewed to the right, the model for the sampling distribution is bell-shaped. The center of our model is the observed sample mean of 3933, and the spread of the sampling distribution is much smaller than that observed in the sample. With a large sample size, we see that the empirical model of the sampling distribution is very similar to that suggested by the CLT.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Bootstrapping can be used to qualitatively assess whether the CLT is appropriate for a particular sample. Of course, if we have gone through the effort of constructing an empirical model, we would likely rely on the empirical model.</p>
</div>
</div>
<p><a href="04-inference.html#fig-samplingdistributions-bootstrap">Figure&nbsp;<span>4.3</span></a> does not include the results from <a href="#thm-students-theorem">Theorem&nbsp;<span>3.3</span></a>; the rationale for excluding this result is that a quick glance at <a href="#fig-samplingdistributions-sample">Figure&nbsp;<span>3.1</span></a> is enough to convince us that the underlying population does not follow a Normal distribution; therefore, those results are inappropriate.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-samplingdistributions-bootstrap" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./images/fig-samplingdistributions-bootstrap-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Histogram of bootstrap statistics with a Normal distribution overlayed."></p>
<figcaption class="figure-caption">Figure&nbsp;3.2: Bootstrap sampling distribution of the sample mean cost of a diamond based on the available data. 5000 bootstrap replicates were used.</figcaption>
</figure>
</div>
</div>
</div>
<p>The next chapter considers ways of using the above tools to perform inference.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-randomvariables.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Variables and Distributions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-inference.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Inference for a Population Mean</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>