<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Statistical Theory - 2&nbsp; Random Variables and Distributions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-samplingdistributions.html" rel="next">
<link href="./01-fundamentals.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="mystyles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-randomvariables.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Variables and Distributions</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Intro Statistical Theory</a> 
        <div class="sidebar-tools-main">
    <a href="./ma382-text.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Essential Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-randomvariables.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Variables and Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-samplingdistributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Sampling Distribution of a Statistic</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Inference for a Population Mean</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Inference for Regression Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-normality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">More on the Classical Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-location-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Location Scale Families</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link active" data-scroll-target="#random-variables"><span class="header-section-number">2.1</span> Random Variables</a></li>
  <li><a href="#characterizing-a-distribution" id="toc-characterizing-a-distribution" class="nav-link" data-scroll-target="#characterizing-a-distribution"><span class="header-section-number">2.2</span> Characterizing a Distribution</a>
  <ul class="collapse">
  <li><a href="#common-parameters" id="toc-common-parameters" class="nav-link" data-scroll-target="#common-parameters"><span class="header-section-number">2.2.1</span> Common Parameters</a></li>
  <li><a href="#distribution-function" id="toc-distribution-function" class="nav-link" data-scroll-target="#distribution-function"><span class="header-section-number">2.2.2</span> Distribution Function</a></li>
  </ul></li>
  <li><a href="#common-probability-distributions" id="toc-common-probability-distributions" class="nav-link" data-scroll-target="#common-probability-distributions"><span class="header-section-number">2.3</span> Common Probability Distributions</a></li>
  <li><a href="#transformations-of-a-random-variable" id="toc-transformations-of-a-random-variable" class="nav-link" data-scroll-target="#transformations-of-a-random-variable"><span class="header-section-number">2.4</span> Transformations of a Random Variable</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-randomvariables" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Random Variables and Distributions</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In <a href="01-fundamentals.html"><span>Chapter&nbsp;1</span></a>, we discussed the probability of an “event.” For statisticians, the events of interests center on measurements, or functions of those measurements, that we plan to take. In this chapter, we begin to connect probability to data analysis. Our goal is to reexamine concepts introduced in a probability course, relating them to their data-centric analogues.</p>
<section id="random-variables" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="random-variables"><span class="header-section-number">2.1</span> Random Variables</h2>
<p>Consider collecting data; before the data is collected, we cannot predict with certainty what we will observe. Therefore, we can think of each observation as the result of a random process. These observations are recorded as variables in our dataset. In probability, a <strong>random variable</strong> is used to represent a measurement that results from a random process.</p>
<div id="def-random-variable" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Random Variable) </strong></span>Let <span class="math inline">\(\mathcal{S}\)</span> be the sample space corresponding to a random process; a random variable <span class="math inline">\(X\)</span> is a function mapping elements of the sample space to the real line.</p>
<p>Random variables represent a measurement that will be collected during the course of a study. Random variables are typically represented by a capital letter.</p>
</div>
<p>While for our purposes, it suffices to think of a random variable as a measurement, mathematically, it is a <em>function</em>. The image (or range) of this function is used to broadly classify random variables as <strong>continuous</strong> or <strong>discrete</strong>; we refer to this image as the <strong>support</strong> of the random variable.</p>
<div id="def-support" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Support) </strong></span>The support of a random variable is the set of all possible values the random variable can take.</p>
</div>
<div id="def-rvtypes" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Continuous and Discrete Random Variable) </strong></span>The random variable <span class="math inline">\(X\)</span> is said to be a discrete random variable if its corresponding support is countable. The random variable <span class="math inline">\(X\)</span> is said to be a continuous random variable if the corresponding support is uncountable (such as an interval or a union of intervals on the real line).</p>
</div>
<p>Discrete random variables are analogous to categorical (or qualitative) variables in data analysis; that is, discrete random variables are used to model the result of a random process which categorizes each unit of observation into a group. Continuous random variables are analogous to numeric (or quantitative) variables in data analysis; continuous random variables are used to model the result of a random process which produces a number for which arithmetic makes sense.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Whether we use a continuous or discrete random variable to represent a measurement is not always obvious. Suppose we consider recording the age of a student selected from a class at a university that typically enrolls “traditional” students (those coming directly from high school). Let the random variable <span class="math inline">\(X\)</span> denote the age of the student.</p>
<p>If we record the student’s age in years since birth, <span class="math inline">\(X\)</span> can take on only a finite number of values (most likely <span class="math inline">\(\{18, 19, 20, 21, 22, 23\}\)</span>), making it a discrete random variable. However, if we record the student’s age as the number of seconds since birth, we might well consider the support of <span class="math inline">\(X\)</span> to be a rather large interval, leading to a continuous random variable.</p>
</div>
</div>
<p>The goal of statistics is to use a sample to say something about the underlying population. Consider taking a sample of size <span class="math inline">\(n\)</span> and measuring a single variable on each unit of observation. Then, we might represent the measurements we will obtain (note the use of the future tense) as <span class="math inline">\(X_1, X_2, \dots, X_n\)</span>. While the majority of probability courses focus on a single, or maybe two, random variables, note that collecting data on a sample requires that we deal with at least <span class="math inline">\(n\)</span> random variables (one measurement for each of the observations in our sample).</p>
</section>
<section id="characterizing-a-distribution" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="characterizing-a-distribution"><span class="header-section-number">2.2</span> Characterizing a Distribution</h2>
<p>Again, the goal of statistics is to use a sample to say something about the underlying population. Consider the following research objective:</p>
<blockquote class="blockquote">
<p>Estimate the cost (in US dollars) of a diamond for sale in the United States.</p>
</blockquote>
<p>For this research objective, our population of interest is all diamonds for sale in the United States. We would not expect every diamond for sale to have the same price; variability is inherent in any process. As a result, the sale price of diamonds has a distribution across this population. This is our primary use of probability theory in a statistical analysis — to model distributions.</p>
<p>Consider taking a sample of size 1 from the population; let <span class="math inline">\(Y\)</span> represent the cost of the diamond that is selected. Since we have not yet observed the cost of this diamond, <span class="math inline">\(Y\)</span> is a random variable. And, since this diamond is sampled from the population of interest, the support of <span class="math inline">\(Y\)</span> is determined by the cost of diamonds in the United States. Further, the likelihood that <span class="math inline">\(Y\)</span> falls within any interval is determined by the distribution of the cost across the population. That is, the distribution of <span class="math inline">\(Y\)</span> is the distribution of the population.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>If a random variable <span class="math inline">\(X\)</span> represents a measurement for a single observation from a population, the distribution of the random variable corresponds to the distribution of the variable across the population.</p>
</div>
</div>
<p>A key realization in statistical analysis is that we will never fully observe the distribution of the population; however, we can posit a model for this distribution. In probability, the most common way to characterize the distribution of a random variable is through its density function.</p>
<div id="def-density-function" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Density Function) </strong></span>A density function <span class="math inline">\(f\)</span> relates the values in the support of a random variable with the probability of observing those values.</p>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable, then its density function <span class="math inline">\(f\)</span> is the function such that</p>
<p><span class="math display">\[Pr(a \leq X \leq b) = \int_a^b f(x) dx\]</span></p>
<p>for any real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the support.</p>
<p>Let <span class="math inline">\(X\)</span> be a discrete random variable, then its density function <span class="math inline">\(f\)</span> is the function such that</p>
<p><span class="math display">\[Pr(X = u) = f(u)\]</span></p>
<p>for any real number <span class="math inline">\(u\)</span> in the support.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of a Density Function
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span> defined over support <span class="math inline">\(\mathcal{S}\)</span>. Then,</p>
<ol type="1">
<li><span class="math inline">\(f(x) \geq 0\)</span> for all <span class="math inline">\(x \in \mathcal{S}\)</span>. That is, the density is non-negative for all values in the support.</li>
<li>If <span class="math inline">\(X\)</span> is a continuous random variable, then <span class="math inline">\(\int_{\mathcal{S}} f(x) dx = 1\)</span>; similarly, if <span class="math inline">\(X\)</span> is a discrete random variable, then <span class="math inline">\(\sum_{\mathcal{S}} f(x) = 1\)</span>. That is, <span class="math inline">\(X\)</span> must take a value in its support; so, <span class="math inline">\(Pr(X \in \mathcal{S}) = 1\)</span>, similar to the second Axiom of Probability (<a href="01-fundamentals.html#def-axioms">Definition&nbsp;<span>1.3</span></a>).</li>
<li><span class="math inline">\(f(x) = 0\)</span> for all values of <span class="math inline">\(x \notin \mathcal{S}\)</span>. The density takes the value of 0 for all values outside the support.</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In a probability course, there is often a distinction made between probability “density” functions (used for continuous random variables) and probability “mass” functions (used for discrete random variables). We do not make this distinction and instead rely on the context to determine whether we are dealing with a continuous or discrete random variable. Throughout, we will note when the operations differ between these two types of variables. Measure theory provides a unifying framework to these issues.</p>
</div>
</div>
<p>With few exceptions, we will be working with continuous random variables. As a result, the density function is a smooth function over some region, and the actual value of the function is not interpretable; instead, we get at a probability by considering the area under the curve. Again, drawing connections to data analysis, we can think of a density function as a mathematical formula representing a smooth histogram. The area under the curve for any region gives the proportion of the population which has a value in that region. That is, we get the probability that a random variable will be in an interval by integrating the density function over that interval.</p>
<p>Figure <a href="#fig-randomvariables-density">Figure&nbsp;<span>2.1</span></a> illustrates this idea; we have data from a sample of diamonds from the population of interest. The sample is summarized with a histogram; we have overlayed a posited density (with the corresponding mathematical function that describes this density) for the population. The sample (summarized with the histogram) is approximating the population (modeled using the density function).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-randomvariables-density" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./images/fig-randomvariables-density-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Histogram with a density function overlayed."></p>
<figcaption class="figure-caption">Figure&nbsp;2.1: Illustration of a density function representing the posited distribution of the population alongside a histogram summarizing the cost of diamonds using a sample of 53940 diamonds.</figcaption>
</figure>
</div>
</div>
</div>
<p>You may recognize the particular form of the density function in <a href="#fig-randomvariables-density">Figure&nbsp;<span>2.1</span></a>. The general form is</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma} e^{-x / \sigma} \qquad \text{for } x &gt; 0\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the <em>scale</em> parameter that defines the distribution (set at 4000 in <a href="#fig-randomvariables-density">Figure&nbsp;<span>2.1</span></a>). This is known as the Exponential distribution with scale parameter <span class="math inline">\(\sigma\)</span>. This illuminates another connection between probability and statistics.</p>
<p>Note that our research objective describe above is an ill-posed question as stated. The answer is “it depends” since each individual diamond in the population has a different value. Well-posed questions in statistics are centered on an appropriately chosen <strong>parameter</strong>.</p>
<div id="def-parameter" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 (Parameter) </strong></span>Numeric quantity which summarizes the distribution of a variable within the <em>population</em> of interest. Generally denoted by Greek letters in statistical formulas.</p>
</div>
<p>In probability, the parameters are values that are tuned or set within a problem; we then work forward to compute the probability of an event of interest. In practice, however, when we posit a functional form for a density function to describe the distribution of the population, the parameters are unknown. We plan to use the data to estimate or characterize the parameter; but, the parameter itself will remain unknown. In both cases, however, the parameter is a <em>fixed quantity</em>, even if we are ignorant of that value.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>When a probability model is specified for a population, it is generally specified up to some unknown parameter(s). Making inference on the unknown parameter(s) therefore characterizes the entire distribution.</p>
</div>
</div>
<section id="common-parameters" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="common-parameters"><span class="header-section-number">2.2.1</span> Common Parameters</h3>
<p>Most scientific questions are focused on the location or spread of a distribution. For example, we are interested in estimating the average cost of a diamond sold in the United States. Introductory statistics introduces summaries of location and spread within the sample (e.g., sample mean for location and sample variance for spread). Analogous summaries exist for density functions. As stated above, parameters are unknown constants that govern the form of the density function. Because they govern the form of the density, the parameters are also related to those summarizing the location or spread of the distribution.</p>
<div id="def-mean" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6 (Expected Value (Mean)) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span> defined over the support <span class="math inline">\(\mathcal{S}\)</span>. The expected value of a random variable, also called the mean and denoted <span class="math inline">\(E(X)\)</span>, is given by</p>
<p><span class="math display">\[E(X) = \int_{\mathcal{S}} x f(x) dx\]</span></p>
<p>for continuous random variables and</p>
<p><span class="math display">\[E(X) = \sum_{\mathcal{S}} x f(x)\]</span></p>
<p>for discrete random variables.</p>
</div>
<p>Notice the similarity between the form of the sample mean and the population mean. A sample mean takes the sum of each value in the sample, weighting each value by <span class="math inline">\(1/n\)</span> (where <span class="math inline">\(n\)</span> is the sample size). Without information about the underlying population, the sample must treat each value observed as equally likely; values become more likely if they appear multiple times. In the population, however, when the form of <span class="math inline">\(f\)</span> is known, the density provides information about the likelihood of each value giving us a better weight than <span class="math inline">\(1/n\)</span>. That is, the population mean is a sum of the values in the support, weighting each value by the corresponding value of the density function.</p>
<div id="def-variance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.7 (Variance) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span> defined over the support <span class="math inline">\(\mathcal{S}\)</span>. The variance of a random variable, denoted <span class="math inline">\(Var(X)\)</span>, is given by</p>
<p><span class="math display">\[Var(X) = E\left[X - E(X)\right]^2 = E\left(X^2\right) - E^2(X).\]</span></p>
<p>If we let <span class="math inline">\(\mu = E(X)\)</span>, then this is equivalent to</p>
<p><span class="math display">\[\int_{\mathcal{S}} (x - \mu)^2 f(x) dx\]</span></p>
<p>for continuous random variables and</p>
<p><span class="math display">\[\sum_{\mathcal{S}} (x - \mu)^2 f(x)\]</span></p>
<p>for discrete random variables.</p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Pay careful attention to the notation. <span class="math inline">\(E^2(X)\)</span> represents the square of the expected value; that is,</p>
<p><span class="math display">\[E^2(X) = \left[E(X)\right]^2.\]</span></p>
<p>However, <span class="math inline">\(E(X)^2\)</span> represents the expected value of the square of <span class="math inline">\(X\)</span>; that is,</p>
<p><span class="math display">\[E(X)^2 = E\left(X^2\right).\]</span></p>
</div>
</div>
<p>The variance provides a measure of spread; in particular, it is capturing distance from the mean. Notice that the form of the variance involves taking the expectation of a squared term; in general, we will need to consider expectations of functions.</p>
<div id="def-expectation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.8 (Expectation of a Function) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span> over the support <span class="math inline">\(\mathcal{S}\)</span>, and let <span class="math inline">\(g\)</span> be a real-valued function. Then,</p>
<p><span class="math display">\[E\left[g(X)\right] = \int_{\mathcal{S}} g(x) f(x) dx\]</span></p>
<p>for continuous random variables and</p>
<p><span class="math display">\[E\left[g(X)\right] = \sum_{\mathcal{S}} g(x) f(x)\]</span></p>
<p>for discrete random variables.</p>
</div>
<p>A result of <a href="#def-expectation">Definition&nbsp;<span>2.8</span></a> is the following, very useful theorem, which states that expectations are linear operators.</p>
<div id="thm-expectation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Expectation of a Linear Combination) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable, and let <span class="math inline">\(a_1, a_2, \dotsc, a_m\)</span> be real-valued constants and <span class="math inline">\(g_1, g_2, \dotsc, g_m\)</span> be real-valued functions; then,</p>
<p><span class="math display">\[E\left[\sum_{i=1}^{m} a_i g_i(X)\right] = \sum_{i=1}^{m} a_i E\left[g_i(X)\right].\]</span></p>
</div>
<p>The mean and variance play an important role in characterizing a distribution, especially within statistical theory (as we will see in future chapters). However, there is another set of parameters which are important.</p>
<div id="def-percentile" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.9 (Percentile) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with density function <span class="math inline">\(f\)</span>. The <span class="math inline">\(100k\)</span> percentile is the value <span class="math inline">\(q\)</span> such that</p>
<p><span class="math display">\[Pr(X \leq q) = k.\]</span></p>
</div>
<div id="exm-parameters" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Parameters of Exponential Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be an Exponential distribution with scale parameter <span class="math inline">\(\sigma\)</span>; that is, the density function <span class="math inline">\(f\)</span> is given by</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma} e^{-x/\sigma} \qquad x &gt; 0\]</span></p>
<p>where <span class="math inline">\(\sigma &gt; 0.\)</span> Compute the mean, variance, and median of this distribution, as a function of the unknown scale parameter.</p>
</div>
<p>The solution to this problem is particularly important as it illustrates a very useful technique when working with known distributions in statistical theory.</p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>We note that the function</p>
<p><span class="math display">\[g(y) = \frac{1}{\beta^{\alpha} \Gamma(\alpha)} y^{\alpha - 1} e^{-y/\beta}\]</span></p>
<p>is a valid density function over the positive real line provided that <span class="math inline">\(\alpha,\beta &gt; 0\)</span>; in particular, this is known as a Gamma distribution. Since <span class="math inline">\(g\)</span> is a valid density function, then we know that</p>
<p><span class="math display">\[\int_{0}^{\infty} g(y) dy = 1\]</span></p>
<p>for all values of <span class="math inline">\(\alpha,\beta &gt; 0\)</span>.</p>
<p>Now, let <span class="math inline">\(X\)</span> be an Exponential random random variable with scale parameter <span class="math inline">\(\sigma\)</span>. Then, the expected value of <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[
\begin{aligned}
  E(X)
    &amp;= \int_{0}^{\infty} x \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &amp;= \int_{0}^{\infty} \frac{1}{\sigma} x^{2-1} e^{-x/\sigma} dx
\end{aligned}
\]</span></p>
<p>where we have simply rewritten the exponent in the second line. Notice that expression within the integral shares a striking similarity to the form of the density function of a Gamma distribution; however, they are not exactly the same. To coerce the expression into that of the Gamma density function, we “do nothing” — multiplying and dividing the expression by the quantity <span class="math inline">\(\sigma\Gamma(2)\)</span>. This gives</p>
<p><span class="math display">\[
\begin{aligned}
  E(X)
    &amp;= \int_{0}^{\infty} x \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &amp;= \int_{0}^{\infty} \frac{1}{\sigma} x^{2-1} e^{-x/\sigma} dx \\
    &amp;= \int_{0}^{\infty} \sigma \Gamma(2) \frac{1}{\sigma^2 \Gamma(2)} x^{2-1} e^{-x/\sigma} dx \\
    &amp;= \sigma \Gamma(2) \int_{0}^{\infty} \frac{1}{\sigma^2 \Gamma(2)} x^{2-1} e^{-x/\sigma} dx \\
    &amp;= \sigma \Gamma(2) \\
    &amp;= \sigma.
\end{aligned}
\]</span></p>
<p>In line 3, we have multiplied and divided by <span class="math inline">\(\sigma \Gamma(2)\)</span>, which does not change the problem. In line 4, we have pulled out the terms <span class="math inline">\(\sigma \Gamma(2)\)</span> since it is a constant with respect to the integral; what is left inside the integral is the form of the density function for a Gamma distribution where <span class="math inline">\(\alpha = 2\)</span> and <span class="math inline">\(\beta = \sigma\)</span>. In line 5, we make use of the fact that the integral of any density function over the entire support for which it is defined must be 1. Finally, in line 6, we recognize that <span class="math inline">\(\Gamma(k) = (k-1)!\)</span> if <span class="math inline">\(k\)</span> is a natural number.</p>
<p>Applying the same process, we also have that</p>
<p><span class="math display">\[
\begin{aligned}
  E\left(X^2\right)
    &amp;= \int_{0}^{\infty} x^2 \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &amp;= \sigma^2 \Gamma(3)\int_{0}^{\infty} \frac{1}{\sigma^3 \Gamma(3)} x^{3-1} e^{-x/\sigma} dx \\
    &amp;= 2\sigma^2.
\end{aligned}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[Var(X) = E\left(X^2\right) - E^2(X) = 2\sigma^2 - \sigma^2 = \sigma^2.\]</span></p>
<p>Finally, the median is the value <span class="math inline">\(q\)</span> such that <span class="math inline">\(Pr(X \leq q) = 0.5\)</span>; but, we recognize that</p>
<p><span class="math display">\[
\begin{aligned}
  Pr(X \leq q)
    &amp;= \int_{0}^{q} \frac{1}{\sigma} e^{-x/\sigma} dx \\
    &amp;= \left. -e^{-x/\sigma} \right|_{0}^{q} \\
    &amp;= -e^{-q/\sigma} + 1.
\end{aligned}
\]</span></p>
<p>Setting this expression equal to 0.5 and solving for <span class="math inline">\(q\)</span> yields <span class="math inline">\(q = -\sigma \log(0.5)\)</span>, where <span class="math inline">\(\log(\cdot)\)</span> represents the <em>natural</em> logarithm.</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose the density <span class="math inline">\(f\)</span> is a function of the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>; then, the mean, variance, and median (as well as any other parameters of interest in a research objective) will be functions of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
</div>
<p><a href="#exm-parameters">Example&nbsp;<span>2.1</span></a> highlighted a useful technique for simplifying integrals in statistical applications, which makes use of the “do nothing” strategy discussed in the previous chapter. The solution also shows that there is more than one way to characterize a distribution.</p>
</section>
<section id="distribution-function" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="distribution-function"><span class="header-section-number">2.2.2</span> Distribution Function</h3>
<p>Especially for visualization, the density function is the most common way of characterizing a probability model. However, computing the probability using the density is problematic due to the integration required. Many software address this by working with the cumulative distribution function (CDF).</p>
<div id="def-cdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.10 (Cumulative Distribution Function (CDF)) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable; the cumulative distribution function (CDF) is defined as</p>
<p><span class="math display">\[F(u) = Pr(X \leq u).\]</span></p>
<p>For a continuous random variable, we have that</p>
<p><span class="math display">\[F(u) = \int_{-\infty}^{u} f(x) dx\]</span></p>
<p>implying that the density function is the derivative of the CDF. For a discrete random variable</p>
<p><span class="math display">\[F(u) = \sum_{x \leq u} f(x).\]</span></p>
</div>
<p>Working with the CDF improves computation because it avoids the need to integrate each time; instead, the integral is computed once (and stored internally in the computer) and we use the result to compute probabilities directly.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Density functions are the mathematical models for distributions; they link values of the variable with the likelihood of occurrence. However, for computational reasons, we often work with the cumulative distribution function which provides the probability of being less than or equal to a value.</p>
</div>
</div>
</section>
</section>
<section id="common-probability-distributions" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="common-probability-distributions"><span class="header-section-number">2.3</span> Common Probability Distributions</h2>
<p>While we could posit any non-negative function as a model for a density function (properly scaled of course), there are some models that are very common. While the following list is not exhaustive, it does include the most commonly used distributions that we will encounter in the text.</p>
<p>When a response is binary (assumes one of two values), it is a Bernoulli distribution. In order to make use of this distribution, we typically define one of the two possible outcomes as a “success” and the other as a “failure.” For example,</p>
<p><span class="math display">\[X = \begin{cases} 1 &amp; \text{if a success is observed} \\ 0 &amp; \text{if a success is not observed.} \end{cases}\]</span></p>
<div id="def-bernoulli-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.11 (Bernoulli Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be a discrete random variable taking the value 0 or 1. <span class="math inline">\(X\)</span> is said to have a Bernoulli distribution with density</p>
<p><span class="math display">\[f(x) = \theta^x (1 - \theta)^{1 - x} \qquad x \in \{0, 1\},\]</span></p>
<p>where <span class="math inline">\(0 &lt; \theta &lt; 1\)</span> is the probability that <span class="math inline">\(X\)</span> takes the value 1.</p>
<ul>
<li><span class="math inline">\(E(X) = \theta\)</span></li>
<li><span class="math inline">\(Var(X) = \theta(1 - \theta)\)</span></li>
</ul>
<p>We write <span class="math inline">\(X \sim Ber(\theta)\)</span>, which is read “X follows a Bernoulli distribution with probability <span class="math inline">\(\theta\)</span>.”</p>
</div>
<p>We can generalize the Bernoulli distribution to count the number of successes out of <span class="math inline">\(n\)</span> independent trials.</p>
<div id="def-binomial-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.12 (Binomial Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be a discrete random variable taking integer values between 0 and <span class="math inline">\(n\)</span>, inclusive. <span class="math inline">\(X\)</span> is said to have a Binomial distribution with density</p>
<p><span class="math display">\[f(x) = \begin{pmatrix}n \\ x\end{pmatrix} \theta^x (1 - \theta)^{1 - x} \qquad x \in \{0, 1, \dotsc, n\},\]</span></p>
<p>where <span class="math inline">\(0 &lt; \theta &lt; 1\)</span> is the probability of a success on an individual trial.</p>
<ul>
<li><span class="math inline">\(E(X) = n\theta\)</span></li>
<li><span class="math inline">\(Var(X) = n\theta(1 - \theta)\)</span></li>
</ul>
<p>We write <span class="math inline">\(X \sim Bin(n, \theta)\)</span>, which is read “X follows a Binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>.”</p>
</div>
<p>While there are many other discrete distributions that play important roles in categorical data analyses, the majority of our text will focus on quantitative response variables. So, we list several key distributions for continuous random variables.</p>
<div id="def-normal-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.13 (Normal (Gaussian) Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have a Normal (or Guassian) distribution if the density is given by</p>
<p><span class="math display">\[f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2} (x - \mu)^2} \qquad -\infty &lt; x &lt; \infty,\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is any real number and <span class="math inline">\(\sigma^2 &gt; 0\)</span>.</p>
<ul>
<li><span class="math inline">\(E(X) = \mu\)</span></li>
<li><span class="math inline">\(Var(X) = \sigma^2\)</span></li>
</ul>
<p>We write <span class="math inline">\(X \sim N\left(\mu, \sigma^2\right)\)</span>, which is read “X follows a Normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.” This short-hand implies the density above. When <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>, this is referred to as the Standard Normal distribution.</p>
</div>
<p>This model is a bell-shaped distribution centered at the mean <span class="math inline">\(\mu\)</span>. While this is a common model, it should not be assumed by default. In future chapters, we will consider methods for assessing whether, given a sample, assuming the population follows a Normal distribution is reasonable.</p>
<div id="def-gamma-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.14 (Gamma Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have a Gamma distribution if the density is given by</p>
<p><span class="math display">\[f(x) = \frac{1}{\beta^{\alpha} \Gamma(\alpha)} x^{\alpha - 1} e^{-x/\beta} \qquad x &gt; 0,\]</span></p>
<p>where <span class="math inline">\(\alpha &gt; 0\)</span> is the shape parameter and <span class="math inline">\(\beta &gt; 0\)</span> is the scale parameter.</p>
<ul>
<li><span class="math inline">\(E(X) = \alpha\beta\)</span></li>
<li><span class="math inline">\(Var(X) = \alpha\beta^2\)</span></li>
</ul>
<p>We write <span class="math inline">\(X \sim Gamma\left(\alpha, \beta\right)\)</span>, which is read “X follows a Gamma distribution with shape <span class="math inline">\(\alpha\)</span> and scale <span class="math inline">\(\beta\)</span>.” This short-hand implies the density above. When <span class="math inline">\(\alpha = 1\)</span>, we refer to this as the Exponential distribution with scale <span class="math inline">\(\beta\)</span>.</p>
<p>We note that, in general, there is no closed form solution for <span class="math inline">\(\Gamma(\alpha)\)</span>, but</p>
<ul>
<li><span class="math inline">\(\Gamma(\alpha) = (\alpha - 1) \Gamma(\alpha - 1)\)</span></li>
<li><span class="math inline">\(\Gamma(k) = (k - 1)!\)</span> for non-negative integer <span class="math inline">\(k\)</span></li>
</ul>
</div>
<p>The Gamma distribution is useful for modeling time-to events.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>We have presented the Gamma (and Exponential) distribution in terms of the <em>scale</em> parameter. It is sometimes easier to parameters the distribution in terms of the <em>rate</em> parameter, where the rate is the inverse of the scale. When consulting tables of distributions<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, be sure to note the parameterization of the distribution provided.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Exponential distribution being a special case of the Gamma distribution is not the only relationship between common distributions. There are many relationships<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that are useful; we will describe these as needed.</p>
</div>
</div>
<p>The (standardized) t-distribution is a bell-shaped distribution, similar to the Normal distribution but with wider tails. It has a single parameter, known as the degrees of freedom. Note that unlike many other distributions, this parameter (the degrees of freedom) is not associated with the location of the distribution. Instead, it governs the spread (but is not equivalent to the variance).</p>
<div id="def-t-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.15 (t-Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have a (standardized) t-distribution, sometimes called the Student’s t-distribution, if the density is given by</p>
<p><span class="math display">\[f(x) = \frac{\Gamma \left(\frac{\nu+1}{2} \right)} {\sqrt{\nu\pi}\,\Gamma \left(\frac{\nu}{2} \right)} \left(1+\frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}} \qquad -\infty &lt; x &lt; \infty\]</span></p>
<p>where <span class="math inline">\(\nu &gt; 0\)</span> is the degrees of freedom.</p>
<p>We write <span class="math inline">\(X \sim t_{\nu}\)</span>, which is read “X follows a t-distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.”</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As the degrees of freedom approach infinity, the density function of the t-distribution approaches that of a Standard Normal random variable.</p>
</div>
</div>
<p>The Chi-Square distribution is a skewed distribution (looks like a giant slide). It has a single parameter, known as the degrees of freedom. The degrees of freedom for this distribution characterize both the location and spread simultaneously.</p>
<div id="def-chi-square-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.16 (Chi-Square Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have a Chi-Square distribution if the density is given by</p>
<p><span class="math display">\[f(x) = \frac{1}{2^{\nu/2}\Gamma (\nu/2)}\;x^{\nu/2-1}e^{-x/2} \qquad x &gt; 0,\]</span></p>
<p>where <span class="math inline">\(\nu &gt; 0\)</span> is the degrees of freedom.</p>
<p>We write <span class="math inline">\(X \sim \chi^2_{\nu}\)</span>, which is read “X follows a Chi-Square distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.” The Chi-Square distribution is a special case of the Gamma distribution where <span class="math inline">\(\alpha = \nu/2\)</span> and <span class="math inline">\(\beta = 2\)</span>.</p>
</div>
<p>The F-distribution is a skewed distribution. It has two parameters, known as the numerator and denominator degrees of freedom. While neither variable is directly the mean or variance, together these two parameters characterize both the location and the spread.</p>
<div id="def-f-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.17 (F-Distribution) </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable. <span class="math inline">\(X\)</span> is said to have an F-distribution if the density is given by</p>
<p><span class="math display">\[f(x) = \frac{\Gamma((r + s)/2)}{(\Gamma(r/2) \Gamma(s/2))} (r/s)^{(r/2)} x^{(r/2 - 1)} (1 + (r/s) x)^{-(r + s)/2} \qquad x &gt; 0,\]</span></p>
<p>where <span class="math inline">\(r,s &gt; 0\)</span> are the numerator and denominator degrees of freedom, respectively.</p>
<p>We write <span class="math inline">\(X \sim F_{r, s}\)</span>, which is read “X has an F-distribution with r numerator degrees of freedom and s denominator degrees of freedom.”</p>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-randomvariables-comparisons" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./images/fig-randomvariables-comparisons-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Four density functions overlayed on same axes for comparison."></p>
<figcaption class="figure-caption">Figure&nbsp;2.2: Comparison of various common distributions for continuous random variables.</figcaption>
</figure>
</div>
</div>
</div>
<p>The formulas above are ugly, but we will not be working with them directly. Instead, statistical software have these distributions embedded. The key idea here is that when we know the model for a distribution, we can make use of several results about this distribution.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some probability models occur so frequently that we give them names for easy reference. Some models are common for modeling the population, in which case they are defined in terms of unknown parameters to be estimated. Some models are used, not to model a population, but to model other distributions that occur in statistical practice.</p>
</div>
</div>
</section>
<section id="transformations-of-a-random-variable" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="transformations-of-a-random-variable"><span class="header-section-number">2.4</span> Transformations of a Random Variable</h2>
<p>Occasionally, we are interested in a transformation of a particular characteristic. That is, we have a model for the distribution of <span class="math inline">\(X\)</span>, but we are interested in <span class="math inline">\(Y = g(X)\)</span>. In this section, we examine one method for determining the density of <span class="math inline">\(Y\)</span> from the density of <span class="math inline">\(X\)</span>.</p>
<p>While there various approaches to this problem, we find this method the most reliable. Further, it does not require the memorization of a formula, but instead builds on fundamental ideals. This is known as the <strong>Method of Distribution Functions</strong>.</p>
<div id="def-method-of-distribution-functions" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.18 (Method of Distribution Functions) </strong></span>Let <span class="math inline">\(X\)</span> be a continuous random variable with density <span class="math inline">\(f\)</span> and cumulative distribution function <span class="math inline">\(F\)</span>. Consider <span class="math inline">\(Y = h(X)\)</span>. The following process provides the density function <span class="math inline">\(g\)</span> of <span class="math inline">\(Y\)</span> by first finding its cumulative distribution function <span class="math inline">\(G\)</span>.</p>
<ol type="1">
<li>Find the set <span class="math inline">\(A\)</span> for which <span class="math inline">\(h(X) \leq t\)</span> if and only if <span class="math inline">\(X \in A\)</span>.</li>
<li>Recognize that <span class="math inline">\(G(y) = Pr(Y \leq y) = Pr\left(h(X) \leq y\right) = Pr(X \in A)\)</span>.</li>
<li>If interested in <span class="math inline">\(g(y)\)</span>, note that <span class="math inline">\(g(y) = \frac{\partial}{\partial y} G(y)\)</span>.</li>
</ol>
</div>
<p>When <span class="math inline">\(h\)</span> is a strictly monotone function (unique inverse exists), then step 1-2 is much easier because we can apply <span class="math inline">\(h^{-1}\)</span>. In step 2 of the above process, the final expression is often left in terms of <span class="math inline">\(F\)</span>, the CDF of <span class="math inline">\(X\)</span>; then, when we find the density in step 3, we can apply the chain rule (avoiding the need to actually have an expression for <span class="math inline">\(F\)</span>).</p>
<div id="exm-transformations" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Transformation of a Random Variable) </strong></span>Previously, we posited the following model for the distribution of the cost of a diamond sold in the US:</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma} e^{-x/\sigma} \qquad x &gt; 0\]</span></p>
<p>for some <span class="math inline">\(\sigma &gt; 0\)</span>. As cost is generally a heavily skewed variable, we may be interested in taking the (natural) logarithm before proceeding with an analysis. Find the density of <span class="math inline">\(Y = \log(X)\)</span>.</p>
</div>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>We note that <span class="math inline">\(\log(x)\)</span> is a strictly monotone function. Therefore, we have that</p>
<p><span class="math display">\[
\begin{aligned}
  G(y) &amp;= Pr(Y \leq y) \\
    &amp;= Pr(\log(X) \leq y) \\
    &amp;= Pr\left(X \leq e^y\right).
\end{aligned}
\]</span></p>
<p>Just to place this within the method described above, since <span class="math inline">\(\log(x) \leq y\)</span> if and only if <span class="math inline">\(x \leq e^y\)</span>, then <span class="math inline">\(A = \{t: x \leq e^t\}\)</span>. Of course, we didn’t really need to identify this because we were able to apply the inverse of <span class="math inline">\(\log(x)\)</span> directly within the probability expression. We now recognize that we have a probability of the form “<span class="math inline">\(X\)</span> less than or equal to something.” And, this matches the form of the CDF of <span class="math inline">\(X\)</span>. That is, we have that</p>
<p><span class="math display">\[G(y) = F\left(e^y\right).\]</span></p>
<p>This completes step 2 of the procedure; we have expressed the CDF of <span class="math inline">\(Y\)</span> as a function of the CDF of <span class="math inline">\(X\)</span>. Now, to find the density, we apply the chain rule.</p>
<p><span class="math display">\[
\begin{aligned}
  g(y)
    &amp;= \frac{\partial}{\partial y} G(y) \\
    &amp;= \left[\left.\frac{\partial}{\partial x} F(x)\right|_{x = e^y}\right] \frac{\partial}{\partial y} e^y \\
    &amp;= \left[\left. f(x) \right|_{x = e^y}\right] e^y \\
    &amp;= f\left(e^y\right) e^y \\
    &amp;= \frac{1}{\sigma} e^{-e^y/\sigma} e^y
\end{aligned}
\]</span></p>
<p>which will be valid for all real values of <span class="math inline">\(y\)</span>; that is, the support of <span class="math inline">\(Y\)</span> is all real numbers. In line 2 above, we applied the chain rule to compute the derivative, avoiding the need to explicitly state the CDF of <span class="math inline">\(X\)</span>.</p>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>While mathematicians distinguish between a derivative <span class="math inline">\(\frac{d}{dx}\)</span> and a partial derivative <span class="math inline">\(\frac{\partial}{\partial x}\)</span>, we do not make that distinction.</p>
</div>
</div>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>A good <a href="https://qiangbo-workspace.oss-cn-shanghai.aliyuncs.com/2018-11-11-common-probability-distributions/distab.pdf">table of common distributions</a> is given in Casella and Berger, a popular text for statistical theory at the graduate level.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>An excellent <a href="http://www.math.wm.edu/~leemis/chart/UDR/UDR.html">summary of the relationships between Distributions</a> was developed by faculty at the College of William and Mary.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-fundamentals.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Essential Probability</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-samplingdistributions.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Sampling Distribution of a Statistic</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>